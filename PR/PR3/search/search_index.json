{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SuperCloud Docs \uf0c1 For full documentation visit mkdocs.org . Commands \uf0c1 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \uf0c1 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-supercloud-docs","text":"For full documentation visit mkdocs.org .","title":"Welcome to SuperCloud Docs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"acknowledging-us/","text":"Acknowledging Us {#acknowledging_us} \uf0c1 The canonical article about the MIT Supercloud is: Interactive Supercomputing on 40,000 Cores for Machine Learning and Data Analysis , Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor, Bill Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin, Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Peter Michaleas, paper presented at the 2018 IEEE High Performance Extreme Computing Conference (HPEC), July 2018. When referencing the MIT Supercloud, please use this reference. The link above is to IEEE Xplore, the paper can also be found on arXiv.org . For convenience, here is a bibtex listing you can copy and paste into your paper: @inproceedings{reuther2018interactive, title={Interactive supercomputing on 40,000 cores for machine learning and data analysis}, author={Reuther, Albert and Kepner, Jeremy and Byun, Chansup and Samsi, Siddharth and Arcand, William and Bestor, David and Bergeron, Bill and Gadepally, Vijay and Houle, Michael and Hubbell, Matthew and Jones, Michael and Klein, Anna and Milechin, Lauren and Mullen, Julia and Prout, Andrew and Rosa, Antonio and Yee, Charles and Michaleas, Peter}, booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)}, pages={1--6}, year={2018}, organization={IEEE} } If you would like to acknowledge the MIT Supercloud in your paper or report, we recommend the following (be sure to select the applicable resource(s) from among the listed resources that we provide): The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing (HPC, database, consultation) resources that have contributed to the research results reported within this paper/report. Thank you for acknowledging us -- we appreciate it.","title":"Acknowledging Us"},{"location":"acknowledging-us/#acknowledging-us-acknowledging_us","text":"The canonical article about the MIT Supercloud is: Interactive Supercomputing on 40,000 Cores for Machine Learning and Data Analysis , Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor, Bill Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin, Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Peter Michaleas, paper presented at the 2018 IEEE High Performance Extreme Computing Conference (HPEC), July 2018. When referencing the MIT Supercloud, please use this reference. The link above is to IEEE Xplore, the paper can also be found on arXiv.org . For convenience, here is a bibtex listing you can copy and paste into your paper: @inproceedings{reuther2018interactive, title={Interactive supercomputing on 40,000 cores for machine learning and data analysis}, author={Reuther, Albert and Kepner, Jeremy and Byun, Chansup and Samsi, Siddharth and Arcand, William and Bestor, David and Bergeron, Bill and Gadepally, Vijay and Houle, Michael and Hubbell, Matthew and Jones, Michael and Klein, Anna and Milechin, Lauren and Mullen, Julia and Prout, Andrew and Rosa, Antonio and Yee, Charles and Michaleas, Peter}, booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)}, pages={1--6}, year={2018}, organization={IEEE} } If you would like to acknowledge the MIT Supercloud in your paper or report, we recommend the following (be sure to select the applicable resource(s) from among the listed resources that we provide): The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing (HPC, database, consultation) resources that have contributed to the research results reported within this paper/report. Thank you for acknowledging us -- we appreciate it.","title":"Acknowledging Us {#acknowledging_us}"},{"location":"faqs/","text":"Frequently Asked Questions {#faq} \uf0c1 This page contains the answers to a few questions that we receive often. As more questions are asked, this page may be updated. How do I get an account? \uf0c1 To request an account, follow the instructions and answer the questions on our requesting_account {.interpreted-text role=\"ref\"}. We will reach out to you once your account is created, or if we have any questions for you. I would like to log in from a new computer. Can I add a new ssh key? \uf0c1 If you have a new computer, or want to add keys for additional computers that you use, you can add your own key on our web portal. Instructions on how to generate a new ssh key and add it to your account are on our Account Request Page<sshkeys> {.interpreted-text role=\"ref\"}. In summary, log in with your credentials (for MIT and other educational institutions this is the middle option when you go to https://txe1-portal.mit.edu ) and then click on the \"sshkeys\" link. Scroll to the bottom and paste your key in the box. How much storage do I have for my account? \uf0c1 We do not impose storage limits. However, it is recommended that users not use their accounts as primary storage. Further, we do not back up the storage on the system, so we strongly recommend transferring your code, data, and any other important files to another machine for backup. How can I share files/code/data with my colleagues? \uf0c1 If you would like to share files with others you can request a shared group directory. Shared group directories are located at /home/gridsan/groups and we will put a symlink in your home directory to use as a shortcut to your shared group directory. To request one, send email to supercloud\\@mit.edu and let us know: What the group should be called. Short, descriptive names are best. Who should be the owner/approver for the group. We will ask this person for approval whenever we receive a request to join a group. Who should be in the group. Supercloud usernames are helpful, but not required. Whether you plan to store any non-public data in the group. If you do, let us know what requirements, restrictions, or agreements are associated with the data. See why we ask here<nonpublic_data> {.interpreted-text role=\"ref\"}. To learn more about Shared Groups and best practices using them, see the page on shared_groups {.interpreted-text role=\"ref\"}. How do I set/change my password? \uf0c1 You most likely do not need to set a password. If you have an active MIT Kerberos or login from another University, you can most likely log in using your institution\\'s credentials. On the Supercloud Web Portal Login page, select the middle option \\\"MIT Touchstone/InCommon Federation\\\". You may have to select your institution from the dropdown list, which should take you to your institution\\'s login page. After you log in, you should see the Portal main page. If you have trouble logging in this way, please contact us and we can help. If you cannot log in using \\\"MIT Touchstone/InCommon Federation\\\", we may set you up with a password. If you have not yet reset your password, or remember your previous password, then follow the instructions on the web_portal {.interpreted-text role=\"ref\"} page. If you have previously set your password and cannot remember it, contact us and we will help you reset your password. Are there any resource limits? \uf0c1 New accounts are created with a small starting resource allocation. Once you have completed the Practical HPC course you can send an email to supercloud@mit.edu to request to be moved to the standard allocation. The starting and standard allocations are listed on the systems_and_software {.interpreted-text role=\"ref\"} page. If you have a deadline and need additional resources you can request more by contacting us . If you looking to request more GPUs, please read through this page on gpu_jobs {.interpreted-text role=\"ref\"} first. Please state the number of additional processors you need, the length of time for which you need it, and tell us about the jobs you are running and how you are submitting them. If you plan to run many independent jobs we will ask you to convert your job to use triples {.interpreted-text role=\"ref\"} before giving and increased allocation. Remember this is a shared system, so during busy times we may not be able to grant your request. We will also only grant increase requests if you have completed the Practical HPC course . It is also important to keep in mind what your fair share of memory is for each process and request additional resources<slurm_memcores> {.interpreted-text role=\"ref\"} if needed. For example, if there are 40 cores and 384GB of RAM on the machine you are using, each processor\\'s fair share would be about 9GB. Check the systems_and_software {.interpreted-text role=\"ref\"} page to see how many cores and how much memory each node type has. If you think your processes will go over this, request additional slots as needed. This ensures you have sufficient memory without killing your job or someone else\\'s. What do I do if my job won\\'t be deleted? \uf0c1 Occasionally this will happen if the node where your job is running goes down, or your job does not exit gracefully. If this happens, contact us with the Job ID, and we\\'ll delete the job and reboot the node if needed. Why do I get an error when I try to install a package? \uf0c1 There are two common reasons you get an error when you try to install a package. If you get a \\\"Permission Denied\\\" or similar error, it is because you are trying to install the package system-wide, rather than your own home directory. See the software_and_packages {.interpreted-text role=\"ref\"} page for more information on how to install packages. If you get a \\\"Network Error\\\", or similar, this is because we don\\'t have internet/network connection on the compute nodes, this includes Jupyter and any interactive jobs. You will have to install the package on one of the login nodes. If you get an error like \\\" Could not install packages due to an EnvironmentError: [Errno 122] Disk quota exceeded \\\" when installing a package with pip or something like \\\" ERROR: could not download https://pkg.julialang.org/registry/... \\\" installing a package with Julia, even though you are on the login node, this is because it is filling up your quota in the /tmp directory. We have set quotas on this directory to prevent a single person from inadvertently filling it up, as when this happens it can cause issues for everyone using the node, including preventing anyone from installing packages. This can be fixed by setting the TMPIDR environment variable like so: mkdir /state/partition1/user/$USER export TMPDIR=/state/partition1/user/$USER After you have installed your package you can clean up any lingering files by removing the temporary directory you have created: rm -rf /state/partition1/user/$USER How can I set up VSCode to edit files remotely on Supercloud? \uf0c1 You can use VSCode to remotely connect to Supercloud via the Remote-SSH extension. The default settings in the VSCode Remote - SSH extension will fail to connect. This is due to it trying to lock files in your home directory, which is disabled for performance reasons. The solution is to have it use the local filesystem. To get it to work, go to your VS Code settings, click \"Extensions\" and then \"Remote - SSH\". Once you\\'re in the settings for Remote - SSH, check the box next to \"Remote.SSH: Lockfiles in Tmp\". What this will do is put any lockfiles in /tmp, rather than your home directory. A side note: we have seen VS Code clutter up /tmp in the past, which we keep fairly small. Disconnecting occasionally should clean these up, however we do not know for sure. If you can check it once in a while and clean up any files that are yours in /tmp, that would be really helpful. How can I use Tensorboard on Supercloud? \uf0c1 Take a look at this page on how to run Tensorboard in an interactive job <tensorboard> {.interpreted-text role=\"ref\"}. I got an Out of Memory error. How can I figure out how much memory my job needs and request more? \uf0c1 This is described on the Submitting Jobs page. If you submit your jobs with sbatch , check out this section<slurm-memcores> {.interpreted-text role=\"ref\"}, and if you use LLsub take a look at this section<llsub-memcores> {.interpreted-text role=\"ref\"}. As described in those links, you can check how much memory your job used using the sacct command, then request enough additional cores for the memory you need. Keep in mind that if your job was killed due to high memory use, your job may not have gotten to the point of highest memory use. To get an accurate measurement you can run your job on an exclusive node long enough to reach the part of the job that would consume the most memory, then stop the job and check the memory use with sacct . My Python/Julia job is running, but I don\\'t see any output in the log files. What is going on? \uf0c1 Julia and Python will buffer output in batch jobs. This means they will hold on to the output and print it out all at once, sometimes this isn\\'t until the end of a loop or the end of the program. You can force both to print the output when it is produced. In Python you can do this by using the -u flag when you call Python in your submission script (ex: python -u myscript.py ). In Julia you can do this by adding flush(stdout) after the print statements in your Julia script that you\\'d like to print immediately (ex: println(\"Hello World!\"); flush(stdout) ). What does the Underutilizing/Oversubscribing the node warning message mean? \uf0c1 When you launch a job in triples mode <#triples> {.interpreted-text role=\"ref\"}, the second and third numbers you provide are the number of processes per node (NPPN) and the number of threads per process (NT). You can multiply these two numbers to get the total number of threads you will have running on the node. You will usually get the best performance if you have the same total number of threads as cores on the node. See the systems_and_software {.interpreted-text role=\"ref\"} page for a list of how many cores are on each node type. Oversubscribing: When you have more threads than the number of cores on the node. Oversubscribing can overwhelm the node, which can slow your job down or even cause it to fail. It can also be harmful for the node. Reduce the number of threads per process or number of processes per node so that the total number of threads is less than or equal to the number of cores. Underutilizing: When you have fewer threads than the number of cores on the node you may be undersubscribing. This means you may not be taking full advantage of the node. Many underlying packages and libraries can take advantage of multithreading. You might try increasing the number of threads per process, while avoiding oversubscribing, to see if you get improved performance. For more information on how to pick the best triple for your job, take a look at the #tuning {.interpreted-text role=\"ref\"} process and recommendations. How can I get more help? \uf0c1 If you have a question that is not answered here, send email to supercloud@mit.edu for more help.","title":"Frequently Asked Questions"},{"location":"faqs/#frequently-asked-questions-faq","text":"This page contains the answers to a few questions that we receive often. As more questions are asked, this page may be updated.","title":"Frequently Asked Questions {#faq}"},{"location":"faqs/#how-do-i-get-an-account","text":"To request an account, follow the instructions and answer the questions on our requesting_account {.interpreted-text role=\"ref\"}. We will reach out to you once your account is created, or if we have any questions for you.","title":"How do I get an account?"},{"location":"faqs/#i-would-like-to-log-in-from-a-new-computer-can-i-add-a-new-ssh-key","text":"If you have a new computer, or want to add keys for additional computers that you use, you can add your own key on our web portal. Instructions on how to generate a new ssh key and add it to your account are on our Account Request Page<sshkeys> {.interpreted-text role=\"ref\"}. In summary, log in with your credentials (for MIT and other educational institutions this is the middle option when you go to https://txe1-portal.mit.edu ) and then click on the \"sshkeys\" link. Scroll to the bottom and paste your key in the box.","title":"I would like to log in from a new computer. Can I add a new ssh key?"},{"location":"faqs/#how-much-storage-do-i-have-for-my-account","text":"We do not impose storage limits. However, it is recommended that users not use their accounts as primary storage. Further, we do not back up the storage on the system, so we strongly recommend transferring your code, data, and any other important files to another machine for backup.","title":"How much storage do I have for my account?"},{"location":"faqs/#how-can-i-share-filescodedata-with-my-colleagues","text":"If you would like to share files with others you can request a shared group directory. Shared group directories are located at /home/gridsan/groups and we will put a symlink in your home directory to use as a shortcut to your shared group directory. To request one, send email to supercloud\\@mit.edu and let us know: What the group should be called. Short, descriptive names are best. Who should be the owner/approver for the group. We will ask this person for approval whenever we receive a request to join a group. Who should be in the group. Supercloud usernames are helpful, but not required. Whether you plan to store any non-public data in the group. If you do, let us know what requirements, restrictions, or agreements are associated with the data. See why we ask here<nonpublic_data> {.interpreted-text role=\"ref\"}. To learn more about Shared Groups and best practices using them, see the page on shared_groups {.interpreted-text role=\"ref\"}.","title":"How can I share files/code/data with my colleagues?"},{"location":"faqs/#how-do-i-setchange-my-password","text":"You most likely do not need to set a password. If you have an active MIT Kerberos or login from another University, you can most likely log in using your institution\\'s credentials. On the Supercloud Web Portal Login page, select the middle option \\\"MIT Touchstone/InCommon Federation\\\". You may have to select your institution from the dropdown list, which should take you to your institution\\'s login page. After you log in, you should see the Portal main page. If you have trouble logging in this way, please contact us and we can help. If you cannot log in using \\\"MIT Touchstone/InCommon Federation\\\", we may set you up with a password. If you have not yet reset your password, or remember your previous password, then follow the instructions on the web_portal {.interpreted-text role=\"ref\"} page. If you have previously set your password and cannot remember it, contact us and we will help you reset your password.","title":"How do I set/change my password?"},{"location":"faqs/#are-there-any-resource-limits","text":"New accounts are created with a small starting resource allocation. Once you have completed the Practical HPC course you can send an email to supercloud@mit.edu to request to be moved to the standard allocation. The starting and standard allocations are listed on the systems_and_software {.interpreted-text role=\"ref\"} page. If you have a deadline and need additional resources you can request more by contacting us . If you looking to request more GPUs, please read through this page on gpu_jobs {.interpreted-text role=\"ref\"} first. Please state the number of additional processors you need, the length of time for which you need it, and tell us about the jobs you are running and how you are submitting them. If you plan to run many independent jobs we will ask you to convert your job to use triples {.interpreted-text role=\"ref\"} before giving and increased allocation. Remember this is a shared system, so during busy times we may not be able to grant your request. We will also only grant increase requests if you have completed the Practical HPC course . It is also important to keep in mind what your fair share of memory is for each process and request additional resources<slurm_memcores> {.interpreted-text role=\"ref\"} if needed. For example, if there are 40 cores and 384GB of RAM on the machine you are using, each processor\\'s fair share would be about 9GB. Check the systems_and_software {.interpreted-text role=\"ref\"} page to see how many cores and how much memory each node type has. If you think your processes will go over this, request additional slots as needed. This ensures you have sufficient memory without killing your job or someone else\\'s.","title":"Are there any resource limits?"},{"location":"faqs/#what-do-i-do-if-my-job-wont-be-deleted","text":"Occasionally this will happen if the node where your job is running goes down, or your job does not exit gracefully. If this happens, contact us with the Job ID, and we\\'ll delete the job and reboot the node if needed.","title":"What do I do if my job won\\'t be deleted?"},{"location":"faqs/#why-do-i-get-an-error-when-i-try-to-install-a-package","text":"There are two common reasons you get an error when you try to install a package. If you get a \\\"Permission Denied\\\" or similar error, it is because you are trying to install the package system-wide, rather than your own home directory. See the software_and_packages {.interpreted-text role=\"ref\"} page for more information on how to install packages. If you get a \\\"Network Error\\\", or similar, this is because we don\\'t have internet/network connection on the compute nodes, this includes Jupyter and any interactive jobs. You will have to install the package on one of the login nodes. If you get an error like \\\" Could not install packages due to an EnvironmentError: [Errno 122] Disk quota exceeded \\\" when installing a package with pip or something like \\\" ERROR: could not download https://pkg.julialang.org/registry/... \\\" installing a package with Julia, even though you are on the login node, this is because it is filling up your quota in the /tmp directory. We have set quotas on this directory to prevent a single person from inadvertently filling it up, as when this happens it can cause issues for everyone using the node, including preventing anyone from installing packages. This can be fixed by setting the TMPIDR environment variable like so: mkdir /state/partition1/user/$USER export TMPDIR=/state/partition1/user/$USER After you have installed your package you can clean up any lingering files by removing the temporary directory you have created: rm -rf /state/partition1/user/$USER","title":"Why do I get an error when I try to install a package?"},{"location":"faqs/#how-can-i-set-up-vscode-to-edit-files-remotely-on-supercloud","text":"You can use VSCode to remotely connect to Supercloud via the Remote-SSH extension. The default settings in the VSCode Remote - SSH extension will fail to connect. This is due to it trying to lock files in your home directory, which is disabled for performance reasons. The solution is to have it use the local filesystem. To get it to work, go to your VS Code settings, click \"Extensions\" and then \"Remote - SSH\". Once you\\'re in the settings for Remote - SSH, check the box next to \"Remote.SSH: Lockfiles in Tmp\". What this will do is put any lockfiles in /tmp, rather than your home directory. A side note: we have seen VS Code clutter up /tmp in the past, which we keep fairly small. Disconnecting occasionally should clean these up, however we do not know for sure. If you can check it once in a while and clean up any files that are yours in /tmp, that would be really helpful.","title":"How can I set up VSCode to edit files remotely on Supercloud?"},{"location":"faqs/#how-can-i-use-tensorboard-on-supercloud","text":"Take a look at this page on how to run Tensorboard in an interactive job <tensorboard> {.interpreted-text role=\"ref\"}.","title":"How can I use Tensorboard on Supercloud?"},{"location":"faqs/#i-got-an-out-of-memory-error-how-can-i-figure-out-how-much-memory-my-job-needs-and-request-more","text":"This is described on the Submitting Jobs page. If you submit your jobs with sbatch , check out this section<slurm-memcores> {.interpreted-text role=\"ref\"}, and if you use LLsub take a look at this section<llsub-memcores> {.interpreted-text role=\"ref\"}. As described in those links, you can check how much memory your job used using the sacct command, then request enough additional cores for the memory you need. Keep in mind that if your job was killed due to high memory use, your job may not have gotten to the point of highest memory use. To get an accurate measurement you can run your job on an exclusive node long enough to reach the part of the job that would consume the most memory, then stop the job and check the memory use with sacct .","title":"I got an Out of Memory error. How can I figure out how much memory my job needs and request more?"},{"location":"faqs/#my-pythonjulia-job-is-running-but-i-dont-see-any-output-in-the-log-files-what-is-going-on","text":"Julia and Python will buffer output in batch jobs. This means they will hold on to the output and print it out all at once, sometimes this isn\\'t until the end of a loop or the end of the program. You can force both to print the output when it is produced. In Python you can do this by using the -u flag when you call Python in your submission script (ex: python -u myscript.py ). In Julia you can do this by adding flush(stdout) after the print statements in your Julia script that you\\'d like to print immediately (ex: println(\"Hello World!\"); flush(stdout) ).","title":"My Python/Julia job is running, but I don\\'t see any output in the log files. What is going on?"},{"location":"faqs/#what-does-the-underutilizingoversubscribing-the-node-warning-message-mean","text":"When you launch a job in triples mode <#triples> {.interpreted-text role=\"ref\"}, the second and third numbers you provide are the number of processes per node (NPPN) and the number of threads per process (NT). You can multiply these two numbers to get the total number of threads you will have running on the node. You will usually get the best performance if you have the same total number of threads as cores on the node. See the systems_and_software {.interpreted-text role=\"ref\"} page for a list of how many cores are on each node type. Oversubscribing: When you have more threads than the number of cores on the node. Oversubscribing can overwhelm the node, which can slow your job down or even cause it to fail. It can also be harmful for the node. Reduce the number of threads per process or number of processes per node so that the total number of threads is less than or equal to the number of cores. Underutilizing: When you have fewer threads than the number of cores on the node you may be undersubscribing. This means you may not be taking full advantage of the node. Many underlying packages and libraries can take advantage of multithreading. You might try increasing the number of threads per process, while avoiding oversubscribing, to see if you get improved performance. For more information on how to pick the best triple for your job, take a look at the #tuning {.interpreted-text role=\"ref\"} process and recommendations.","title":"What does the Underutilizing/Oversubscribing the node warning message mean?"},{"location":"faqs/#how-can-i-get-more-help","text":"If you have a question that is not answered here, send email to supercloud@mit.edu for more help.","title":"How can I get more help?"},{"location":"getting-help/","text":"Getting Help {#getting_help} \uf0c1 Additional Documentation: \uf0c1 If you haven\\'t found your answer elsewhere in this wiki, you may find it here. We provide a list of FAQs <faq> {.interpreted-text role=\"ref\"} and external Resources . Email: \uf0c1 If none of these resources are answering your question, please contact us at supercloud\\@mit.edu . In this email, please provide, where applicable: Description of your issue or request Job ID(s) What you tried The full error message you are recieving Any supporing files (code, submission scripts, screenshots, etc) Office Hours: \uf0c1 We also host weekly office hours. A reminder email is sent out weekly with the exact location and time. If you would like to attend office hours and don\\'t think you are getting these reminder emails, please email supercloud\\@mit.edu . SuperCloud Office hours are also listed on the ORCD webpage: https://orcd.mit.edu/orcd-public-calendar/ . The office hours are listed under: HPC Help Office Hours HPC Help Virtual Office Hours","title":"Getting Help"},{"location":"getting-help/#getting-help-getting_help","text":"","title":"Getting Help {#getting_help}"},{"location":"getting-help/#additional-documentation","text":"If you haven\\'t found your answer elsewhere in this wiki, you may find it here. We provide a list of FAQs <faq> {.interpreted-text role=\"ref\"} and external Resources .","title":"Additional Documentation:"},{"location":"getting-help/#email","text":"If none of these resources are answering your question, please contact us at supercloud\\@mit.edu . In this email, please provide, where applicable: Description of your issue or request Job ID(s) What you tried The full error message you are recieving Any supporing files (code, submission scripts, screenshots, etc)","title":"Email:"},{"location":"getting-help/#office-hours","text":"We also host weekly office hours. A reminder email is sent out weekly with the exact location and time. If you would like to attend office hours and don\\'t think you are getting these reminder emails, please email supercloud\\@mit.edu . SuperCloud Office hours are also listed on the ORCD webpage: https://orcd.mit.edu/orcd-public-calendar/ . The office hours are listed under: HPC Help Office Hours HPC Help Virtual Office Hours","title":"Office Hours:"},{"location":"getting-started/","text":"Getting Started \uf0c1 This page contains the most common steps for setting up and getting started with your SuperCloud account. We provide this page as a convenient reference to get started. To learn how to use your SuperCloud account, complete the Practical HPC course , which contains the material below and more. The course will walk you through these steps in more detail and often with videos to see how it is done. The course is self paced, can be accessed anytime, and is kept up to date. More reference material is available throughout site (some of this material links to those pages). When your account is first created you will have a small startup allocation. Once you complete the Practical HPC course you can request your account be updated to the standard allocation by sending email to supercloud@mit.edu . Resource allocations are listed on the Systems and Software page. Logging in Via ssh \uf0c1 The first thing you should do when you get a new account is verify that you can log in. The primary way to access the MIT SuperCloud system is through ssh. Instructions for different operating systems are below. Keep in mind that you will only be able to access the system from ssh from the machine where you generated your ssh key. You will not be able to log in until we have sent you an email stating that we have created your account, which will contain your username. First, add your ssh key to the web portal. Go to https://txe1-portal.mit.edu/ . If you affiliated with MIT or another institution/university select the middle option \"MIT Touchstone/InCommon Federation\" to log in. Select your institution from the dropdown (be aware they are spelled out, MIT is listed as Massachusetts Institute of Technology, for example), click \"Remember my choice\" box and then the \"Select\" button. Then log in with your institution credentials. Once you are logged in, click on the \"sshkeys\" link and paste your ssh key in the box at the bottom of the page and click \"Update Keys\". For instructions on how to generate ssh keys, retrieve your public key, and additional troubleshooting tips, watch the videos in the \"Account Setup and SSH Keys\" section in the \"Getting Started\" module of the Practical HPC course , or see this page . First open a command line terminal window where you generated your ssh keys. Enter the following command, where USERNAME is your username on the MIT SuperCloud system: ssh USERNAME@txe1-login.mit.edu If you generated your keys using PuTTY, open a PuTTY window. In the box labeled \"Host Name\" enter USERNAME@txe1-login.mit.edu , where USERNAME is your username on the MIT SuperCloud system. Keep the ssh box checked (this should be default) and Port should be set to 22. Click \"Open\" to start the session. You may also need to indicate your private key on the Connection -> SSH -> Auth page. Shared HPC Clusters \uf0c1 The MIT SuperCloud is an HPC-style shared cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended. Being a cluster, there are several machines connected together with a network. We refer to these as nodes . Most nodes in the cluster are referred to as compute nodes , this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node . The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes. Each job is started using a piece of software called the scheduler , which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler. Software and Packages \uf0c1 The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review our page on Software and Package Management , paying particular attention to the section on modules and installing packages for the language that you use. If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image. Linux Command Line \uf0c1 The MIT SuperCloud runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher. Most Linux commands deal with directories and files . A directory , synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path . In Linux, directories on a path are separated by forward slashes / . It is also important to note that everything in Linux is case sensitive, so a file myScript.sh is not the same as the file myscript.sh . When you first log in you are in you home directory . Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you. The path to your home directory on SuperCloud is /home/gridsan/[USERNAME] , where [USERNAME] is your username. The character ~ is also shorthand for your home directory in any Linux commands. Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here . Finally, below is a list of Linux Commands. Try them out for yourself at the command line. Creating, navigating and viewing directories: pwd : tells you the full path of the directory you are currently in mkdir dirname : creates a directory with the name \"dirname\" cd dirname : change directory to directory \"dirname\" cd ../ : takes you up one level ls : lists the files in the directory ls -a : lists all files including hidden files ls -l : lists files in \"long format\" including ownership and date of last update ls -t : lists files by date stamp, most recently updated file first ls -tr : lists files by dates stamp in reverse order, most recently updated file is listed last (this is useful if you have a lot of files, you want to know which file you changed last and the list of files results in a scrolling window) ls dirname : lists the files in the directory \"dirname\" Viewing files more filename : shows the first part of a file, hitting the space bar allows you to scroll through the rest of the file, q will cause you to exit out of the file. less filename : allows you to scroll through the file, forward and backward, using the arrow keys. tail filename : shows the last 10 lines of a file (useful when you are monitoring a logfile or output file to see that the values are correct) t ail <number> filename : show you the last <number> lines of a file. tail -f filename : shows you new lines as they are written to the end of the file. Press CMD+C or Control+C to exit. This is helpful to monitor the log file of a batch job. Copying, moving, renaming, and deleting files mv filename dirname : moves filename to directory dirname. mv filename1 filename2 : moves filename1 to filename2, in essence renames the file. The date and time are not changed by the mv command. cp filename dirname : copies to directory dirname. cp filename1 filename2 : copies filename1 to filename2. The date stamp on filename2 will be the date/time that the file was moved cp -r dirname1 dirname2 : copies directory dirname1 and its contents to dirname2. rm filename : removes (deletes) the file Transferring Files to MIT SuperCloud \uf0c1 One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands scp or rsync. Read the page on Transferring Files to learn how to use these commands and transfer what you need to your home directory. Testing your Code \uf0c1 At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command: LLsub -i After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, or uses a lot of memory, you should request a full node to be sure you don't impact other jobs on the system: LLsub -i full These commands by no means represent the full use of the system, and most likely won't be the primary way you run your code. In our tutorial we go over much more on how to submit jobs and will make sure you have the tools you need to get the most out of the MIT SuperCloud. SuperCloud Downtimes \uf0c1 Note that SuperCloud has Monthly Downtimes which are scheduled for the Third Thursday of each month. During downtimes the system is not available. Downtimes usually last about a day and emails are sent when they are complete. We also send out a reminder email a few days before each downtime.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This page contains the most common steps for setting up and getting started with your SuperCloud account. We provide this page as a convenient reference to get started. To learn how to use your SuperCloud account, complete the Practical HPC course , which contains the material below and more. The course will walk you through these steps in more detail and often with videos to see how it is done. The course is self paced, can be accessed anytime, and is kept up to date. More reference material is available throughout site (some of this material links to those pages). When your account is first created you will have a small startup allocation. Once you complete the Practical HPC course you can request your account be updated to the standard allocation by sending email to supercloud@mit.edu . Resource allocations are listed on the Systems and Software page.","title":"Getting Started"},{"location":"getting-started/#logging-in-via-ssh","text":"The first thing you should do when you get a new account is verify that you can log in. The primary way to access the MIT SuperCloud system is through ssh. Instructions for different operating systems are below. Keep in mind that you will only be able to access the system from ssh from the machine where you generated your ssh key. You will not be able to log in until we have sent you an email stating that we have created your account, which will contain your username. First, add your ssh key to the web portal. Go to https://txe1-portal.mit.edu/ . If you affiliated with MIT or another institution/university select the middle option \"MIT Touchstone/InCommon Federation\" to log in. Select your institution from the dropdown (be aware they are spelled out, MIT is listed as Massachusetts Institute of Technology, for example), click \"Remember my choice\" box and then the \"Select\" button. Then log in with your institution credentials. Once you are logged in, click on the \"sshkeys\" link and paste your ssh key in the box at the bottom of the page and click \"Update Keys\". For instructions on how to generate ssh keys, retrieve your public key, and additional troubleshooting tips, watch the videos in the \"Account Setup and SSH Keys\" section in the \"Getting Started\" module of the Practical HPC course , or see this page . First open a command line terminal window where you generated your ssh keys. Enter the following command, where USERNAME is your username on the MIT SuperCloud system: ssh USERNAME@txe1-login.mit.edu If you generated your keys using PuTTY, open a PuTTY window. In the box labeled \"Host Name\" enter USERNAME@txe1-login.mit.edu , where USERNAME is your username on the MIT SuperCloud system. Keep the ssh box checked (this should be default) and Port should be set to 22. Click \"Open\" to start the session. You may also need to indicate your private key on the Connection -> SSH -> Auth page.","title":"Logging in Via ssh"},{"location":"getting-started/#shared-hpc-clusters","text":"The MIT SuperCloud is an HPC-style shared cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended. Being a cluster, there are several machines connected together with a network. We refer to these as nodes . Most nodes in the cluster are referred to as compute nodes , this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node . The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes. Each job is started using a piece of software called the scheduler , which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.","title":"Shared HPC Clusters"},{"location":"getting-started/#software-and-packages","text":"The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review our page on Software and Package Management , paying particular attention to the section on modules and installing packages for the language that you use. If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.","title":"Software and Packages"},{"location":"getting-started/#linux-command-line","text":"The MIT SuperCloud runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher. Most Linux commands deal with directories and files . A directory , synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path . In Linux, directories on a path are separated by forward slashes / . It is also important to note that everything in Linux is case sensitive, so a file myScript.sh is not the same as the file myscript.sh . When you first log in you are in you home directory . Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you. The path to your home directory on SuperCloud is /home/gridsan/[USERNAME] , where [USERNAME] is your username. The character ~ is also shorthand for your home directory in any Linux commands. Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here . Finally, below is a list of Linux Commands. Try them out for yourself at the command line. Creating, navigating and viewing directories: pwd : tells you the full path of the directory you are currently in mkdir dirname : creates a directory with the name \"dirname\" cd dirname : change directory to directory \"dirname\" cd ../ : takes you up one level ls : lists the files in the directory ls -a : lists all files including hidden files ls -l : lists files in \"long format\" including ownership and date of last update ls -t : lists files by date stamp, most recently updated file first ls -tr : lists files by dates stamp in reverse order, most recently updated file is listed last (this is useful if you have a lot of files, you want to know which file you changed last and the list of files results in a scrolling window) ls dirname : lists the files in the directory \"dirname\" Viewing files more filename : shows the first part of a file, hitting the space bar allows you to scroll through the rest of the file, q will cause you to exit out of the file. less filename : allows you to scroll through the file, forward and backward, using the arrow keys. tail filename : shows the last 10 lines of a file (useful when you are monitoring a logfile or output file to see that the values are correct) t ail <number> filename : show you the last <number> lines of a file. tail -f filename : shows you new lines as they are written to the end of the file. Press CMD+C or Control+C to exit. This is helpful to monitor the log file of a batch job. Copying, moving, renaming, and deleting files mv filename dirname : moves filename to directory dirname. mv filename1 filename2 : moves filename1 to filename2, in essence renames the file. The date and time are not changed by the mv command. cp filename dirname : copies to directory dirname. cp filename1 filename2 : copies filename1 to filename2. The date stamp on filename2 will be the date/time that the file was moved cp -r dirname1 dirname2 : copies directory dirname1 and its contents to dirname2. rm filename : removes (deletes) the file","title":"Linux Command Line"},{"location":"getting-started/#transferring-files-to-mit-supercloud","text":"One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands scp or rsync. Read the page on Transferring Files to learn how to use these commands and transfer what you need to your home directory.","title":"Transferring Files to MIT SuperCloud"},{"location":"getting-started/#testing-your-code","text":"At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command: LLsub -i After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, or uses a lot of memory, you should request a full node to be sure you don't impact other jobs on the system: LLsub -i full These commands by no means represent the full use of the system, and most likely won't be the primary way you run your code. In our tutorial we go over much more on how to submit jobs and will make sure you have the tools you need to get the most out of the MIT SuperCloud.","title":"Testing your Code"},{"location":"getting-started/#supercloud-downtimes","text":"Note that SuperCloud has Monthly Downtimes which are scheduled for the Third Thursday of each month. During downtimes the system is not available. Downtimes usually last about a day and emails are sent when they are complete. We also send out a reminder email a few days before each downtime.","title":"SuperCloud Downtimes"},{"location":"glossary/","text":"Glossary \uf0c1 The following is a list of commonly used terms and acronyms, and their definitions when used in the context of the MIT SuperCloud. First is a visual labeling the portions of the system with the terminology we tend to use for each piece. | Accelerators | A piece of hardware used to speed up computation, usually for a specific operation. GPUs are used as an accelerator for certain matrix operations. | Bandwidth | A theoretical measure of how much data could be transferred from source to destination in a given amount of time. | Bash shell | A specific shell and language used at the command line. This is the shell used on SuperCloud. | Bash script/Shell script | A script using bash command syntax. | Batch job | A job for running a pre-written script or executable. Resources are requested through the scheduler, the schedule allocates the resources when they are available, runs the script, and then exits. | Cluster | Many nodes connected via a fast network interconnect. | Command Line | A text-based user interface that allows a user to type commands that the computer then executes. | Compute Nodes | Nodes where the computation is done on the system (where you will run your code). Compute nodes are managed by the scheduler. | Core | A core is the smallest computation unit that can run a program. | CPU | The Central Processing Unit (CPU) is the part of a computer which executes software programs. CPU refers to an individual silicon chip, such as Intel\\'s Xeon-E5 or AMD\\'s Opteron. A CPU contains one or more cores. Also known as a processor or socket. | Data Server | Also called an Object Storage Server. A component of a parallel file system which stores all of the data of the files on the file system. | (Job) Dependency | Defer the start of a job until the specified dependencies have been satisfied completed. This is usually the completion of another job. | Distributed Memory (see \\\"Memory Models\\\") | In a distributed memory system, each CPU has its own private memory. Processes can only operate on local data. If remote data is required, the process must communicate with the remote process over an interconnect. | Downtime | A regular maintenance day during which the system is unavailable. | Environment Variable | Environment variables allow you to customize the environment in which programs run. They become part of the environment in which the programs run and can be queried by running programs. For example, you can set an environment variable to contain the path to your data files. Your running process can query this environment variable to get the location of the files. | File Permissions | Properties of a file that determine who can read, write, or execute (run) a file. | Filesystem | The system that controls how and where data is stored on storage disk. See Shared/Central Filesystem and Local Filesystem. | GPU | A Graphics Processing Unit (GPU) is a specialized device originally used to generate computer output. Each compute node can host one or more GPUs. Modern GPUs have many simple compute cores and have been used for parallel processing. | Group Shared Directory | A directory, created upon user request, where members of the group shared directory can share files with other members of the group. Since a user's home directory is accessible only to the user, a group shared directory is the only mechanism for users to share files. | GUI | Graphical User Interface- these are interfaces that allow the user to interact with a program with a mouse through visual icons, as opposed to a command line interface. | Home Directory | Where the user keeps their files. Each user has their own home directory. | HPC | High Performance Computing (HPC) refers to the practice of aggregating computing power to achieve higher performance that would not possible by using a typical computer. The community often used * concurrent computing * to mean programs running at the same time v in serial one after another. | Hub | A networking component that takes an incoming message and broadcasts it across all of the other ports of the hub. | Independent (Tasks/Processes) | Tasks/processes that can operate by themselves without needing data from another. | Interactive Job | An interactive job allows you to actually log in to a compute node. This is useful for when you need to compile software, test jobs and scripts, or run software that requires keyboard inputs and user interaction, such as a graphical interface . | Interconnects | The connections between components of the computer (this interconnect is called the System Network), and the computer to the Internet network (this interconnect is called the Network Connection). | I/O (Input/Output) | Refers operations that involve a transfer of data, particularly reading from and writing to the filesystem. | Job | A job is a separately executable unit of work whose resources are allocated and shared. Users create job submission scripts to ask the scheduler for resources (cores, a specific processor type, etc). The scheduler places the requests in a queue and allocates the requested resources. | Job Array | According to the Slurm documentation: \"Job arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily\". Job arrays are useful for applying the same processing routine to a collection of multiple inputs, data, or files. Job arrays offer a very simple way to submit a large number of independent or * High Throughput * processing jobs. | Job Slot | A computational resource unit that is roughly equivalent to a processor core. One or more job slots can be used to execute a process. | Jupyter Notebook | An interactive browser-based programming environment. | Latency | The delay before a transfer of data begins following an instruction for its transfer. | Lgpn | Average observed Load per GPU on the node. | LLGrid Beta | LLGrid Beta is a collection of software packages that are released as a beta test on the SuperCloud. The beta software packages are ones that SuperCloud users have requested but are not included in the SuperCloud system image. | LLMapReduce | A language-agnostic command for running loosely coupled or MapReduce applications. | LLx | A course platform containing online courses that use the SuperCloud system for exercises. | Lnode | Average observed Load on the node. | Local Filesystem | Each node in the cluster has its own local filesystem that is only accessible from that node. The system image and software stack is on the local filesystem. It also contains space that can be used during jobs for fast file access. | Login Node | The login node controls user access to a parallel computer. Users usually connect to login nodes via SSH to compile and debug their code, review their results, do some simple tests, and submit their interactive and batch jobs to the scheduler. | Loosely Coupled | Applications that involve an independent (map) step where the same operation can be performed by many processes on different inputs, followed by a serial step that uses the output of the first step as its input. Also called MapReduce. | Lppn | Average observed Load per process on the node. | Man page | Short for \"manual page\". Documentation for a command or program. | MapReduce | See \\\"Loosely Coupled\\\". | Mbpc | Memory Bytes Per Core. | Mbpn | Memory Bytes Per Node. | Mbpp | Memory Bytes Per Process. | Memory | See \\\"Volatile Memory\\\". Memory Models (see \\\"Distributed Memory\\\" and \\\"Shared Memory\\\") | Metadata Server | A component of a parallel file system which maintains the state of all files and folders within the file system, and the list of data servers where it can find the data for the files. | MIMO Mode | Multiple input, multiple output is an application mode for use with LLMapReduce. In MIMO mode your application iterates through multiple inputs. LLMapReduce calls and loads your application once in order to process multiple assigned inputs. | Modules | Here we are referring to \\\"environment modules\\\", but we often refer to them just as \\\"modules\\\". An open source software management tool used in most HPC facilities. Using modules enable users to selectively pick the software that they want and add them to their environment. Using the module command, you can manipulate your environment to gain access to new software or different versions of a package. | MPI | The Message Passing Interface (MPI) is a library for passing messages between processes and between compute nodes within a parallel job running on a cluster. There are a variety of open source and commercial versions of MPI that have been developed over the past several decades including mpich, OpenMPI, and Intel MPI. | Multi-Threaded | Describes an application that uses multiple threads. See \"Shared Memory\". | Ncpn | Number of hardware Cores Per Node. | Ngpn | Number of hardware GPUs Per Node. | Nnode | Number of Nodes. | Node | A stand-alone computer where jobs are run. Each node is connected to other compute nodes via a fast network interconnect. While accessible via interactive jobs, compute nodes are not meant to be accessed directly by users. | Non-Volatile Memory | Storage device where the information stored on it remains intact even when the computer is shut down or restarted, e.g., disk drives. | Np | Number of Processes = Nnode * Nppn. | Nppn | Number of Processes Per Node. | Ntpn | Number of Threads Per Node = Nppn * Ntpp. | Ntpp | Number of Threads Per Process. | Operating System (OS) | The software that manages how each of the applications running on the computer interact with the hardware of the computer to accomplish tasks. | Path | A list of directories separated by \"/\" characters that shows the location of a file or directory in the directory structure. | Absolute Path | The full path from the root of the filesystem, /. For example, the absolute path to the home directory for studentx would be: /home/gridsan/studentx. | Relative Path | The path to a file or directory from the current location. | Process | An independent computation running on a computer. Processes have their own address space and may create threads that will share their address space. Processes must use interprocess communication to communicate with other processes. | Router | A networking component that acts as a special switch that moves messages across defined network boundaries. | Rsync | A command for transferring and syncing files between systems. | Scheduler | The scheduler receives job and task execution requests from users and manages how and where they are executed across the many compute nodes in the HPC system. Before starting a job, it ensures that the needed resources are available for the job. The scheduler monitors running jobs, can stop jobs, and can provide information about completed jobs and the status of the system (e.g. what resources are currently available). | Shared/Central Filesystem | The shared filesystem is the filesystem that is available to all nodes in the cluster. Home and group directories are on the shared filesystem. | Shared Memory (see \\\"Memory Models\\\") | In a shared memory system, there is shared memory that can be simultaneously accessed by multiple CPUs in a multiprocessor CPU. Communication or data passing among threads or processes in a shared memory system is via memory. | Shell | Another term for the Linux command line interface. | SISO Mode | Single input, single output is an application mode for use with LLMapReduce. In SISO mode, your application runs on a single input. LLMapReduce calls and loads your application once in order to process one assigned input. | Slurm | Simple Linux Utility for Resource Management (SLURM) is a job scheduler which coordinates the running of many programs on a shared facility. Slurm is used on the MIT SuperCloud system. It replaced the SGE scheduler. | Socket | A computational unit packaged as one, and usually made of a single chip often called processor. Modern sockets carry many cores. | SPMD | Single Program Multiple Data | SSH | Secure Shell (SSH) is a protocol to securely access remote computers. Based on the client-server model, users with an SSH client can access a remote computer. Some operating systems such as Linux and Mac OS have a built-in SSH client and others can use one of many publicly available clients. For Windows, we recommend PuTTY or Cygwin for ssh. | SSH Keys | Credentials used as an authentication method for ssh. These come in pairs: a public and private key. Public keys are placed on the system you need to access, private keys are placed on your computer. When you ssh in the ssh program checks to see whether the public key fits your private key. | Submission/batch script | A script for submitting a batch job to the scheduler. It is a bash script that tells the scheduler how to run your job, and may include the resources you are requesting for you job. | Switch | A networking component that is more efficient than a hub. It takes an incoming network message and sends it out only onto the switch port on which its destination will be reached. Switches only transmit messages within a defined network. | Symlink | Short for symbolic link. A file that acts as a shortcut by pointing to another file or directory on the filesystem. If you are in a group you may see a symbolic link to the shared group directory in your home directory. | Terminal (Window) | A window containing a command line prompt. | Third-party software | According to Wikipedia: a third-party software component is a reusable software component developed to be either freely distributed or sold by an entity other than the original vendor of the development platform. Examples of third-party software on the SuperCloud system include MATLAB and TensorFlow. | Thread | Threads are lightweight processes which exist within a single operating system process. Threads share the address space of the process that created them and can communicate directly with other threads in the same process. | Throughput | An actual measure of how much data is successfully transferred from source to destination in a given amount of time. | Throughput (Workflow) | A throughput application is one that is fully independent. Often this means it is running the same operation on a number of different inputs or parameters, and the result of an operation on one input does not depend on the result of another. | Triples (Mode) | A job submission mode that allows you to request resources in a triple: Number of Nodes, Number of Processes per Node, and Number of Threads per Process. Available for LLsub job arrays, LLMapReduce, and pMatlab jobs. | Ubpn | Average observed Used bytes per node. | Ubpp | Average observed Used bytes per process. | Unix, Linux | Unix is a family of portable, multi-tasking, multi-user operating systems. Linux is an open source, Unix-like operating system that is derived from Unix. The SuperCloud system runs the Ubuntu version of the Linux operating system. | User space | User space is a set of locations where normal user processes (i.e. everything other than the kernel, the lowest part of the operating system) run. | Volatile Memory | Storage device where applications and data are loaded so that the processors can actively work with them, e.g. RAM and cache. The information stored on it does not remain intact when the computer is shut down or restarted. | Web Portal | A web page for SuperCloud where you can access your SuperCloud account. On the Web Portal you can add ssh keys, access the files in your home directory, and start Jupyter Notebooks.","title":"Glossary"},{"location":"glossary/#glossary","text":"The following is a list of commonly used terms and acronyms, and their definitions when used in the context of the MIT SuperCloud. First is a visual labeling the portions of the system with the terminology we tend to use for each piece. | Accelerators | A piece of hardware used to speed up computation, usually for a specific operation. GPUs are used as an accelerator for certain matrix operations. | Bandwidth | A theoretical measure of how much data could be transferred from source to destination in a given amount of time. | Bash shell | A specific shell and language used at the command line. This is the shell used on SuperCloud. | Bash script/Shell script | A script using bash command syntax. | Batch job | A job for running a pre-written script or executable. Resources are requested through the scheduler, the schedule allocates the resources when they are available, runs the script, and then exits. | Cluster | Many nodes connected via a fast network interconnect. | Command Line | A text-based user interface that allows a user to type commands that the computer then executes. | Compute Nodes | Nodes where the computation is done on the system (where you will run your code). Compute nodes are managed by the scheduler. | Core | A core is the smallest computation unit that can run a program. | CPU | The Central Processing Unit (CPU) is the part of a computer which executes software programs. CPU refers to an individual silicon chip, such as Intel\\'s Xeon-E5 or AMD\\'s Opteron. A CPU contains one or more cores. Also known as a processor or socket. | Data Server | Also called an Object Storage Server. A component of a parallel file system which stores all of the data of the files on the file system. | (Job) Dependency | Defer the start of a job until the specified dependencies have been satisfied completed. This is usually the completion of another job. | Distributed Memory (see \\\"Memory Models\\\") | In a distributed memory system, each CPU has its own private memory. Processes can only operate on local data. If remote data is required, the process must communicate with the remote process over an interconnect. | Downtime | A regular maintenance day during which the system is unavailable. | Environment Variable | Environment variables allow you to customize the environment in which programs run. They become part of the environment in which the programs run and can be queried by running programs. For example, you can set an environment variable to contain the path to your data files. Your running process can query this environment variable to get the location of the files. | File Permissions | Properties of a file that determine who can read, write, or execute (run) a file. | Filesystem | The system that controls how and where data is stored on storage disk. See Shared/Central Filesystem and Local Filesystem. | GPU | A Graphics Processing Unit (GPU) is a specialized device originally used to generate computer output. Each compute node can host one or more GPUs. Modern GPUs have many simple compute cores and have been used for parallel processing. | Group Shared Directory | A directory, created upon user request, where members of the group shared directory can share files with other members of the group. Since a user's home directory is accessible only to the user, a group shared directory is the only mechanism for users to share files. | GUI | Graphical User Interface- these are interfaces that allow the user to interact with a program with a mouse through visual icons, as opposed to a command line interface. | Home Directory | Where the user keeps their files. Each user has their own home directory. | HPC | High Performance Computing (HPC) refers to the practice of aggregating computing power to achieve higher performance that would not possible by using a typical computer. The community often used * concurrent computing * to mean programs running at the same time v in serial one after another. | Hub | A networking component that takes an incoming message and broadcasts it across all of the other ports of the hub. | Independent (Tasks/Processes) | Tasks/processes that can operate by themselves without needing data from another. | Interactive Job | An interactive job allows you to actually log in to a compute node. This is useful for when you need to compile software, test jobs and scripts, or run software that requires keyboard inputs and user interaction, such as a graphical interface . | Interconnects | The connections between components of the computer (this interconnect is called the System Network), and the computer to the Internet network (this interconnect is called the Network Connection). | I/O (Input/Output) | Refers operations that involve a transfer of data, particularly reading from and writing to the filesystem. | Job | A job is a separately executable unit of work whose resources are allocated and shared. Users create job submission scripts to ask the scheduler for resources (cores, a specific processor type, etc). The scheduler places the requests in a queue and allocates the requested resources. | Job Array | According to the Slurm documentation: \"Job arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily\". Job arrays are useful for applying the same processing routine to a collection of multiple inputs, data, or files. Job arrays offer a very simple way to submit a large number of independent or * High Throughput * processing jobs. | Job Slot | A computational resource unit that is roughly equivalent to a processor core. One or more job slots can be used to execute a process. | Jupyter Notebook | An interactive browser-based programming environment. | Latency | The delay before a transfer of data begins following an instruction for its transfer. | Lgpn | Average observed Load per GPU on the node. | LLGrid Beta | LLGrid Beta is a collection of software packages that are released as a beta test on the SuperCloud. The beta software packages are ones that SuperCloud users have requested but are not included in the SuperCloud system image. | LLMapReduce | A language-agnostic command for running loosely coupled or MapReduce applications. | LLx | A course platform containing online courses that use the SuperCloud system for exercises. | Lnode | Average observed Load on the node. | Local Filesystem | Each node in the cluster has its own local filesystem that is only accessible from that node. The system image and software stack is on the local filesystem. It also contains space that can be used during jobs for fast file access. | Login Node | The login node controls user access to a parallel computer. Users usually connect to login nodes via SSH to compile and debug their code, review their results, do some simple tests, and submit their interactive and batch jobs to the scheduler. | Loosely Coupled | Applications that involve an independent (map) step where the same operation can be performed by many processes on different inputs, followed by a serial step that uses the output of the first step as its input. Also called MapReduce. | Lppn | Average observed Load per process on the node. | Man page | Short for \"manual page\". Documentation for a command or program. | MapReduce | See \\\"Loosely Coupled\\\". | Mbpc | Memory Bytes Per Core. | Mbpn | Memory Bytes Per Node. | Mbpp | Memory Bytes Per Process. | Memory | See \\\"Volatile Memory\\\". Memory Models (see \\\"Distributed Memory\\\" and \\\"Shared Memory\\\") | Metadata Server | A component of a parallel file system which maintains the state of all files and folders within the file system, and the list of data servers where it can find the data for the files. | MIMO Mode | Multiple input, multiple output is an application mode for use with LLMapReduce. In MIMO mode your application iterates through multiple inputs. LLMapReduce calls and loads your application once in order to process multiple assigned inputs. | Modules | Here we are referring to \\\"environment modules\\\", but we often refer to them just as \\\"modules\\\". An open source software management tool used in most HPC facilities. Using modules enable users to selectively pick the software that they want and add them to their environment. Using the module command, you can manipulate your environment to gain access to new software or different versions of a package. | MPI | The Message Passing Interface (MPI) is a library for passing messages between processes and between compute nodes within a parallel job running on a cluster. There are a variety of open source and commercial versions of MPI that have been developed over the past several decades including mpich, OpenMPI, and Intel MPI. | Multi-Threaded | Describes an application that uses multiple threads. See \"Shared Memory\". | Ncpn | Number of hardware Cores Per Node. | Ngpn | Number of hardware GPUs Per Node. | Nnode | Number of Nodes. | Node | A stand-alone computer where jobs are run. Each node is connected to other compute nodes via a fast network interconnect. While accessible via interactive jobs, compute nodes are not meant to be accessed directly by users. | Non-Volatile Memory | Storage device where the information stored on it remains intact even when the computer is shut down or restarted, e.g., disk drives. | Np | Number of Processes = Nnode * Nppn. | Nppn | Number of Processes Per Node. | Ntpn | Number of Threads Per Node = Nppn * Ntpp. | Ntpp | Number of Threads Per Process. | Operating System (OS) | The software that manages how each of the applications running on the computer interact with the hardware of the computer to accomplish tasks. | Path | A list of directories separated by \"/\" characters that shows the location of a file or directory in the directory structure. | Absolute Path | The full path from the root of the filesystem, /. For example, the absolute path to the home directory for studentx would be: /home/gridsan/studentx. | Relative Path | The path to a file or directory from the current location. | Process | An independent computation running on a computer. Processes have their own address space and may create threads that will share their address space. Processes must use interprocess communication to communicate with other processes. | Router | A networking component that acts as a special switch that moves messages across defined network boundaries. | Rsync | A command for transferring and syncing files between systems. | Scheduler | The scheduler receives job and task execution requests from users and manages how and where they are executed across the many compute nodes in the HPC system. Before starting a job, it ensures that the needed resources are available for the job. The scheduler monitors running jobs, can stop jobs, and can provide information about completed jobs and the status of the system (e.g. what resources are currently available). | Shared/Central Filesystem | The shared filesystem is the filesystem that is available to all nodes in the cluster. Home and group directories are on the shared filesystem. | Shared Memory (see \\\"Memory Models\\\") | In a shared memory system, there is shared memory that can be simultaneously accessed by multiple CPUs in a multiprocessor CPU. Communication or data passing among threads or processes in a shared memory system is via memory. | Shell | Another term for the Linux command line interface. | SISO Mode | Single input, single output is an application mode for use with LLMapReduce. In SISO mode, your application runs on a single input. LLMapReduce calls and loads your application once in order to process one assigned input. | Slurm | Simple Linux Utility for Resource Management (SLURM) is a job scheduler which coordinates the running of many programs on a shared facility. Slurm is used on the MIT SuperCloud system. It replaced the SGE scheduler. | Socket | A computational unit packaged as one, and usually made of a single chip often called processor. Modern sockets carry many cores. | SPMD | Single Program Multiple Data | SSH | Secure Shell (SSH) is a protocol to securely access remote computers. Based on the client-server model, users with an SSH client can access a remote computer. Some operating systems such as Linux and Mac OS have a built-in SSH client and others can use one of many publicly available clients. For Windows, we recommend PuTTY or Cygwin for ssh. | SSH Keys | Credentials used as an authentication method for ssh. These come in pairs: a public and private key. Public keys are placed on the system you need to access, private keys are placed on your computer. When you ssh in the ssh program checks to see whether the public key fits your private key. | Submission/batch script | A script for submitting a batch job to the scheduler. It is a bash script that tells the scheduler how to run your job, and may include the resources you are requesting for you job. | Switch | A networking component that is more efficient than a hub. It takes an incoming network message and sends it out only onto the switch port on which its destination will be reached. Switches only transmit messages within a defined network. | Symlink | Short for symbolic link. A file that acts as a shortcut by pointing to another file or directory on the filesystem. If you are in a group you may see a symbolic link to the shared group directory in your home directory. | Terminal (Window) | A window containing a command line prompt. | Third-party software | According to Wikipedia: a third-party software component is a reusable software component developed to be either freely distributed or sold by an entity other than the original vendor of the development platform. Examples of third-party software on the SuperCloud system include MATLAB and TensorFlow. | Thread | Threads are lightweight processes which exist within a single operating system process. Threads share the address space of the process that created them and can communicate directly with other threads in the same process. | Throughput | An actual measure of how much data is successfully transferred from source to destination in a given amount of time. | Throughput (Workflow) | A throughput application is one that is fully independent. Often this means it is running the same operation on a number of different inputs or parameters, and the result of an operation on one input does not depend on the result of another. | Triples (Mode) | A job submission mode that allows you to request resources in a triple: Number of Nodes, Number of Processes per Node, and Number of Threads per Process. Available for LLsub job arrays, LLMapReduce, and pMatlab jobs. | Ubpn | Average observed Used bytes per node. | Ubpp | Average observed Used bytes per process. | Unix, Linux | Unix is a family of portable, multi-tasking, multi-user operating systems. Linux is an open source, Unix-like operating system that is derived from Unix. The SuperCloud system runs the Ubuntu version of the Linux operating system. | User space | User space is a set of locations where normal user processes (i.e. everything other than the kernel, the lowest part of the operating system) run. | Volatile Memory | Storage device where applications and data are loaded so that the processors can actively work with them, e.g. RAM and cache. The information stored on it does not remain intact when the computer is shut down or restarted. | Web Portal | A web page for SuperCloud where you can access your SuperCloud account. On the Web Portal you can add ssh keys, access the files in your home directory, and start Jupyter Notebooks.","title":"Glossary"},{"location":"online-courses/","text":"Online Courses \uf0c1 Some Available Online Courses \uf0c1 Practical HPC : An introductory course that: Includes an introduction to HPC, canonical HPC Workflows, and the SuperCloud system. Walks you through setting up your account, installing software, running your first test job, submitting your first batch job. Describes how to scale up efficiently and measure your performance. Mathematics of Big Data and Machine Learning : Available through OCW. Accessing the LLx Online Course Site \uf0c1 Navigate to LLx and follow the instructions below to create an account, or click \\\"Sign In\\\" if you have an account. Click the \\\"Courses\\\" tab to register for courses. Once you have logged in hover over your username in the top right corner and click on the \\\"Dashboard\\\" link to access the courses you are enrolled in. Creating an Online Course Account \uf0c1 Below, you will find instructions on registering for an online course account: To sign up for an LLx account, go to the the LLx Platform In the upper right corner, click on \"Register\". Complete the Registration Form to create your LLx account. Note, the following items are required: A valid and accessible email address Your name A public username of your choosing, it cannot include spaces A password, you can change this later Once you have completed the registration form, click on the button to create your account. Once you click on the button to create your account, you should see a message stating that an activation email has been sent to your email address. Check your email. When you receive the activation email, click on the link to activate your account. Note you will not be able to log back into the course site if you have not activated your account. Once you have activated your account you can register for courses. NOTE: If you forget your login password you may request a new password by clicking on \\\"Need help logging in?\\\" to the right above the password box. A password will be sent to your specified email. Follow the instructions for changing your password. Questions? \uf0c1 If you have any questions about our online courses or are having trouble with the platform, please contact us at llx- help@mit.edu . If you have any questions about the MIT SuperCloud System, contact us at supercloud@mit.edu .","title":"Online courses"},{"location":"online-courses/#online-courses","text":"","title":"Online Courses"},{"location":"online-courses/#some-available-online-courses","text":"Practical HPC : An introductory course that: Includes an introduction to HPC, canonical HPC Workflows, and the SuperCloud system. Walks you through setting up your account, installing software, running your first test job, submitting your first batch job. Describes how to scale up efficiently and measure your performance. Mathematics of Big Data and Machine Learning : Available through OCW.","title":"Some Available Online Courses"},{"location":"online-courses/#accessing-the-llx-online-course-site","text":"Navigate to LLx and follow the instructions below to create an account, or click \\\"Sign In\\\" if you have an account. Click the \\\"Courses\\\" tab to register for courses. Once you have logged in hover over your username in the top right corner and click on the \\\"Dashboard\\\" link to access the courses you are enrolled in.","title":"Accessing the LLx Online Course Site"},{"location":"online-courses/#creating-an-online-course-account","text":"Below, you will find instructions on registering for an online course account: To sign up for an LLx account, go to the the LLx Platform In the upper right corner, click on \"Register\". Complete the Registration Form to create your LLx account. Note, the following items are required: A valid and accessible email address Your name A public username of your choosing, it cannot include spaces A password, you can change this later Once you have completed the registration form, click on the button to create your account. Once you click on the button to create your account, you should see a message stating that an activation email has been sent to your email address. Check your email. When you receive the activation email, click on the link to activate your account. Note you will not be able to log back into the course site if you have not activated your account. Once you have activated your account you can register for courses. NOTE: If you forget your login password you may request a new password by clicking on \\\"Need help logging in?\\\" to the right above the password box. A password will be sent to your specified email. Follow the instructions for changing your password.","title":"Creating an Online Course Account"},{"location":"online-courses/#questions","text":"If you have any questions about our online courses or are having trouble with the platform, please contact us at llx- help@mit.edu . If you have any questions about the MIT SuperCloud System, contact us at supercloud@mit.edu .","title":"Questions?"},{"location":"requesting-account/","text":"Requesting an Account \uf0c1 The MIT SuperCloud is intended to support research and collaboration between MIT Lincoln Laboratory and students, faculty and researchers at MIT and other academic institutions. It is our practice to allow access from within the United States. Account Request Process \uf0c1 The account request, approval, and creation process is: Request: There are two steps to the request: Fill out all fields of our Account Request Form . In this form we ask if you are using non-public data, see why below . If you are not part of an MGHPCC institution, list your MIT or Lincoln Laboratory collaborator. Do not submit this form without answering these question, it will cause significant delay in the process. If you have any questions about the form, ask us by sending email to supercloud@mit.edu . Ask your faculty advisor or PI to send us a short confirmation email for your account verifying that you will be using your Supercloud account for your work. This email should be sent to supercloud@mit.edu . We will not email your advisor for you. We will not proceed with the account creation process until we receive an email from your advisor/PI. Faculty Advisor/PI Confirmation: Once we receive an email from your faculty advisor/PI we can continue the next step. This confirmation must come from a faculty member or PI on the project that you are using Supercloud for. Approval: This usually happens behind the scenes. You may receive an email with additional questions before you are approved. While you are waiting you can start learning to use your account by working through the Practical HPC course. Creation: When your account is created, you will receive an email with your username and further instructions to set up your account. When your account is first created you will have a small startup allocation. Once you complete steps 5 and 6 you can request your account be updated to the standard allocation. Set up your account: Create an ssh key and add it to your account , then make sure you can log in through ssh. The Practical HPC course also has a section with videos that walks you through this process. Learn to use your account: Work through the Practical HPC course. This course: Includes an Introduction to HPC, canonical HPC Workflows, and the SuperCloud system. Walks you through setting up your account, installing software, running your first test job, submitting your first batch job. Describes how to scale up efficiently and measure your performance. The account creation process is manual and can take approximately * two weeks *. You can make this process smoother by making sure you have fully filled out your request form before submitting it and making sure your advisor has sent us an email confirmation. While you are waiting for your account you can get a head start learning how to use the account by reviewing the Practical HPC course. Why do we ask if you are using data that is not publicly available? \uf0c1 Not all data is appropriate for Supercloud. If your data is not publicly available, we ask for any agreements or requirements you have for your data to make sure Supercloud is the right place to be putting the data. Please be as detailed as you can. To get a general idea of the sorts of data that may or may not be appropriate for Supercloud, take a look at MIT IS&T\\'s guidance for storing data in Dropbox, OneDrive, and Google Drive here . Generating ssh Keys \uf0c1 If you have any issues or questions regarding the generation of ssh keys, please contact the team at supercloud@mit.edu . To access the system you will need ssh keys. For additional security you can create a passphrase when you generate your key, which you must enter every time you log in. Since you set this yourself on your own computer, we cannot help you reset it if you forget it. If you can\\'t remember your passphrase you\\'ll have to generate a new key and re-add it using the Web Portal. If you cannot generate ssh keys on your system, let us know and we can help you. If you have no existing ssh keys, from the command line in a terminal window, follow the steps below. On Mac and Linux, open your standard terminal window. On Windows 10 and higher, you can use the Windows command prompt. If the command prompt does not recognize the ssh-keygen command, you can install OpenSSH by following the instructions on this page . If your Windows operating system is older than Windows 10, see the note below <#pre-windows10> {.interpreted-text role=\"ref\"}. If you already have ssh keys then you can use those. You will need your public key, id_rsa.pub. [user1234@yourMachine]$ ssh-keygen -t rsa You will see the following: Generating public/private rsa key pair. When answering the 3 prompts (first 3 lines) hit return to create passwordless keys and save them in the default location. Alternatively, for extra security you can create a passphrase for your key that you\\'ll have to enter every time you log in. To do this, instead of pressing \\\"enter\\\" or \\\"return\\\", enter the passphrase you\\'ve chosen when prompted. Enter file in which to save the key (/home/user1234/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user1234/.ssh/id_rsa. Your public key has been saved in /home/user1234/.ssh/id_rsa.pub. The key fingerprint is: 88:90:6a:dc:f1:bd:ed:fb:b1:aa:46:14:34:5e:b9:70 user1234@yourMachine The key's randomart image is: +--[ RSA 2048]----+ | .o .. | | . .ooE | | o. .+ . | |....o..o . | |.o ...o.S | |. .o | | .. . . | | .. o | | ...++o | +\u2014\u2014\u2014\u2014\u2014------------+ To view your public ssh key, go to your .ssh directory. [user1234@yourMachine]$ cd .ssh In ~/.ssh you would see two files id_rsa and id_rsa.pub . The id_rsa.pub file contains your public key. [user1234@yourMachine]$ ls id_rsa id_rsa.pub This is the id_rsa.pub file content after generating a public SSH key that we would require. To view it, type cat id_rsa.pub at the command line. [user1234@yourMachine]$ cat id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA1NAD8v4nFzQ6G7KIEzkDLOnlH7t/4zmw0vVXlJjjFW4kLBgLJa0tkk61jHCxO2CurDr4zdEs2NeHG9agZJgMKMJZdIVaxtPcEBVVaNutvn/ZDRe3VsrRjToKEoR0xlAUdoef++AwiwI6K6vBOGIq6whLIlY5L9tZJfaLF3xMwmQRRhf4C+al/yZ5hX7BfGba2fqZmugTPpeSbLnFMVPKK/wy6XZasBSAKgLBA141EMXIKuGrpXpxLMECPBN5GDd/xmjmD0pC2o2z5OdfdYJj/FRWL2sC8hWTZSPa4p/n7Qc9ErFW5wM7FkynwguN4t/A+QOCa+p8C/nrOcTQKugrtw user1234@yourMachine Copy the entire output, including the ssh-rsa at the beginning. Adding your SSH Keys to your Account \uf0c1 Once you have created your ssh keys and copied your public key, you can add your key to your account using the Web Portal: Go to https://txe1-portal.mit.edu . Log in. If you are an MIT affiliate or an affiliate at another university or institution you can log in with your MIT or institutional credentials. Click on MIT Touchstone/InCommon. a. Select your institution (note these are spelled out, MIT is listed as Massachusetts Institute of Technology, for example). b. Click the checkbox next to \\\"Remember my Choice\\\" and click the \\\"Select\\\" button. c. Log in with your institutional credentials. Click on the \\\"sshkeys\\\" link. Paste your public ssh key in the box at the bottom of the page, click \\\"Update Keys\\\". Verify you can log in by running ssh USERNAME@txe1-login.mit.edu in the terminal where you created your ssh keys, where USERNAME is the username we sent you in your new account email. ::: {##pre-windows10} NOTE: For other Windows users there are a number of ssh clients you can use. Some ssh clients like Moba Xterm and Cygwin give you a Linux-like environment, and so once you start the program (which should look include a command line window), you can follow the instructions for creating an ssh key in above <ssh_keys> {.interpreted-text role=\"ref\"} once you install the client. ::: To install Moba Xterm, follow the instructions here through the section \\\"Create Local Shell\\\". Anytime you are instructed to open a terminal window, you can follow the instructions to create a local shell. Once you have installed, follow the instructions above <#ssh-keys> {.interpreted-text role=\"ref\"} for creating an ssh key. Instructions for installing PuTTY are here . (Please note, the link will open in a new window.) Once PuTTY is installed please follow the instructions at this link to manually generate your ssh-keys, only follow the instructions in the \\\"Generating an SSH Key\\\" section. Current Approver List \uf0c1 Boston University: Wayne Gilmore Harvard: Scott Yockel MIT: Jeremy Kepner, Vijay Gadepally, Chris Hill, Lauren Milechin Lincoln Laboratory: Jeremy Kepner, Vijay Gadepally, Albert Reuther Northeastern: David Kaeli UMass Amherst: John Griffin UMass Dartmouth: Geoffrey Cowles UMass Lowell: Anne Maglia UMass Medical: Paul Langlois University of Rhode Island: Gaurav Khanna","title":"Requesting an Account"},{"location":"requesting-account/#requesting-an-account","text":"The MIT SuperCloud is intended to support research and collaboration between MIT Lincoln Laboratory and students, faculty and researchers at MIT and other academic institutions. It is our practice to allow access from within the United States.","title":"Requesting an Account"},{"location":"requesting-account/#account-request-process","text":"The account request, approval, and creation process is: Request: There are two steps to the request: Fill out all fields of our Account Request Form . In this form we ask if you are using non-public data, see why below . If you are not part of an MGHPCC institution, list your MIT or Lincoln Laboratory collaborator. Do not submit this form without answering these question, it will cause significant delay in the process. If you have any questions about the form, ask us by sending email to supercloud@mit.edu . Ask your faculty advisor or PI to send us a short confirmation email for your account verifying that you will be using your Supercloud account for your work. This email should be sent to supercloud@mit.edu . We will not email your advisor for you. We will not proceed with the account creation process until we receive an email from your advisor/PI. Faculty Advisor/PI Confirmation: Once we receive an email from your faculty advisor/PI we can continue the next step. This confirmation must come from a faculty member or PI on the project that you are using Supercloud for. Approval: This usually happens behind the scenes. You may receive an email with additional questions before you are approved. While you are waiting you can start learning to use your account by working through the Practical HPC course. Creation: When your account is created, you will receive an email with your username and further instructions to set up your account. When your account is first created you will have a small startup allocation. Once you complete steps 5 and 6 you can request your account be updated to the standard allocation. Set up your account: Create an ssh key and add it to your account , then make sure you can log in through ssh. The Practical HPC course also has a section with videos that walks you through this process. Learn to use your account: Work through the Practical HPC course. This course: Includes an Introduction to HPC, canonical HPC Workflows, and the SuperCloud system. Walks you through setting up your account, installing software, running your first test job, submitting your first batch job. Describes how to scale up efficiently and measure your performance. The account creation process is manual and can take approximately * two weeks *. You can make this process smoother by making sure you have fully filled out your request form before submitting it and making sure your advisor has sent us an email confirmation. While you are waiting for your account you can get a head start learning how to use the account by reviewing the Practical HPC course.","title":"Account Request Process"},{"location":"requesting-account/#why-do-we-ask-if-you-are-using-data-that-is-not-publicly-available","text":"Not all data is appropriate for Supercloud. If your data is not publicly available, we ask for any agreements or requirements you have for your data to make sure Supercloud is the right place to be putting the data. Please be as detailed as you can. To get a general idea of the sorts of data that may or may not be appropriate for Supercloud, take a look at MIT IS&T\\'s guidance for storing data in Dropbox, OneDrive, and Google Drive here .","title":"Why do we ask if you are using data that is not publicly available?"},{"location":"requesting-account/#generating-ssh-keys","text":"If you have any issues or questions regarding the generation of ssh keys, please contact the team at supercloud@mit.edu . To access the system you will need ssh keys. For additional security you can create a passphrase when you generate your key, which you must enter every time you log in. Since you set this yourself on your own computer, we cannot help you reset it if you forget it. If you can\\'t remember your passphrase you\\'ll have to generate a new key and re-add it using the Web Portal. If you cannot generate ssh keys on your system, let us know and we can help you. If you have no existing ssh keys, from the command line in a terminal window, follow the steps below. On Mac and Linux, open your standard terminal window. On Windows 10 and higher, you can use the Windows command prompt. If the command prompt does not recognize the ssh-keygen command, you can install OpenSSH by following the instructions on this page . If your Windows operating system is older than Windows 10, see the note below <#pre-windows10> {.interpreted-text role=\"ref\"}. If you already have ssh keys then you can use those. You will need your public key, id_rsa.pub. [user1234@yourMachine]$ ssh-keygen -t rsa You will see the following: Generating public/private rsa key pair. When answering the 3 prompts (first 3 lines) hit return to create passwordless keys and save them in the default location. Alternatively, for extra security you can create a passphrase for your key that you\\'ll have to enter every time you log in. To do this, instead of pressing \\\"enter\\\" or \\\"return\\\", enter the passphrase you\\'ve chosen when prompted. Enter file in which to save the key (/home/user1234/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user1234/.ssh/id_rsa. Your public key has been saved in /home/user1234/.ssh/id_rsa.pub. The key fingerprint is: 88:90:6a:dc:f1:bd:ed:fb:b1:aa:46:14:34:5e:b9:70 user1234@yourMachine The key's randomart image is: +--[ RSA 2048]----+ | .o .. | | . .ooE | | o. .+ . | |....o..o . | |.o ...o.S | |. .o | | .. . . | | .. o | | ...++o | +\u2014\u2014\u2014\u2014\u2014------------+ To view your public ssh key, go to your .ssh directory. [user1234@yourMachine]$ cd .ssh In ~/.ssh you would see two files id_rsa and id_rsa.pub . The id_rsa.pub file contains your public key. [user1234@yourMachine]$ ls id_rsa id_rsa.pub This is the id_rsa.pub file content after generating a public SSH key that we would require. To view it, type cat id_rsa.pub at the command line. [user1234@yourMachine]$ cat id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA1NAD8v4nFzQ6G7KIEzkDLOnlH7t/4zmw0vVXlJjjFW4kLBgLJa0tkk61jHCxO2CurDr4zdEs2NeHG9agZJgMKMJZdIVaxtPcEBVVaNutvn/ZDRe3VsrRjToKEoR0xlAUdoef++AwiwI6K6vBOGIq6whLIlY5L9tZJfaLF3xMwmQRRhf4C+al/yZ5hX7BfGba2fqZmugTPpeSbLnFMVPKK/wy6XZasBSAKgLBA141EMXIKuGrpXpxLMECPBN5GDd/xmjmD0pC2o2z5OdfdYJj/FRWL2sC8hWTZSPa4p/n7Qc9ErFW5wM7FkynwguN4t/A+QOCa+p8C/nrOcTQKugrtw user1234@yourMachine Copy the entire output, including the ssh-rsa at the beginning.","title":"Generating ssh Keys"},{"location":"requesting-account/#adding-your-ssh-keys-to-your-account","text":"Once you have created your ssh keys and copied your public key, you can add your key to your account using the Web Portal: Go to https://txe1-portal.mit.edu . Log in. If you are an MIT affiliate or an affiliate at another university or institution you can log in with your MIT or institutional credentials. Click on MIT Touchstone/InCommon. a. Select your institution (note these are spelled out, MIT is listed as Massachusetts Institute of Technology, for example). b. Click the checkbox next to \\\"Remember my Choice\\\" and click the \\\"Select\\\" button. c. Log in with your institutional credentials. Click on the \\\"sshkeys\\\" link. Paste your public ssh key in the box at the bottom of the page, click \\\"Update Keys\\\". Verify you can log in by running ssh USERNAME@txe1-login.mit.edu in the terminal where you created your ssh keys, where USERNAME is the username we sent you in your new account email. ::: {##pre-windows10} NOTE: For other Windows users there are a number of ssh clients you can use. Some ssh clients like Moba Xterm and Cygwin give you a Linux-like environment, and so once you start the program (which should look include a command line window), you can follow the instructions for creating an ssh key in above <ssh_keys> {.interpreted-text role=\"ref\"} once you install the client. ::: To install Moba Xterm, follow the instructions here through the section \\\"Create Local Shell\\\". Anytime you are instructed to open a terminal window, you can follow the instructions to create a local shell. Once you have installed, follow the instructions above <#ssh-keys> {.interpreted-text role=\"ref\"} for creating an ssh key. Instructions for installing PuTTY are here . (Please note, the link will open in a new window.) Once PuTTY is installed please follow the instructions at this link to manually generate your ssh-keys, only follow the instructions in the \\\"Generating an SSH Key\\\" section.","title":"Adding your SSH Keys to your Account"},{"location":"requesting-account/#current-approver-list","text":"Boston University: Wayne Gilmore Harvard: Scott Yockel MIT: Jeremy Kepner, Vijay Gadepally, Chris Hill, Lauren Milechin Lincoln Laboratory: Jeremy Kepner, Vijay Gadepally, Albert Reuther Northeastern: David Kaeli UMass Amherst: John Griffin UMass Dartmouth: Geoffrey Cowles UMass Lowell: Anne Maglia UMass Medical: Paul Langlois University of Rhode Island: Gaurav Khanna","title":"Current Approver List"},{"location":"systems-and-software/","text":"Systems and Software {#systems_and_software} \uf0c1 This page lists information about the system and available software <#software> {.interpreted-text role=\"ref\"}, languages <#languages> {.interpreted-text role=\"ref\"}, compilers <#compilers> {.interpreted-text role=\"ref\"}, modules <#avail-modules> {.interpreted-text role=\"ref\"}, etc. This is only a partial list, so if there is anything you are interested in that isn\\'t listed here, please contact us . MGHPCC TX-E1 Specifications \uf0c1 Summary Number of Nodes 704 Total CPU Cores 32000 Total GPUs 448 Distributed Storage 873 TB Individual s Node Processor Nodes CPU Node RAM/core GPU Type GPU GPUs/node Local Cores RAM RAM Disk Intel Xeon 480 48 192 GB 4 GB NA NA NA 4.4 TB Platinum 8260 Intel Xeon 224 40 384 GB 9 GB NVidia 32 GB 2 3.8 TB Gold 6248 Volta V100 Resource Alloca tions Processor Partition Starting Standard Intel Xeon xeon-p8 2 Nodes (96 16 Nodes (768 Platinum 8260 Cores) Cores) Intel Xeon Gold xeon-g6-volta 1 Node (40 6 Nodes (240 6248 Cores 2 Cores 12 GPUs) GPUs) Available Languages {##languages} \uf0c1 Julia Python (Anaconda available through modules) Matlab(R)/Octave R C/C++ Fortran Java Perl 5 Ruby Modules {##avail-modules} \uf0c1 To see the most up-to-date list of currently available modules, run the command module avail . For more information about modules, see the module section <#modules> {.interpreted-text role=\"ref\"} on the Software and Package Management page. Software {##software} \uf0c1 Machine Learning Tools \uf0c1 Tensorflow Pytorch Big Data Software Stack \uf0c1 Hadoop Zookeeper Accumulo Middleware Software Stack \uf0c1 ARPACK ATLAS Boost BLAS FFTW LAPAC OpenMPI OpenBLAS Lincoln Laboratory Developed Software \uf0c1 pMatlab D4M Graphulo LLMapReduce Compilers {##compilers} \uf0c1 gcc g++ gfortran icc","title":"Systems and Software"},{"location":"systems-and-software/#systems-and-software-systems_and_software","text":"This page lists information about the system and available software <#software> {.interpreted-text role=\"ref\"}, languages <#languages> {.interpreted-text role=\"ref\"}, compilers <#compilers> {.interpreted-text role=\"ref\"}, modules <#avail-modules> {.interpreted-text role=\"ref\"}, etc. This is only a partial list, so if there is anything you are interested in that isn\\'t listed here, please contact us .","title":"Systems and Software {#systems_and_software}"},{"location":"systems-and-software/#mghpcc-tx-e1-specifications","text":"Summary Number of Nodes 704 Total CPU Cores 32000 Total GPUs 448 Distributed Storage 873 TB Individual s Node Processor Nodes CPU Node RAM/core GPU Type GPU GPUs/node Local Cores RAM RAM Disk Intel Xeon 480 48 192 GB 4 GB NA NA NA 4.4 TB Platinum 8260 Intel Xeon 224 40 384 GB 9 GB NVidia 32 GB 2 3.8 TB Gold 6248 Volta V100 Resource Alloca tions Processor Partition Starting Standard Intel Xeon xeon-p8 2 Nodes (96 16 Nodes (768 Platinum 8260 Cores) Cores) Intel Xeon Gold xeon-g6-volta 1 Node (40 6 Nodes (240 6248 Cores 2 Cores 12 GPUs) GPUs)","title":"MGHPCC TX-E1 Specifications"},{"location":"systems-and-software/#available-languages-languages","text":"Julia Python (Anaconda available through modules) Matlab(R)/Octave R C/C++ Fortran Java Perl 5 Ruby","title":"Available Languages {##languages}"},{"location":"systems-and-software/#modules-avail-modules","text":"To see the most up-to-date list of currently available modules, run the command module avail . For more information about modules, see the module section <#modules> {.interpreted-text role=\"ref\"} on the Software and Package Management page.","title":"Modules {##avail-modules}"},{"location":"systems-and-software/#software-software","text":"","title":"Software {##software}"},{"location":"systems-and-software/#machine-learning-tools","text":"Tensorflow Pytorch","title":"Machine Learning Tools"},{"location":"systems-and-software/#big-data-software-stack","text":"Hadoop Zookeeper Accumulo","title":"Big Data Software Stack"},{"location":"systems-and-software/#middleware-software-stack","text":"ARPACK ATLAS Boost BLAS FFTW LAPAC OpenMPI OpenBLAS","title":"Middleware Software Stack"},{"location":"systems-and-software/#lincoln-laboratory-developed-software","text":"pMatlab D4M Graphulo LLMapReduce","title":"Lincoln Laboratory Developed Software"},{"location":"systems-and-software/#compilers-compilers","text":"gcc g++ gfortran icc","title":"Compilers {##compilers}"},{"location":"tensorboard/","text":"| You can run Tensorboard as a job, this is the preferred method of doing this. | | First start an interactive session with a reserved port: | [studentx@login-1 ~]$ LLsub -i --resv-ports 1 salloc --immediate=60 -p normal --constraint=xeon-e5 --cpus-per-task=4 --qos=high srun --resv-ports=1 --pty bash -i salloc: Granted job allocation 355286 salloc: Waiting for resource configuration salloc: Nodes node-052 are ready for job | Then create your logging directory in TMPDIR: | [studentx@node-052 ~]$ mkdir -p ${TMPDIR}/tensorboard | Set up your forwading name and file: | [studentx@node-052 ~]$ PORTAL_FWNAME=\"$(id -un | tr '[A-Z]' '[a-z]')-tensorboard\" [studentx@node-052 ~]$ PORTAL_FWFILE=\"/home/gridsan/portal-url-fw/${PORTAL_FWNAME}\" [studentx@node-052 ~]$ echo $PORTAL_FWFILE /home/gridsan/portal-url-fw/studentx-tensorboard [studentx@node-052 ~]$ echo \"Portal URL is: https://${PORTAL_FWNAME}.fn.txe1-portal.mit.edu/\" Portal URL is: https://studentx-tensorboard.fn.txe1-portal.mit.edu/ | | Put the forward URL in the forwarding file (when you run \"cat \\$PORTAL_FWFILE\" you should only see one line- if you see two or more, delete all but the last line): | [studentx@node-052 ~]$ echo \"http://$(hostname -s):${SLURM_STEP_RESV_PORTS}/\" >> $PORTAL_FWFILE [studentx@node-052 ~]$ cat $PORTAL_FWFILE http://node-052:12637/ | Set the permissions on the forward file properly: | [studentx@node-052 ~]$ chmod u+x ${PORTAL_FWFILE} | Load an anaconda module and start tensorboard | [studentx@node-052 ~]$ module load anaconda/2020a [studentx@node-052 ~]$ tensorboard --logdir ${TMPDIR}/tensorboard --host \"$(hostname -s)\" --port ${SLURM_STEP_RESV_PORTS} In the browser, go to the URL listed above (for example, mine is https://studentx-tensorboard.fn.txe1-portal.mit.edu/ )","title":"Tensorboard"},{"location":"using-the-system/","text":"Using the Systetm {#using_the_system} \uf0c1 ::: {.toctree maxdepth=\"2\"} files_and_data/index ::: ::: {.toctree maxdepth=\"1\"} software_packages/index monitoring_system_and_jobs/index ::: ::: {.toctree maxdepth=\"2\"} submitting_jobs/index best_practices/index ::: ::: {.toctree maxdepth=\"1\"} web_portal/index databases/index jupyter_notebooks/index :::","title":"Index"},{"location":"using-the-system/#using-the-systetm-using_the_system","text":"::: {.toctree maxdepth=\"2\"} files_and_data/index ::: ::: {.toctree maxdepth=\"1\"} software_packages/index monitoring_system_and_jobs/index ::: ::: {.toctree maxdepth=\"2\"} submitting_jobs/index best_practices/index ::: ::: {.toctree maxdepth=\"1\"} web_portal/index databases/index jupyter_notebooks/index :::","title":"Using the Systetm {#using_the_system}"},{"location":"using-the-system/databases/","text":"Databases \uf0c1 The MIT Supercloud allows users to launch their own databases through the database portal. The portal is located at: https://txe1-portal.mit.edu/db/dbstatus.php This page requires you to authenticate into the portal <web_portal> {.interpreted-text role=\"ref\"}. From here you will see all the databases you have access to. There are Accumulo and Postgress databases available. To start up a database instance, press \\\"Start\\\". You can stop it by clicking on the \\\"Stop\\\" button, and can checkpoint a stopped database as well. Clicking on \\\"View Info\\\" for Accumulo databases will take you to the Accumulo Monitoring page, where you can view ingest/query plots and current tables. If you need an instance created and cannot use one of those already available, contact us . Let us know the type of database you need, what you are using it for, what it should be called, and who should have access to the database. Let us know if there are any special configurations you need. There are many ways to insert and query data. While this cannot be done from the portal page, you can query and insert data from any of the nodes on the MIT Supercloud system. We reccommed using D4M, which has been installed and pre-configured to work on our system with very little effort. For more information about databases and how to use them with D4M, take a look at the Advanced Database Technologies course on our online course platform. This course contains a good introduction on how to set up a data pipeline, including parsing, ingesting, and querying data for Accumulo.","title":"Databases"},{"location":"using-the-system/databases/#databases","text":"The MIT Supercloud allows users to launch their own databases through the database portal. The portal is located at: https://txe1-portal.mit.edu/db/dbstatus.php This page requires you to authenticate into the portal <web_portal> {.interpreted-text role=\"ref\"}. From here you will see all the databases you have access to. There are Accumulo and Postgress databases available. To start up a database instance, press \\\"Start\\\". You can stop it by clicking on the \\\"Stop\\\" button, and can checkpoint a stopped database as well. Clicking on \\\"View Info\\\" for Accumulo databases will take you to the Accumulo Monitoring page, where you can view ingest/query plots and current tables. If you need an instance created and cannot use one of those already available, contact us . Let us know the type of database you need, what you are using it for, what it should be called, and who should have access to the database. Let us know if there are any special configurations you need. There are many ways to insert and query data. While this cannot be done from the portal page, you can query and insert data from any of the nodes on the MIT Supercloud system. We reccommed using D4M, which has been installed and pre-configured to work on our system with very little effort. For more information about databases and how to use them with D4M, take a look at the Advanced Database Technologies course on our online course platform. This course contains a good introduction on how to set up a data pipeline, including parsing, ingesting, and querying data for Accumulo.","title":"Databases"},{"location":"using-the-system/jupyter-notebooks/","text":"Jupyter Notebooks {#jupyter} \uf0c1 Jupyter notebooks are an interactive IDE environment. The environment allows you to start up notebooks in a variety of languages (Python, Matlab, Octave, Julia), terminal windows, and a simple text editor. You can also download and upload files to your home directory through this interface. This page will describe how to start up <#starting-jupyter> {.interpreted-text role=\"ref\"} and use <#about-jupyter> {.interpreted-text role=\"ref\"} your own Jupyter instance and how to set environment variables for Jupyter <#jupyter-env> {.interpreted-text role=\"ref\"}. Starting Up Jupyter {##starting-jupyter} \uf0c1 When you start up Jupyter, the Jupyter instance gets submitted through the scheduler as a job. To do this, navigate to the following page: https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php To launch a notebook with default options, you can simply click on the \\\"Launch Notebook\\\" button. To see what the default options are or to change these options, click \\\"Show Advanced Launch Options\\\". A form will appear where you can select alternative options. Partitions: The partition the job is submitted to. You do not have to change this option, it will adjust based on the resources you select. CPU Type: The type of CPU you would like to launch to. For Jupyter it doesn\\'t make a big difference which you choose, however occasionally one CPU type has more resources available than another (you can use the LLfree command in a terminal to check). GPU Resource Flag: If you would like to allocate GPUs to your Jupyter instance you can select the GPU type here. Note your code or packages must take advantage of GPUs, otherwise they will sit idle if you request them. GPU Resource Count: If you select a GPU the number of GPUs to allocate to your Jupyter instance. ncpu: The number of CPUs or cores allocated to your notebook. The number of CPUs your request is limited to the number of CPUs on the CPU type you have selected. exclusive: Check this box if you would like exclusive use of a compute node. You will be allocated all the CPUs on the node. Time limit: Select how long you would like your notebook to run for. Anaconda/Python Version: Select the Anaconda and/or Python version that you want. Note some languages are not available on all Anaconda versions. Refer to our How To pages to see which versions you should select for the language you want to use. Application: Choose between Jupyter Notebook and Jupyter Lab. They both have the same features but with different layouts. Jupyter Notebook is the stable production application, Jupyter Lab is a beta application. The application you choose is personal preference. In this course we will be showing examples using Jupyter Notebook. Once you click on the \\\"Launch\\\" button, the scheduler launches your job, if the resources are available. When your Jupyter instance is ready, a link will appear on the page. This can take a minute or so. One very important thing to note is that once your Jupyter instance has launched, it will continue to run until you stop the job. This is particularly important if you are using a lot of resources when launching your job. Stopping the job can be done by clicking on the \\\"Shutdown\\\" button in the top right corner of the Jupyter interface page, by navigating back to the inital launch page and clicking the \\\"Shutdown\\\" button, or by using one of the scheduler commands (LLkill). The Jupyter Environment {##about-jupyter} \uf0c1 When you first enter the Jupyter environment, you will see the files and directories in your home directory. You can navigate through these by clicking. By clicking the \\\"New\\\" button in the top right, you can: Start a new notebook (Julia, Python, Matlab, Octave, R) Open a terminal Create a new text file with a simple text editor In addition to these, you can upload and download data, and edit text files from Jupyter. Click on the checkbox next to the file you want to edit, and go to the top of the page and click \\\"Edit\\\". This will bring you to a simple text editor. For a bit of an intro to the Jupyter interface, see this page on Notebook Basics . A Note on Environment Variables and Modules {##jupyter-env} \uf0c1 Environment variables you have defined may not be set in the Jupyter environment, and modules you need may not be loaded. You can add these to the file ~/.jupyter/llsc_notebook_bashrc , then shutdown and restart your Jupyter instance on the Jupyter portal ( https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php ). You can check that your environment is set the way you need it by opening a terminal and running the env command or by running the module list command to check which moduels are loaded. Note that the Anaconda module is automatically loaded when starting up Jupyter, so you do not need to load it, and loading multiple Ananconda modules may cause unexpected behavior.","title":"Jupyter Notebooks"},{"location":"using-the-system/jupyter-notebooks/#jupyter-notebooks-jupyter","text":"Jupyter notebooks are an interactive IDE environment. The environment allows you to start up notebooks in a variety of languages (Python, Matlab, Octave, Julia), terminal windows, and a simple text editor. You can also download and upload files to your home directory through this interface. This page will describe how to start up <#starting-jupyter> {.interpreted-text role=\"ref\"} and use <#about-jupyter> {.interpreted-text role=\"ref\"} your own Jupyter instance and how to set environment variables for Jupyter <#jupyter-env> {.interpreted-text role=\"ref\"}.","title":"Jupyter Notebooks {#jupyter}"},{"location":"using-the-system/jupyter-notebooks/#starting-up-jupyter-starting-jupyter","text":"When you start up Jupyter, the Jupyter instance gets submitted through the scheduler as a job. To do this, navigate to the following page: https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php To launch a notebook with default options, you can simply click on the \\\"Launch Notebook\\\" button. To see what the default options are or to change these options, click \\\"Show Advanced Launch Options\\\". A form will appear where you can select alternative options. Partitions: The partition the job is submitted to. You do not have to change this option, it will adjust based on the resources you select. CPU Type: The type of CPU you would like to launch to. For Jupyter it doesn\\'t make a big difference which you choose, however occasionally one CPU type has more resources available than another (you can use the LLfree command in a terminal to check). GPU Resource Flag: If you would like to allocate GPUs to your Jupyter instance you can select the GPU type here. Note your code or packages must take advantage of GPUs, otherwise they will sit idle if you request them. GPU Resource Count: If you select a GPU the number of GPUs to allocate to your Jupyter instance. ncpu: The number of CPUs or cores allocated to your notebook. The number of CPUs your request is limited to the number of CPUs on the CPU type you have selected. exclusive: Check this box if you would like exclusive use of a compute node. You will be allocated all the CPUs on the node. Time limit: Select how long you would like your notebook to run for. Anaconda/Python Version: Select the Anaconda and/or Python version that you want. Note some languages are not available on all Anaconda versions. Refer to our How To pages to see which versions you should select for the language you want to use. Application: Choose between Jupyter Notebook and Jupyter Lab. They both have the same features but with different layouts. Jupyter Notebook is the stable production application, Jupyter Lab is a beta application. The application you choose is personal preference. In this course we will be showing examples using Jupyter Notebook. Once you click on the \\\"Launch\\\" button, the scheduler launches your job, if the resources are available. When your Jupyter instance is ready, a link will appear on the page. This can take a minute or so. One very important thing to note is that once your Jupyter instance has launched, it will continue to run until you stop the job. This is particularly important if you are using a lot of resources when launching your job. Stopping the job can be done by clicking on the \\\"Shutdown\\\" button in the top right corner of the Jupyter interface page, by navigating back to the inital launch page and clicking the \\\"Shutdown\\\" button, or by using one of the scheduler commands (LLkill).","title":"Starting Up Jupyter {##starting-jupyter}"},{"location":"using-the-system/jupyter-notebooks/#the-jupyter-environment-about-jupyter","text":"When you first enter the Jupyter environment, you will see the files and directories in your home directory. You can navigate through these by clicking. By clicking the \\\"New\\\" button in the top right, you can: Start a new notebook (Julia, Python, Matlab, Octave, R) Open a terminal Create a new text file with a simple text editor In addition to these, you can upload and download data, and edit text files from Jupyter. Click on the checkbox next to the file you want to edit, and go to the top of the page and click \\\"Edit\\\". This will bring you to a simple text editor. For a bit of an intro to the Jupyter interface, see this page on Notebook Basics .","title":"The Jupyter Environment {##about-jupyter}"},{"location":"using-the-system/jupyter-notebooks/#a-note-on-environment-variables-and-modules-jupyter-env","text":"Environment variables you have defined may not be set in the Jupyter environment, and modules you need may not be loaded. You can add these to the file ~/.jupyter/llsc_notebook_bashrc , then shutdown and restart your Jupyter instance on the Jupyter portal ( https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php ). You can check that your environment is set the way you need it by opening a terminal and running the env command or by running the module list command to check which moduels are loaded. Note that the Anaconda module is automatically loaded when starting up Jupyter, so you do not need to load it, and loading multiple Ananconda modules may cause unexpected behavior.","title":"A Note on Environment Variables and Modules {##jupyter-env}"},{"location":"using-the-system/monitoring-system-and-jobs/","text":"Monitoring System and Job Status {#monitoring_system_and_jobs} \uf0c1 The four actions you may take the most are checking system status and starting, monitoring, and stopping jobs. Since scheduling jobs is a longer topic, see this page <submitting_jobs> {.interpreted-text role=\"ref\"} for an in-depth description of how to start your job. Here we describe how to check the status of the system <#system-status> {.interpreted-text role=\"ref\"} for available resources, monitor a currently running job <#monitoring-jobs> {.interpreted-text role=\"ref\"}, and stop a running job <#stopping-jobs> {.interpreted-text role=\"ref\"}. Each of these tasks is done through the scheduler, which is Slurm on the MIT Supercloud system. On this page and the job submission page <submitting_jobs> {.interpreted-text role=\"ref\"} we describe some of the basic options for submitting, monitoring, and stopping jobs. More advanced options are described in Slurm\\'s documentation , and this handy two-page guide gives a breif description of the commands and their options. Checking System Status {##system-status} \uf0c1 Our wrapper command, LLGrid_status , has a nicely formatted and easy to read output for checking system status: | [StudentX@login-0 ~]$ LLGrid_status | LLGrid: txe1 (running slurm 16.05.8) ============================================ Online Intel xeon-e5 nodes: 36 Unclaimed nodes: 24 Claimed slots: 172 Claimed slots for exclusive jobs: 80 -------------------------------------------- Available slots: 404 In the output, you can see the name of the system you are on (e1 here), the scheduler that\\'s being used (Slurm), the number of unclaimed nodes, and the number of available slots. You can also get information about the system status using the scheduler command sinfo : [ StudentX@login-0 ~]$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST normal* up infinite 15 down* node-[039,049,051,058,063-072],raid-2 normal* up infinite 2 mix gpu-2,node-050 normal* up infinite 10 alloc gpu-1,node-[037-038,062,099,109-113] normal* up infinite 24 idle node-[040-048,055-057,059-061,115-119,124,129-130],raid-1 normal* up infinite 1 down node-114 ... gpu up infinite 1 alloc gpu-1 dgx up 1-00:00:00 1 idle txdgx This command will list the nodes that are down (unavailable), fully allocated, partially allocated, and idle. Note that the same node can appear in multiple partitions. Monitoring Jobs {##monitoring-jobs} \uf0c1 You can monitor your jobs using the LLstat command: [StudentX@login-0 ~]$ LLstat LLGrid: txe1 (running slurm 16.05.8) JOBID ARRAY_J NAME USER START_TIME PARTITION CPUS FEATURES MIN_MEMORY ST NODELIST(REASON) 40986 40986 myJob Student 2017-10-19T15:35:46 normal 1 xeon-e5 5G R gpu-2 40980_100 40980 myArrayJob Student 2017-10-19T15:35:37 normal 1 xeon-e5 5G R gpu-2 40980_101 40980 myArrayJob Student 2017-10-19T15:35:37 normal 1 xeon-e5 5G R gpu-2 40980_102 40980 myArrayJob Student 2017-10-19T15:35:37 normal 1 xeon-e5 5G R gpu-2 The output of the LLstat command lists the job IDs of the jobs running, their names, the start time, the number of cpus per task, its status, and the node that it is running on. If it is in error state, it lists that as well. Stopping Jobs {##stopping-jobs} \uf0c1 Jobs can be stopped using the LLkill command. You specify the list of job ID\\'s, separated by commas that you would like to stop, for example: LLkill 40986,40980 Stops the jobs with job IDs 40986 and 40980. You can also use the LLkill command to stop all of your currently running jobs: LLkill -u USERNAME","title":"Monintoring System and Job Status"},{"location":"using-the-system/monitoring-system-and-jobs/#monitoring-system-and-job-status-monitoring_system_and_jobs","text":"The four actions you may take the most are checking system status and starting, monitoring, and stopping jobs. Since scheduling jobs is a longer topic, see this page <submitting_jobs> {.interpreted-text role=\"ref\"} for an in-depth description of how to start your job. Here we describe how to check the status of the system <#system-status> {.interpreted-text role=\"ref\"} for available resources, monitor a currently running job <#monitoring-jobs> {.interpreted-text role=\"ref\"}, and stop a running job <#stopping-jobs> {.interpreted-text role=\"ref\"}. Each of these tasks is done through the scheduler, which is Slurm on the MIT Supercloud system. On this page and the job submission page <submitting_jobs> {.interpreted-text role=\"ref\"} we describe some of the basic options for submitting, monitoring, and stopping jobs. More advanced options are described in Slurm\\'s documentation , and this handy two-page guide gives a breif description of the commands and their options.","title":"Monitoring System and Job Status {#monitoring_system_and_jobs}"},{"location":"using-the-system/monitoring-system-and-jobs/#checking-system-status-system-status","text":"Our wrapper command, LLGrid_status , has a nicely formatted and easy to read output for checking system status: | [StudentX@login-0 ~]$ LLGrid_status | LLGrid: txe1 (running slurm 16.05.8) ============================================ Online Intel xeon-e5 nodes: 36 Unclaimed nodes: 24 Claimed slots: 172 Claimed slots for exclusive jobs: 80 -------------------------------------------- Available slots: 404 In the output, you can see the name of the system you are on (e1 here), the scheduler that\\'s being used (Slurm), the number of unclaimed nodes, and the number of available slots. You can also get information about the system status using the scheduler command sinfo : [ StudentX@login-0 ~]$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST normal* up infinite 15 down* node-[039,049,051,058,063-072],raid-2 normal* up infinite 2 mix gpu-2,node-050 normal* up infinite 10 alloc gpu-1,node-[037-038,062,099,109-113] normal* up infinite 24 idle node-[040-048,055-057,059-061,115-119,124,129-130],raid-1 normal* up infinite 1 down node-114 ... gpu up infinite 1 alloc gpu-1 dgx up 1-00:00:00 1 idle txdgx This command will list the nodes that are down (unavailable), fully allocated, partially allocated, and idle. Note that the same node can appear in multiple partitions.","title":"Checking System Status {##system-status}"},{"location":"using-the-system/monitoring-system-and-jobs/#monitoring-jobs-monitoring-jobs","text":"You can monitor your jobs using the LLstat command: [StudentX@login-0 ~]$ LLstat LLGrid: txe1 (running slurm 16.05.8) JOBID ARRAY_J NAME USER START_TIME PARTITION CPUS FEATURES MIN_MEMORY ST NODELIST(REASON) 40986 40986 myJob Student 2017-10-19T15:35:46 normal 1 xeon-e5 5G R gpu-2 40980_100 40980 myArrayJob Student 2017-10-19T15:35:37 normal 1 xeon-e5 5G R gpu-2 40980_101 40980 myArrayJob Student 2017-10-19T15:35:37 normal 1 xeon-e5 5G R gpu-2 40980_102 40980 myArrayJob Student 2017-10-19T15:35:37 normal 1 xeon-e5 5G R gpu-2 The output of the LLstat command lists the job IDs of the jobs running, their names, the start time, the number of cpus per task, its status, and the node that it is running on. If it is in error state, it lists that as well.","title":"Monitoring Jobs {##monitoring-jobs}"},{"location":"using-the-system/monitoring-system-and-jobs/#stopping-jobs-stopping-jobs","text":"Jobs can be stopped using the LLkill command. You specify the list of job ID\\'s, separated by commas that you would like to stop, for example: LLkill 40986,40980 Stops the jobs with job IDs 40986 and 40980. You can also use the LLkill command to stop all of your currently running jobs: LLkill -u USERNAME","title":"Stopping Jobs {##stopping-jobs}"},{"location":"using-the-system/software-packages/","text":"Software and Package Management {#software_and_packages} \uf0c1 The standard environment on the MIT Supercloud System is sufficient for most. If it is not, first check to see if the tool you need is included in a module. #modules {.interpreted-text role=\"ref\"} contain environment variables that set you up to use other software, packages, or compilers not included in the standard stack. If there is no module with what you need, you can often install your package or software in your home directory <#home-install> {.interpreted-text role=\"ref\"}. Below we have instructions on how to install Julia <#julia> {.interpreted-text role=\"ref\"}, Python <#python> {.interpreted-text role=\"ref\"}, and R <#R> {.interpreted-text role=\"ref\"} packages. If you have explored both these options or are having trouble, contact us . Modules {##modules} \uf0c1 Modules are a handy way to set up environment variables for particular work, especially in a shared environment. They provide an easy way to load a particular version of a language or compiler. To see what modules are available, type the command: module avail See a list currently the modules available here <#avail-modules> {.interpreted-text role=\"ref\"}. To load a module, use the command: module load moduleName Where moduleName can be any of the modules listed by the module avail command. If you want to list the modules you currently have loaded, you can use the module list command: module list If you want to change to a different version of the module you have loaded, you can switch the module you have loaded. This is important to do when loading a different version of a module you already have loaded, as environment variables from one version could interfere with those of another. To switch modules: module switch oldModuleName newModuleName Where oldModuleName is the name of the module you currently have loaded, and newModuleName is the new module that you would like to load. If you would like to unload the module, or remove the changes the module has made to your environment, use the following command: module unload moduleName Finally, in order to use the module command inside a script, you will need to initialize it first. The following shows a Bourne shell script example: #!/bin/bash # Initialize the module command first source source /etc/profile # Then use the module command to load the module needed for your work module load anaconda/2020a Installing Software or Packages in your Home Directory {##home-install} \uf0c1 Many packages and software can be installed in user space, meaning they are installed just for the user installing the package or software. The way to do this for Julia, Python, and R packages is described below. For other software, look at their installation documentation and see if they have instructions on how to install in your home directory. Sometimes this is described changing the installation location. Often you will have to download the source and build the software in your home directory to do this. Any dependencies can usually be installed in a similar way. If you run into trouble installing software you can reach out to us for help . Let us know what you have tried so far and we can often point you in the right direction. Julia Packages {##julia} \uf0c1 Adding new packages in Julia doesn\\'t require doing anything special. On the login node, load a julia module and start Julia. You can enter package mode by pressing the \\\" ] \\\" key and entering add packagename , where packagename is the name of your package. Or you can load Pkg and run Pkg.add(\"packagename\") . The easiest way to check if a package already exists is to try to load it by running using packagename . The Pkg.status() command will only show packages that you have added to your home environment. If you would like a list of the packages we have installed, the following lines should do the trick (where v1.# is your version number, for example v1.3): using Pkg; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.3\"); installed_pkgs = Pkg.installed(); Pkg.activate(DEPOT_PATH[1]*\"/environments/v1.3\"); installed_pkgs If you get an error trying to install a Julia package, first check to make sure you are on the login node, as the compute nodes don\\'t have internet access. If you are already on the login node, it is possible that the installation is filling up the /tmp directory. The errors for this can be vague and differ between the different Julia versions. You can try changing the temporary directory that Julia uses to unpackage its packages for installation by setting the $TMPDIR environment variable. You can create the new temporary directory and set the environment variable like this: mkdir /state/partition1/user/$USER export TMPDIR=/state/partition1/user/$USER Once you have done this you can start up Julia and install packages as you normally would. Once you are done it is good practice to delete these temporary files. Note: If you are using Jupyter there is an additional step you can optionally do so that Jupyter can find both our installed packages and your own. You can also run this if you are missing a Julia Kernel. First load a Julia module. Then, in a Julia shell, run: using IJulia installkernel(\"Julia MyKernel\", env=Dict(\"JULIA_LOAD_PATH\"=>ENV[\"JULIA_LOAD_PATH\"])) The first part \\\"Julia MyKernel\\\" is what you want to call your kernel, so feel free to change this. The second part makes sure both our packages and any you\\'ve installed in your home directory show up on the load path when you use a Jupyter Notebook with this kernel. Python Packages {##python} \uf0c1 Many python packages are included in the Anaconda distribution. The quickest way to check if the package you want is in our module is to load the anaconda module, start python, and try to import the package you are interested in. Importing the packages in our Anaconda modules will also be much faster than importing packages that are installed in your home directory. This is because the packages in our Anaconda modules are installed on the local disk of every node, which is faster to access than packages installed in your home directory. If the package you are looking to install is not included in Anaconda, then you can install it in user space in your home directory- this allows you to install the package for you to use without affecting other users. We recommend that you use pip to do this, as pip allows you to only install the additional packages that you need in your home directory. Conda environments will result in installing all packages in your home directory, which can slow down the import process quite a bit. Installing Packages in your Home Directory with pip \uf0c1 First, load the Anaconda module that you want to use if you haven't already: module load anaconda/2021a Here we are loading the 2021a module, the newer modules will have newer packages. Then, install the package with pip as you normally would, but with the --user flag: pip install --user packageName Where packageName is the name of the package that you are installing. If you get an error trying to install a package with pip, first check to make sure you are on the login node, as the compute nodes don\\'t have internet access. If you are already on the login node, it is possible that the installation is filling up the /tmp directory, you may get a \\\"Disk quota exceeded\\\" error. You can change the temporary directory that pip uses to unpackage its packages for installation by setting the $TMPDIR environment variable. You can create the new temporary directory, set the environment variable, and install your package like this: mkdir /state/partition1/user/$USER export TMPDIR=/state/partition1/user/$USER pip install --user --no-cache-dir packageName Once you are done it is good practice to delete these temporary files. Installing Packages with Conda \uf0c1 As mentioned above, if at all possible we recommend you install packages in your home directory with pip rather than create a conda environment, as it\\'ll be much faster. However, if you need to use a conda environment (usually this is because a package isn\\'t available through pip or to help manage complex dependencies), you can do so by loading our anaconda module (this will give you access to the \\\"conda\\\" command) and then creating an environment the same way you would anywhere else. For example: module load anaconda/2021a conda create -n my_env python=3.8 pkg1 pkg2 pkg3 In this example I am loading the anaconda/2021a module, then creating a conda environment named my_env with Python 3.8 and installing packages pkg1, pkg2, pkg3. We have found that conda creates more robust environments when you include all the packages you need when you create the environment. Then, whenever you want to activate the environment, first load the anaconda module, then activate with source activate my_env . Using source activate instead of conda activate allows you to use your conda environment at the command line and in submission scripts without additional steps. If you would like to use your conda environment in Jupyter, simply install the \\\"jupyter\\\" package into your environment. Once you have done that, you should see your conda environment listed in the available kernels. Note: If you are using a conda environment and would like to install the package with pip in that environment rather than in the standard home directory location, you should not include the --user flag. Further, if you are using a conda environment and want Python to use packages in your environment first, you can run the following two command: export PYTHONNOUSERSITE=True This will make sure your conda environment packages will be chosen before those that may be installed in your home directory. If you are using Jupyter, you will need to add this line to the .jupyter/llsc_notebook_bashrc file. See the section on the bottom of the Jupyter <jupyter> {.interpreted-text role=\"ref\"} page for more information. R Libraries {##R} \uf0c1 There are two different ways we recommend that you use R. First, is using a preset R environment that comes with the anaconda module, second would be to create your own R conda environment. This first way works best if you don't need to install any additional packages than what we already have. To use our R conda environment, log in and load an anaconda module. Then you can activate the R environment with source activate . You can see what packages are installed with the conda list command. Any packages that start with r- are R libraries. module load anaconda/2020a source activate R conda list Then you can use R as you did before. If you need to install additional packages, it's best to do it in a new conda environment. First thing to know is that many R packages are available through conda, and some are not. What you want to do is include as many R libraries that you'll need as you can when you create your conda environment- this helps avoid version conflicts. Conda r libraries are all prefixed with r- , so for example if you need rjava, you'd search for r-rjava . You can check if conda has a library with the command: conda search r-LIBNAME , where LIBNAME is the name of the library you're looking for. You'll see a lot of versions, but as long as you see something you should be good to add it. Once you have a list of all the libraries available through conda, create your conda environment (I'm calling the environment myR, feel free to change that): conda create -n myR -c conda-forge r-essentials r-LIB1 r-LIB2\u2026 Where LIB1, LIB2, etc are the additional R libraries you'd like to include. Sometimes this step takes a while. It'll tell you which new packages are going to be installed, and then you can confirm by typing y . If you have any other libraries that weren't available through conda, install them now. First activate your new environment and then start R: source activate myR R Then you can install your remaining libraries. You can do some test loads here as well, to make sure the libraries installed properly. install.packages(\u201cPKGNAME\u201d) library(PKGNAME) In Jupyter, you should see your environment show up as a kernel. For a batch job, you'll have to activate the environment either in your submission script or before you submit the job.","title":"Software and Package Management"},{"location":"using-the-system/software-packages/#software-and-package-management-software_and_packages","text":"The standard environment on the MIT Supercloud System is sufficient for most. If it is not, first check to see if the tool you need is included in a module. #modules {.interpreted-text role=\"ref\"} contain environment variables that set you up to use other software, packages, or compilers not included in the standard stack. If there is no module with what you need, you can often install your package or software in your home directory <#home-install> {.interpreted-text role=\"ref\"}. Below we have instructions on how to install Julia <#julia> {.interpreted-text role=\"ref\"}, Python <#python> {.interpreted-text role=\"ref\"}, and R <#R> {.interpreted-text role=\"ref\"} packages. If you have explored both these options or are having trouble, contact us .","title":"Software and Package Management {#software_and_packages}"},{"location":"using-the-system/software-packages/#modules-modules","text":"Modules are a handy way to set up environment variables for particular work, especially in a shared environment. They provide an easy way to load a particular version of a language or compiler. To see what modules are available, type the command: module avail See a list currently the modules available here <#avail-modules> {.interpreted-text role=\"ref\"}. To load a module, use the command: module load moduleName Where moduleName can be any of the modules listed by the module avail command. If you want to list the modules you currently have loaded, you can use the module list command: module list If you want to change to a different version of the module you have loaded, you can switch the module you have loaded. This is important to do when loading a different version of a module you already have loaded, as environment variables from one version could interfere with those of another. To switch modules: module switch oldModuleName newModuleName Where oldModuleName is the name of the module you currently have loaded, and newModuleName is the new module that you would like to load. If you would like to unload the module, or remove the changes the module has made to your environment, use the following command: module unload moduleName Finally, in order to use the module command inside a script, you will need to initialize it first. The following shows a Bourne shell script example: #!/bin/bash # Initialize the module command first source source /etc/profile # Then use the module command to load the module needed for your work module load anaconda/2020a","title":"Modules {##modules}"},{"location":"using-the-system/software-packages/#installing-software-or-packages-in-your-home-directory-home-install","text":"Many packages and software can be installed in user space, meaning they are installed just for the user installing the package or software. The way to do this for Julia, Python, and R packages is described below. For other software, look at their installation documentation and see if they have instructions on how to install in your home directory. Sometimes this is described changing the installation location. Often you will have to download the source and build the software in your home directory to do this. Any dependencies can usually be installed in a similar way. If you run into trouble installing software you can reach out to us for help . Let us know what you have tried so far and we can often point you in the right direction.","title":"Installing Software or Packages in your Home Directory {##home-install}"},{"location":"using-the-system/software-packages/#julia-packages-julia","text":"Adding new packages in Julia doesn\\'t require doing anything special. On the login node, load a julia module and start Julia. You can enter package mode by pressing the \\\" ] \\\" key and entering add packagename , where packagename is the name of your package. Or you can load Pkg and run Pkg.add(\"packagename\") . The easiest way to check if a package already exists is to try to load it by running using packagename . The Pkg.status() command will only show packages that you have added to your home environment. If you would like a list of the packages we have installed, the following lines should do the trick (where v1.# is your version number, for example v1.3): using Pkg; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.3\"); installed_pkgs = Pkg.installed(); Pkg.activate(DEPOT_PATH[1]*\"/environments/v1.3\"); installed_pkgs If you get an error trying to install a Julia package, first check to make sure you are on the login node, as the compute nodes don\\'t have internet access. If you are already on the login node, it is possible that the installation is filling up the /tmp directory. The errors for this can be vague and differ between the different Julia versions. You can try changing the temporary directory that Julia uses to unpackage its packages for installation by setting the $TMPDIR environment variable. You can create the new temporary directory and set the environment variable like this: mkdir /state/partition1/user/$USER export TMPDIR=/state/partition1/user/$USER Once you have done this you can start up Julia and install packages as you normally would. Once you are done it is good practice to delete these temporary files. Note: If you are using Jupyter there is an additional step you can optionally do so that Jupyter can find both our installed packages and your own. You can also run this if you are missing a Julia Kernel. First load a Julia module. Then, in a Julia shell, run: using IJulia installkernel(\"Julia MyKernel\", env=Dict(\"JULIA_LOAD_PATH\"=>ENV[\"JULIA_LOAD_PATH\"])) The first part \\\"Julia MyKernel\\\" is what you want to call your kernel, so feel free to change this. The second part makes sure both our packages and any you\\'ve installed in your home directory show up on the load path when you use a Jupyter Notebook with this kernel.","title":"Julia Packages {##julia}"},{"location":"using-the-system/software-packages/#python-packages-python","text":"Many python packages are included in the Anaconda distribution. The quickest way to check if the package you want is in our module is to load the anaconda module, start python, and try to import the package you are interested in. Importing the packages in our Anaconda modules will also be much faster than importing packages that are installed in your home directory. This is because the packages in our Anaconda modules are installed on the local disk of every node, which is faster to access than packages installed in your home directory. If the package you are looking to install is not included in Anaconda, then you can install it in user space in your home directory- this allows you to install the package for you to use without affecting other users. We recommend that you use pip to do this, as pip allows you to only install the additional packages that you need in your home directory. Conda environments will result in installing all packages in your home directory, which can slow down the import process quite a bit.","title":"Python Packages {##python}"},{"location":"using-the-system/software-packages/#installing-packages-in-your-home-directory-with-pip","text":"First, load the Anaconda module that you want to use if you haven't already: module load anaconda/2021a Here we are loading the 2021a module, the newer modules will have newer packages. Then, install the package with pip as you normally would, but with the --user flag: pip install --user packageName Where packageName is the name of the package that you are installing. If you get an error trying to install a package with pip, first check to make sure you are on the login node, as the compute nodes don\\'t have internet access. If you are already on the login node, it is possible that the installation is filling up the /tmp directory, you may get a \\\"Disk quota exceeded\\\" error. You can change the temporary directory that pip uses to unpackage its packages for installation by setting the $TMPDIR environment variable. You can create the new temporary directory, set the environment variable, and install your package like this: mkdir /state/partition1/user/$USER export TMPDIR=/state/partition1/user/$USER pip install --user --no-cache-dir packageName Once you are done it is good practice to delete these temporary files.","title":"Installing Packages in your Home Directory with pip"},{"location":"using-the-system/software-packages/#installing-packages-with-conda","text":"As mentioned above, if at all possible we recommend you install packages in your home directory with pip rather than create a conda environment, as it\\'ll be much faster. However, if you need to use a conda environment (usually this is because a package isn\\'t available through pip or to help manage complex dependencies), you can do so by loading our anaconda module (this will give you access to the \\\"conda\\\" command) and then creating an environment the same way you would anywhere else. For example: module load anaconda/2021a conda create -n my_env python=3.8 pkg1 pkg2 pkg3 In this example I am loading the anaconda/2021a module, then creating a conda environment named my_env with Python 3.8 and installing packages pkg1, pkg2, pkg3. We have found that conda creates more robust environments when you include all the packages you need when you create the environment. Then, whenever you want to activate the environment, first load the anaconda module, then activate with source activate my_env . Using source activate instead of conda activate allows you to use your conda environment at the command line and in submission scripts without additional steps. If you would like to use your conda environment in Jupyter, simply install the \\\"jupyter\\\" package into your environment. Once you have done that, you should see your conda environment listed in the available kernels. Note: If you are using a conda environment and would like to install the package with pip in that environment rather than in the standard home directory location, you should not include the --user flag. Further, if you are using a conda environment and want Python to use packages in your environment first, you can run the following two command: export PYTHONNOUSERSITE=True This will make sure your conda environment packages will be chosen before those that may be installed in your home directory. If you are using Jupyter, you will need to add this line to the .jupyter/llsc_notebook_bashrc file. See the section on the bottom of the Jupyter <jupyter> {.interpreted-text role=\"ref\"} page for more information.","title":"Installing Packages with Conda"},{"location":"using-the-system/software-packages/#r-libraries-r","text":"There are two different ways we recommend that you use R. First, is using a preset R environment that comes with the anaconda module, second would be to create your own R conda environment. This first way works best if you don't need to install any additional packages than what we already have. To use our R conda environment, log in and load an anaconda module. Then you can activate the R environment with source activate . You can see what packages are installed with the conda list command. Any packages that start with r- are R libraries. module load anaconda/2020a source activate R conda list Then you can use R as you did before. If you need to install additional packages, it's best to do it in a new conda environment. First thing to know is that many R packages are available through conda, and some are not. What you want to do is include as many R libraries that you'll need as you can when you create your conda environment- this helps avoid version conflicts. Conda r libraries are all prefixed with r- , so for example if you need rjava, you'd search for r-rjava . You can check if conda has a library with the command: conda search r-LIBNAME , where LIBNAME is the name of the library you're looking for. You'll see a lot of versions, but as long as you see something you should be good to add it. Once you have a list of all the libraries available through conda, create your conda environment (I'm calling the environment myR, feel free to change that): conda create -n myR -c conda-forge r-essentials r-LIB1 r-LIB2\u2026 Where LIB1, LIB2, etc are the additional R libraries you'd like to include. Sometimes this step takes a while. It'll tell you which new packages are going to be installed, and then you can confirm by typing y . If you have any other libraries that weren't available through conda, install them now. First activate your new environment and then start R: source activate myR R Then you can install your remaining libraries. You can do some test loads here as well, to make sure the libraries installed properly. install.packages(\u201cPKGNAME\u201d) library(PKGNAME) In Jupyter, you should see your environment show up as a kernel. For a batch job, you'll have to activate the environment either in your submission script or before you submit the job.","title":"R Libraries {##R}"},{"location":"using-the-system/web-portal/","text":"Web Portal {#web_portal} \uf0c1 You can get to the Supercloud Web Portal at this URL: https://txe1-portal.mit.edu On this page you will find links to a number of useful tools. These include Page to add or remove ssh keys <#ssh> {.interpreted-text role=\"ref\"} Home directory file browser <#home> {.interpreted-text role=\"ref\"} Dynamic databse launching system <databases> {.interpreted-text role=\"ref\"} Jupyter Notebook portal <jupyter> {.interpreted-text role=\"ref\"} Portal Authentication \uf0c1 There are three ways you can authenticate into the Web Portal: PKI Certificate/Smart Card, MIT Touchstone/InCommon Federation, Username and Password. MIT Touchstone/InCommon Federation \uf0c1 Most Supercloud users can log in via the MIT Touchstone/InCommon Federation link. These include users with an active MIT Kerberos account or account at most other educational institutions. When you click the \\\"MIT Touchstone/InCommon Federation\\\" link for the first time you will be taken to a page where you can select your institution. Note most options are spelled out, for example \\\"MIT\\\" is listed as \\\"Massachusetts Institute of Technology\\\". UMass Lowell users should select \\\"University of Massachusetts System\\\". When you have selected your institution, you should check the box that says \\\"Remember my Choice\\\" and click select. You\\'ll be taken to your institution\\'s login page where you can log in. Then you will be taken to the Portal main page. If you get a message saying \\\" The MIT Touchstone / InCommon Federation principal you presented is not associated with an account on this system. Please sign up at: https://supercloud.mit.edu/requesting-account. \\\" and you already have an account contact us and let us know. Username and Password \uf0c1 If you do not have an active login for an educational institution you may need to log into the portal using your username and password. Let us know and we will let you know how to do this. PKI Certificate/Smart Card \uf0c1 Those who have a Smart Card or PKI Certificate can log in using the first link on the page. If you would like to log in using your PKI Certificate or Smart Card, please let us know . Adding/Removing SSH Keys {##ssh} \uf0c1 The first link on the Portal Home page is \\\"/sshkeys/\\\". Clicking on this link takes you to a page where you can add new keys or remove old ones. To add a new key, paste the key in the box at the bottom of the page and click \\\"Update Keys\\\". You should see your new key populated in the table and display similarly to the other keys listed. To remove an old key, click the check box next to the key you would like to remove and click \\\"Update Keys\\\" at the bottom. Instructions for generating a new key are on our account request page <ssh_keys> {.interpreted-text role=\"ref\"}. Browsing your Home Directory {##home} \uf0c1 The second link on the Portal Home page is \\\"/gridsan/\\\". Click this link and then select your username to see the files in your home directory. You can click on a file to download it or click on a directory to navigate to that directory.","title":"Web Portal"},{"location":"using-the-system/web-portal/#web-portal-web_portal","text":"You can get to the Supercloud Web Portal at this URL: https://txe1-portal.mit.edu On this page you will find links to a number of useful tools. These include Page to add or remove ssh keys <#ssh> {.interpreted-text role=\"ref\"} Home directory file browser <#home> {.interpreted-text role=\"ref\"} Dynamic databse launching system <databases> {.interpreted-text role=\"ref\"} Jupyter Notebook portal <jupyter> {.interpreted-text role=\"ref\"}","title":"Web Portal {#web_portal}"},{"location":"using-the-system/web-portal/#portal-authentication","text":"There are three ways you can authenticate into the Web Portal: PKI Certificate/Smart Card, MIT Touchstone/InCommon Federation, Username and Password.","title":"Portal Authentication"},{"location":"using-the-system/web-portal/#mit-touchstoneincommon-federation","text":"Most Supercloud users can log in via the MIT Touchstone/InCommon Federation link. These include users with an active MIT Kerberos account or account at most other educational institutions. When you click the \\\"MIT Touchstone/InCommon Federation\\\" link for the first time you will be taken to a page where you can select your institution. Note most options are spelled out, for example \\\"MIT\\\" is listed as \\\"Massachusetts Institute of Technology\\\". UMass Lowell users should select \\\"University of Massachusetts System\\\". When you have selected your institution, you should check the box that says \\\"Remember my Choice\\\" and click select. You\\'ll be taken to your institution\\'s login page where you can log in. Then you will be taken to the Portal main page. If you get a message saying \\\" The MIT Touchstone / InCommon Federation principal you presented is not associated with an account on this system. Please sign up at: https://supercloud.mit.edu/requesting-account. \\\" and you already have an account contact us and let us know.","title":"MIT Touchstone/InCommon Federation"},{"location":"using-the-system/web-portal/#username-and-password","text":"If you do not have an active login for an educational institution you may need to log into the portal using your username and password. Let us know and we will let you know how to do this.","title":"Username and Password"},{"location":"using-the-system/web-portal/#pki-certificatesmart-card","text":"Those who have a Smart Card or PKI Certificate can log in using the first link on the page. If you would like to log in using your PKI Certificate or Smart Card, please let us know .","title":"PKI Certificate/Smart Card"},{"location":"using-the-system/web-portal/#addingremoving-ssh-keys-ssh","text":"The first link on the Portal Home page is \\\"/sshkeys/\\\". Clicking on this link takes you to a page where you can add new keys or remove old ones. To add a new key, paste the key in the box at the bottom of the page and click \\\"Update Keys\\\". You should see your new key populated in the table and display similarly to the other keys listed. To remove an old key, click the check box next to the key you would like to remove and click \\\"Update Keys\\\" at the bottom. Instructions for generating a new key are on our account request page <ssh_keys> {.interpreted-text role=\"ref\"}.","title":"Adding/Removing SSH Keys {##ssh}"},{"location":"using-the-system/web-portal/#browsing-your-home-directory-home","text":"The second link on the Portal Home page is \\\"/gridsan/\\\". Click this link and then select your username to see the files in your home directory. You can click on a file to download it or click on a directory to navigate to that directory.","title":"Browsing your Home Directory {##home}"},{"location":"using-the-system/best-practices/filesystem/","text":"Best Practices for Using the Filesystem {#filesystem_best_practices} \uf0c1 Installing Python Packages \uf0c1 Use our anaconda modules whenever possible. The newest anaconda module should have the most up to date versions. Our anaconda modules are on the local disk of all the nodes and so does not affect the shared filesystem. If you need to install additional Python packages, install with pip using the --user flag as described in our documentation <#home-install> {.interpreted-text role=\"ref\"}. Python will then only go to your home directory for these installed packages, and so should be less load on the shared filesystem. Only use conda environments as a last resort, as this puts ALL packages you use in your home directory, and creates many small files. DO NOT install anaconda or miniconda in your home directory. There is no reason to do this and will slow your Supercloud experience down significantly, as these installations contain many, many small files. If you absolutely need to use conda to install a package create a conda environment using our anaconda modules. If you have previously installed anaconda or miniconda in your home directory, delete it now. Submitting Jobs \uf0c1 Use Triples Mode <#triples> {.interpreted-text role=\"ref\"} for submitting Job Arrays <job_array_triples> {.interpreted-text role=\"ref\"} and LLMapReduce jobs. These create fewer log files and group them by node, reducing the number of files per directory. It is also lighter weight on the scheduler, as it creates fewer tasks/jobs that the scheduler has to keep track of. DO NOT create very large Job Arrays. Each task in a Job Array creates a log file, the more tasks in your array, the more files. The best practice is to use Triples to submit your job arrays (see bullet above). DO NOT submit many small jobs, most likely a Job Array or LLMapReduce with Triples would be appropriate. Avoid doing things that actively stress the filesystem, for example checking whether a file exists repeatedly over a long period of time or across many tasks. File Organization \uf0c1 Aim for less than \\~1000 files per directory. Fewer, larger files are better than many small files (file size should be a minimum of 1MB, target \\~100MB). Within a job, you can use \\$TMPDIR for temporary or intermediate files you don't need after the job. \\$TMPDIR points to a temporary directory on the local filesystem that is set up at the start of your job and removed at the end of your job. If your job requires a lot of I/O you may see significant performance improvement by copying the files you need to \\$TMPDIR at the start of your job and copy any new files you need to your home directory at the end of your job. Use shared directories <shared_groups> {.interpreted-text role=\"ref\"} to share data among team members rather than having a separate copy in everyone's home directory. Check /home/gridsan/groups/datasets before downloading large public datasets. If there is a public dataset that you are considering downloading that you think others may also want to use, send us a email to supercloud@mit.edu to suggest that we add it. If you are using ImageNet, we have a special setup that will stress the filesystem signficantly less and should be much faster. First load its modulefile: module load /home/gridsan/groups/datasets/ImageNet/modulefile . This will set up the $IMAGENET_PATH environment variable, which you can use in your code to point to ImageNet.","title":"Best Practices for using the Filesystem"},{"location":"using-the-system/best-practices/filesystem/#best-practices-for-using-the-filesystem-filesystem_best_practices","text":"","title":"Best Practices for Using the Filesystem {#filesystem_best_practices}"},{"location":"using-the-system/best-practices/filesystem/#installing-python-packages","text":"Use our anaconda modules whenever possible. The newest anaconda module should have the most up to date versions. Our anaconda modules are on the local disk of all the nodes and so does not affect the shared filesystem. If you need to install additional Python packages, install with pip using the --user flag as described in our documentation <#home-install> {.interpreted-text role=\"ref\"}. Python will then only go to your home directory for these installed packages, and so should be less load on the shared filesystem. Only use conda environments as a last resort, as this puts ALL packages you use in your home directory, and creates many small files. DO NOT install anaconda or miniconda in your home directory. There is no reason to do this and will slow your Supercloud experience down significantly, as these installations contain many, many small files. If you absolutely need to use conda to install a package create a conda environment using our anaconda modules. If you have previously installed anaconda or miniconda in your home directory, delete it now.","title":"Installing Python Packages"},{"location":"using-the-system/best-practices/filesystem/#submitting-jobs","text":"Use Triples Mode <#triples> {.interpreted-text role=\"ref\"} for submitting Job Arrays <job_array_triples> {.interpreted-text role=\"ref\"} and LLMapReduce jobs. These create fewer log files and group them by node, reducing the number of files per directory. It is also lighter weight on the scheduler, as it creates fewer tasks/jobs that the scheduler has to keep track of. DO NOT create very large Job Arrays. Each task in a Job Array creates a log file, the more tasks in your array, the more files. The best practice is to use Triples to submit your job arrays (see bullet above). DO NOT submit many small jobs, most likely a Job Array or LLMapReduce with Triples would be appropriate. Avoid doing things that actively stress the filesystem, for example checking whether a file exists repeatedly over a long period of time or across many tasks.","title":"Submitting Jobs"},{"location":"using-the-system/best-practices/filesystem/#file-organization","text":"Aim for less than \\~1000 files per directory. Fewer, larger files are better than many small files (file size should be a minimum of 1MB, target \\~100MB). Within a job, you can use \\$TMPDIR for temporary or intermediate files you don't need after the job. \\$TMPDIR points to a temporary directory on the local filesystem that is set up at the start of your job and removed at the end of your job. If your job requires a lot of I/O you may see significant performance improvement by copying the files you need to \\$TMPDIR at the start of your job and copy any new files you need to your home directory at the end of your job. Use shared directories <shared_groups> {.interpreted-text role=\"ref\"} to share data among team members rather than having a separate copy in everyone's home directory. Check /home/gridsan/groups/datasets before downloading large public datasets. If there is a public dataset that you are considering downloading that you think others may also want to use, send us a email to supercloud@mit.edu to suggest that we add it. If you are using ImageNet, we have a special setup that will stress the filesystem signficantly less and should be much faster. First load its modulefile: module load /home/gridsan/groups/datasets/ImageNet/modulefile . This will set up the $IMAGENET_PATH environment variable, which you can use in your code to point to ImageNet.","title":"File Organization"},{"location":"using-the-system/best-practices/gpu-jobs/","text":"Optimizing your GPU Jobs {#gpu_jobs} \uf0c1 The GPUs on Supercloud often are in very high demand. Before asking for more GPUs you\\'ll want to do what you can to optimize your code to get the most out of the resources you do have. The first thing you need to do is profile your code. Time your code both with and without the GPU. When you are running with a GPU, monitor the GPU Utilization and GPU Memory use. You can do this by going to the compute node where it is running (get the node name from LLstat, then ssh to the node with \\\" ssh nodename \\\") and run the nvidia-smi -l command (press Ctrl+C to exit nvidia-smi -l when you are done). If your GPU utilization and GPU memory use is low, that means you aren\\'t getting the full advantage out of the GPU. If this is the case, try out some of the suggestions below. If you are only getting a small speedup, say 2x or 4x, especially after trying the suggestions below, it may not be worthwhile using GPUs at all. In general, the CPU nodes on Supercloud are more available so we will likely be willing to increase your allocation of CPU nodes to make up for the small loss of speedup switching to CPUs. Getting More out of the GPUs \uf0c1 If your GPU utilization and memory use are low, usually this means you might be able to get some more performance out of the GPUs. There are a few things you can try if you are training machine learning models: Increase the batch size. Enable CUDA kernel tuning. For Pytorch, add this before the training starts: torch.backends.cudnn.benchmark = True For Tensorflow, set this environment variable in your submission script: export TF_CUDNN_USE_AUTOTUNE=1 If you are using Tensorflow you can also try mixed-precision training (we haven't played with this in Pytorch, but it could be possible). Tensorflow 2.4.1 and newer (anaconda/2021a) Add the following to the beginning of your Python code: from tensorflow.keras import mixed_precision policy = mixed_precision.Policy('mixed_float16') mixed_precision.set_global_policy(policy) Pre-Tensorflow 2.4.1 (anaconda/2020b and older) To do so set the following environment variables in your submission script: export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1 export TF_ENABLE_AUTO_MIXED_PRECISION=1 And add this to your Python code, here opt is the optimizer object: opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) If the number of filters in the model layers is a multiple of 64, you will see an additional speedup because the V100 tensor cores are optimized for matmul/conv ops of these sizes. If both GPU utilization and memory are below 50% you could try to train two models per GPU. This isn't too hard to do with Triples Mode <job_array_triples> {.interpreted-text role=\"ref\"}, simply set the number of processes per node to 4. Requesting More GPUs \uf0c1 If you have tried the above suggestions and you still find you need more GPUs you can put in a request. Send an email to supercloud\\@mit.edu and tell us what you need, what you need it for, and how long you need the allocation increase. Are you trying to run a large distributed training run, or are you doing hyperparameter searches? Justify your request with numbers: show us that you have compared CPU and GPU run times, that you have good GPU utilization, and that you have tried the above suggestions. Explain how you got to the number of GPUs you are requesting. If the system is not too busy we may grant your request. Keep in mind that others are likely to have the same paper deadlines as you and we may not be able to grant requests during these busy periods, so plan ahead.","title":"Optimizing your GPU Jobs"},{"location":"using-the-system/best-practices/gpu-jobs/#optimizing-your-gpu-jobs-gpu_jobs","text":"The GPUs on Supercloud often are in very high demand. Before asking for more GPUs you\\'ll want to do what you can to optimize your code to get the most out of the resources you do have. The first thing you need to do is profile your code. Time your code both with and without the GPU. When you are running with a GPU, monitor the GPU Utilization and GPU Memory use. You can do this by going to the compute node where it is running (get the node name from LLstat, then ssh to the node with \\\" ssh nodename \\\") and run the nvidia-smi -l command (press Ctrl+C to exit nvidia-smi -l when you are done). If your GPU utilization and GPU memory use is low, that means you aren\\'t getting the full advantage out of the GPU. If this is the case, try out some of the suggestions below. If you are only getting a small speedup, say 2x or 4x, especially after trying the suggestions below, it may not be worthwhile using GPUs at all. In general, the CPU nodes on Supercloud are more available so we will likely be willing to increase your allocation of CPU nodes to make up for the small loss of speedup switching to CPUs.","title":"Optimizing your GPU Jobs {#gpu_jobs}"},{"location":"using-the-system/best-practices/gpu-jobs/#getting-more-out-of-the-gpus","text":"If your GPU utilization and memory use are low, usually this means you might be able to get some more performance out of the GPUs. There are a few things you can try if you are training machine learning models: Increase the batch size. Enable CUDA kernel tuning. For Pytorch, add this before the training starts: torch.backends.cudnn.benchmark = True For Tensorflow, set this environment variable in your submission script: export TF_CUDNN_USE_AUTOTUNE=1 If you are using Tensorflow you can also try mixed-precision training (we haven't played with this in Pytorch, but it could be possible). Tensorflow 2.4.1 and newer (anaconda/2021a) Add the following to the beginning of your Python code: from tensorflow.keras import mixed_precision policy = mixed_precision.Policy('mixed_float16') mixed_precision.set_global_policy(policy) Pre-Tensorflow 2.4.1 (anaconda/2020b and older) To do so set the following environment variables in your submission script: export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1 export TF_ENABLE_AUTO_MIXED_PRECISION=1 And add this to your Python code, here opt is the optimizer object: opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) If the number of filters in the model layers is a multiple of 64, you will see an additional speedup because the V100 tensor cores are optimized for matmul/conv ops of these sizes. If both GPU utilization and memory are below 50% you could try to train two models per GPU. This isn't too hard to do with Triples Mode <job_array_triples> {.interpreted-text role=\"ref\"}, simply set the number of processes per node to 4.","title":"Getting More out of the GPUs"},{"location":"using-the-system/best-practices/gpu-jobs/#requesting-more-gpus","text":"If you have tried the above suggestions and you still find you need more GPUs you can put in a request. Send an email to supercloud\\@mit.edu and tell us what you need, what you need it for, and how long you need the allocation increase. Are you trying to run a large distributed training run, or are you doing hyperparameter searches? Justify your request with numbers: show us that you have compared CPU and GPU run times, that you have good GPU utilization, and that you have tried the above suggestions. Explain how you got to the number of GPUs you are requesting. If the system is not too busy we may grant your request. Keep in mind that others are likely to have the same paper deadlines as you and we may not be able to grant requests during these busy periods, so plan ahead.","title":"Requesting More GPUs"},{"location":"using-the-system/files-and-data/shared-groups/","text":"Shared Groups {#shared_groups} \uf0c1 Overview \uf0c1 This page summarizes Shared Groups, including how to request to join or create a group <#request-group> {.interpreted-text role=\"ref\"}, some best practices for working with shared groups <#using-groups> {.interpreted-text role=\"ref\"}, and how to inspect and change file permissions and group ownership <#permissions> {.interpreted-text role=\"ref\"}. Supercloud users are welcome to either join an existing group and receive the benefit of access to a shared file directory, or propose the creation of a group if there is some common interest amongst certain Supercloud account holders, perhaps they are members of the same lab. You can look at the current list of groups by listing the groups directory: studentx@login-3:~$ ls /home/gridsan/groups/ You can see what groups you are currently in by running the \"groups\" command: studentx@login-3:~$ groups Joining or Creating a Group {##request-group} \uf0c1 If you would like to join a group, send an e-mail to supercloud@mit.edu with that request and CC the group owner for approval. The group owner must give approval before we can add you to the group. If you are not sure who the group owner/approver is, you can send in your request and we will reach out to the approver. If you would like to create a group, please email supercloud\\@mit.edu with the following info. What should the group be called? Who should the group owner/approver be? We will ask this person for approval if anyone asks to be added. Who should be in the group, listing usernames is most helpful to us, but not required. Whether you plan to store any non-public data in the group. If so, please list any requirements, restrictions, or agreements associated with the data. The more information you give us, the better. Using Shared Groups Effectively {##using-groups} \uf0c1 Once you have been added to a group you will be able to access that group\\'s shared directory. All group directories are located in the /home/gridsan/groups directory on the filesystem. Since this is part of the central filesystem along with your home directory, all nodes in Supercloud can access the group directories. We will also add a symlink in your home directory to your group shared directory, this symlink will have the suffix \\\"_shared\\\" to indicate it is linking to a group directory. If you are sharing code with other members of your team that includes paths to a shared group, it is good practice to use a path that does not include your home directory, otherwise your team members will get a permission denied error when they try to run your code. Instead, it is best to use the absolute path through /home/gridsan/groups . All of our Best Practices for using the Filesystem <_filesystem_best_practices> {.interpreted-text role=\"ref\"} apply to the group directories. Additionally, NEVER use a GUI to drag and drop files into a group directory. Doing so can alter the permissions in the group directory, preventing others in your group from accessing the files you\\'ve moved into the shared group directory. When using rsync to transfer files into a group directory, be sure to use the -g flag, which will also help keep the group ownership set properly. Linux File Permissions {##permissions} \uf0c1 Sometimes, despite your best efforts, the permissions on a group can be altered such that you or others in your group can't interact with a file the way they need to. If that happens, you can always contact us at supercloud@mit.edu and we can fix it. However, you may find it more convenient to fix it on your own. Here is a brief introduction to Linux File Permissions to help you learn what is going on and how to fix it. Inspecting File Permissions \uf0c1 If you do a long form listing of the files in a directory using ls -l : drwxrwx--- 2 studentz studentz 4096 Jun 15 14:51 mydirectory lrwxrwxrwx 1 root root 26 Jun 15 17:24 files_shared -> ../groups/fileshare -rw------- 1 studenty studenty 4096 Jun 30 09:02 logfile1 -rw-rw--- 1 studentx Alpha 4096 Jun 30 09:02 logfile2 You will see the file permissions of your various directories, symlinks, and files in the leftmost columns. The first column indicates whether the file is a directory (d), symlink (l), or a regular file. Columns 2 through 10 can be viewed as triplets that define access permissions for the file or folder. To explicitly define permissions you will need to reference the Permission Group and Permission Types: The Permission Groups are: u -- Owner g -- Group o -- Others The Permission Types are: r -- Read w -- Write x -- Execute The first of these triplets represent the Owner's permissions, the second the Group's, and the third Others'. An r,w, or x represent the ability to perform that action, and a \"-\" means that action is not permitted. For a file like logfile1 above you can see that it is owned by user studenty (from group studenty ) and only the owner has read and write permissions. The file named logfile2 currently has the permissions set to -rw-rw---- , which means that the owner and group have read and write permission. Supercloud does not allow you to add read, write, or execute permissions for others, or all users. One important thing to note: in order to go into a directory you must have execute permissions on that directory. So if you get a \\\"Permission denied\\\" error when trying to enter or look at the files in a directory, check whether the directory has read and execute permissions. Changing File Permissions \uf0c1 Now say we want to change permissions for a file. One of the easiest ways is to use the Assignment Operators, + (plus) and -- (minus). These are used to tell the system whether to add or remove the specific permissions. For example, to add group read and write permission for logfile1 , you would invoke the command: chmod g+rw logfile1 Now say you want your group to be able to read logfile2 , but don't want anyone to accidentally modify it. To remove group write permissions you would invoke the command: chmod g-w logfile2 It's very important to know that if you want to apply these changes recursively that you use the -R (with a capital R) flag. Using a lowercase -r flag like you do for other Linux commands like cp will remove write permissions for everyone, including yourself. If you make this mistake, it is not the end of the world, but you will need to send us an email and have us fix it. Alternately you can define the full permissions options with binary references like chmod 750 logfile1 which would grant full privileges (7) to the owner, and rw privileges (5) to the group and nothing (0) to others in a single command. You can learn more options and about chmod either from an online tutorial or from your local man pages ( man chmod , typing q will exit) or with the quick cheat sheet you can display with chmod --help . The following is also pretty good tutorial, but be aware it talks about permissions in general, and not everything will be relevant to shared groups or Supercloud: How to use the chmod Command on Linux . Linux File Ownership \uf0c1 If we take another look at the example directory above: drwxrwx--- 2 studentz studentz 4096 Jun 15 14:51 mydirectory lrwxrwxrwx 1 root root 26 Jun 15 17:24 files_shared -> ../groups/fileshare -rw-rw---- 1 studenty studenty 4096 Jun 30 09:02 logfile1 -rw-r---- 1 studentx Alpha 4096 Jun 30 09:02 logfile2 -rw-r-x--- 1 studenty studenty 4096 Jun 30 09:02 myscript.py the 12th and 13th column of the ls -l output is the owner of the file, listed first, and the group for the file, listed second. For example, logfile2 is owned by studentx and its group is Alpha . Based on the permissions above, studentx can read and write to the file, and anyone in the Alpha group can read the file, but cannot write to it. In a group directory the group owner for a file should usually be the group associated with that directory. Sometimes it unintentionally gets set to the username of the person who created or put the file there. This can easily be remedied by using the \"chgrp\" command. For example, let's say we'd like everyone in the Alpha group to be able to read and run (execute) the file myscript.py , but not have write permissions. The group permissions are set properly, but the group is set to studenty instead of Alpha . To fix this, we can run: chgrp Alpha myscript.py Again, if you would like to apply this change recursively, the flag is -R (with a capital R).","title":"Shared Groups"},{"location":"using-the-system/files-and-data/shared-groups/#shared-groups-shared_groups","text":"","title":"Shared Groups {#shared_groups}"},{"location":"using-the-system/files-and-data/shared-groups/#overview","text":"This page summarizes Shared Groups, including how to request to join or create a group <#request-group> {.interpreted-text role=\"ref\"}, some best practices for working with shared groups <#using-groups> {.interpreted-text role=\"ref\"}, and how to inspect and change file permissions and group ownership <#permissions> {.interpreted-text role=\"ref\"}. Supercloud users are welcome to either join an existing group and receive the benefit of access to a shared file directory, or propose the creation of a group if there is some common interest amongst certain Supercloud account holders, perhaps they are members of the same lab. You can look at the current list of groups by listing the groups directory: studentx@login-3:~$ ls /home/gridsan/groups/ You can see what groups you are currently in by running the \"groups\" command: studentx@login-3:~$ groups","title":"Overview"},{"location":"using-the-system/files-and-data/shared-groups/#joining-or-creating-a-group-request-group","text":"If you would like to join a group, send an e-mail to supercloud@mit.edu with that request and CC the group owner for approval. The group owner must give approval before we can add you to the group. If you are not sure who the group owner/approver is, you can send in your request and we will reach out to the approver. If you would like to create a group, please email supercloud\\@mit.edu with the following info. What should the group be called? Who should the group owner/approver be? We will ask this person for approval if anyone asks to be added. Who should be in the group, listing usernames is most helpful to us, but not required. Whether you plan to store any non-public data in the group. If so, please list any requirements, restrictions, or agreements associated with the data. The more information you give us, the better.","title":"Joining or Creating a Group {##request-group}"},{"location":"using-the-system/files-and-data/shared-groups/#using-shared-groups-effectively-using-groups","text":"Once you have been added to a group you will be able to access that group\\'s shared directory. All group directories are located in the /home/gridsan/groups directory on the filesystem. Since this is part of the central filesystem along with your home directory, all nodes in Supercloud can access the group directories. We will also add a symlink in your home directory to your group shared directory, this symlink will have the suffix \\\"_shared\\\" to indicate it is linking to a group directory. If you are sharing code with other members of your team that includes paths to a shared group, it is good practice to use a path that does not include your home directory, otherwise your team members will get a permission denied error when they try to run your code. Instead, it is best to use the absolute path through /home/gridsan/groups . All of our Best Practices for using the Filesystem <_filesystem_best_practices> {.interpreted-text role=\"ref\"} apply to the group directories. Additionally, NEVER use a GUI to drag and drop files into a group directory. Doing so can alter the permissions in the group directory, preventing others in your group from accessing the files you\\'ve moved into the shared group directory. When using rsync to transfer files into a group directory, be sure to use the -g flag, which will also help keep the group ownership set properly.","title":"Using Shared Groups Effectively {##using-groups}"},{"location":"using-the-system/files-and-data/shared-groups/#linux-file-permissions-permissions","text":"Sometimes, despite your best efforts, the permissions on a group can be altered such that you or others in your group can't interact with a file the way they need to. If that happens, you can always contact us at supercloud@mit.edu and we can fix it. However, you may find it more convenient to fix it on your own. Here is a brief introduction to Linux File Permissions to help you learn what is going on and how to fix it.","title":"Linux File Permissions {##permissions}"},{"location":"using-the-system/files-and-data/shared-groups/#inspecting-file-permissions","text":"If you do a long form listing of the files in a directory using ls -l : drwxrwx--- 2 studentz studentz 4096 Jun 15 14:51 mydirectory lrwxrwxrwx 1 root root 26 Jun 15 17:24 files_shared -> ../groups/fileshare -rw------- 1 studenty studenty 4096 Jun 30 09:02 logfile1 -rw-rw--- 1 studentx Alpha 4096 Jun 30 09:02 logfile2 You will see the file permissions of your various directories, symlinks, and files in the leftmost columns. The first column indicates whether the file is a directory (d), symlink (l), or a regular file. Columns 2 through 10 can be viewed as triplets that define access permissions for the file or folder. To explicitly define permissions you will need to reference the Permission Group and Permission Types: The Permission Groups are: u -- Owner g -- Group o -- Others The Permission Types are: r -- Read w -- Write x -- Execute The first of these triplets represent the Owner's permissions, the second the Group's, and the third Others'. An r,w, or x represent the ability to perform that action, and a \"-\" means that action is not permitted. For a file like logfile1 above you can see that it is owned by user studenty (from group studenty ) and only the owner has read and write permissions. The file named logfile2 currently has the permissions set to -rw-rw---- , which means that the owner and group have read and write permission. Supercloud does not allow you to add read, write, or execute permissions for others, or all users. One important thing to note: in order to go into a directory you must have execute permissions on that directory. So if you get a \\\"Permission denied\\\" error when trying to enter or look at the files in a directory, check whether the directory has read and execute permissions.","title":"Inspecting File Permissions"},{"location":"using-the-system/files-and-data/shared-groups/#changing-file-permissions","text":"Now say we want to change permissions for a file. One of the easiest ways is to use the Assignment Operators, + (plus) and -- (minus). These are used to tell the system whether to add or remove the specific permissions. For example, to add group read and write permission for logfile1 , you would invoke the command: chmod g+rw logfile1 Now say you want your group to be able to read logfile2 , but don't want anyone to accidentally modify it. To remove group write permissions you would invoke the command: chmod g-w logfile2 It's very important to know that if you want to apply these changes recursively that you use the -R (with a capital R) flag. Using a lowercase -r flag like you do for other Linux commands like cp will remove write permissions for everyone, including yourself. If you make this mistake, it is not the end of the world, but you will need to send us an email and have us fix it. Alternately you can define the full permissions options with binary references like chmod 750 logfile1 which would grant full privileges (7) to the owner, and rw privileges (5) to the group and nothing (0) to others in a single command. You can learn more options and about chmod either from an online tutorial or from your local man pages ( man chmod , typing q will exit) or with the quick cheat sheet you can display with chmod --help . The following is also pretty good tutorial, but be aware it talks about permissions in general, and not everything will be relevant to shared groups or Supercloud: How to use the chmod Command on Linux .","title":"Changing File Permissions"},{"location":"using-the-system/files-and-data/shared-groups/#linux-file-ownership","text":"If we take another look at the example directory above: drwxrwx--- 2 studentz studentz 4096 Jun 15 14:51 mydirectory lrwxrwxrwx 1 root root 26 Jun 15 17:24 files_shared -> ../groups/fileshare -rw-rw---- 1 studenty studenty 4096 Jun 30 09:02 logfile1 -rw-r---- 1 studentx Alpha 4096 Jun 30 09:02 logfile2 -rw-r-x--- 1 studenty studenty 4096 Jun 30 09:02 myscript.py the 12th and 13th column of the ls -l output is the owner of the file, listed first, and the group for the file, listed second. For example, logfile2 is owned by studentx and its group is Alpha . Based on the permissions above, studentx can read and write to the file, and anyone in the Alpha group can read the file, but cannot write to it. In a group directory the group owner for a file should usually be the group associated with that directory. Sometimes it unintentionally gets set to the username of the person who created or put the file there. This can easily be remedied by using the \"chgrp\" command. For example, let's say we'd like everyone in the Alpha group to be able to read and run (execute) the file myscript.py , but not have write permissions. The group permissions are set properly, but the group is set to studenty instead of Alpha . To fix this, we can run: chgrp Alpha myscript.py Again, if you would like to apply this change recursively, the flag is -R (with a capital R).","title":"Linux File Ownership"},{"location":"using-the-system/files-and-data/transferring-files/","text":"Transferring Files {#transferring_files} \uf0c1 There are number of ways to access and transfer data and files between your computer and the MIT Supercloud System depending on your OS ( Mac/Linux <#MacLinux> {.interpreted-text role=\"ref\"} or Windows <#Windows> {.interpreted-text role=\"ref\"}) and your comfort with Linux/Unix commands. Regardless of OS, files can be downloaded, but not uploaded, through the web portal <#transferring-portal> {.interpreted-text role=\"ref\"} and both uploaded and downloaded through the Jupyter <#transferring-jupyter> {.interpreted-text role=\"ref\"} web interface. For Mac/Linux or Windows with Cygwin, you have the additional options of using scp and rsync from a terminal window. If you are using PuTTY on Windows, there is a way to transfer through that program as well. All of these are described below. Mac/Linux and Most Windows {##MacLinux} \uf0c1 Via scp \uf0c1 You can use scp (secure copy) to copy files to the MIT Supercloud. The scp command expects a path to the file or directory name that you would like to transfer followed by a space and then the destination. Note, it is best to copy to or from your local machine. In addition, the location on the system is the login node followed by a \\':\\' (colon) followed by the path to the directory where you want to put the file. If you do not specify the directory location scp will place it in your top level directory, which on the Supercloud system is your home directory. An example of copying a directory to TX-E1 is shown below. Note that the scp command is followed by -r , to recursively copy all the files in the directory, and the directory name is followed by a \\'/\\' so that scp copies all of the files in the directory. myLocalMachine UserName > scp -r testCodes/ <username>@txe1-login.mit.edu:/home/gridsan/<username>/snippets If you want to copy data from TX-E1 to your computer, you just swap the \\\"from\\\" and \\\"to\\\": myLocalMachine UserName > scp -r \\<username>\\@txe1-login.mit.edu:/home/gridsan/\\<username>/results/ results Via rsync \uf0c1 The rsync (remote synchronization) command is similar to the scp command. However, after the first time that you copy files, excuting the rsync command will copy only the files that have changed since the last rsync. This can often save time. Note that if you have the slash at the end, it means \\\"all contents in the directory\\\". If not, it means the directory itself and its contents. As shown in case A below, the directory, myDir , and its all contents will be copied over to the snippets directory on the remote host. However, in case B, only the contents in the myDir directory will be copied to the snippets directory on the remote host. CaseA: rsync -rlug myDir <username>@txe1-login.mit.edu:/home/gridsan/<username>/snippets CaseB: rsync -rlug myDir/ <username>@txe1-login.mit.edu:/home/gridsan/<username>/snippets There are a number of options that can be used with rsync. Among the most commonly used are: r to recursively copy files in all sub-directories l to copy and retain symbolic links. u is needed if you have modified files on the destination and you don\\'t want the old file to overwrite over the newer version on the destination. g is used to preserve group attributes associated with files in a shared group. h human readable v verbose so that you get any error or warning information Again, if you want to copy data from TX-E1 to your computer, you just swap the \\\"from\\\" and \\\"to\\\": CaseA: rsync -rlug <username>@txe1-login.mit.edu:/home/gridsan/<username>/results myDir CaseB: rsync -rlug <username>@txe1-login.mit.edu:/home/gridsan/<username>/results/ myDir Windows {##Windows} \uf0c1 Depending on which ssh client you use you will transfer data differently. If you use Bash on Windows 10, Mobaxterm, Cygwin, or other Linux-like command line environment, you can use scp or rsync directly and should follow the instructions above for Mac/Linux <#MacLinux> {.interpreted-text role=\"ref\"}. If you use PuTTY, you can follow the directions below to use pscp. You run these commands from your own computer, not from the MIT SuperCloud login node (if you have aready ssh\\'d in, you\\'ve gone too far). IMPORTANT: MIT SuperCloud runs a Linux operating system. Two big differences between Windows and Linux are the direction that the folder separators face (Windows uses \\\"\\\" and Linux uses \\\"/\\\") and case sensitivity (Linux file and directory names are case sensitive, while Windows file and folder names are not). Whether you are using scp, rsync, or pscp you must use the Linux slashes \\\"/\\\" in your path and the correct case in your filenames (Documents =/= documents). It will also be much easier to transfer data if you avoid using spaces in your file and directory/folder names. HINT: Windows paths sometimes specify different \\\"drives\\\" (such as the C: drive, where most local documents are kept). Different ssh clients treat these drives differently. Using the example of a folder in the C drive (say C:myFolder), the translations for each are: Bash for Windows 10: /mnt/c/myFolder Mobaxterm: /drives/c/myFolder Cygwin: /cygdrive/C/myFolder When in doubt, you can navigate to the directory that you want to transfer and issue the \\\"pwd\\\" command, which should give you the full path to your current location. PuTTY \uf0c1 When transferring files from a Windows machine to the Supercloud system, which are linux machines, you need to use a version of scp , or secure copy. The software package, PuTTY, which you installed to provide ssh , includes scp . To transfer a file between your Windows desktop and the Supercloud linux system: Open a command window by typing run in the search box. The scp command must be run from a command window. Set the PATH variable so that your system sees the putty command: To do it for this session only, type set PATH=C:\\Program Files\\PuTTY at the command prompt To make this change more permanent, click on the start button, in the search box type Environment Variables: Select Add/Modify Environment Variables for user Click on Path and then click to edit Path. Add C:\\Program File\\PuTTY (This assumes that you installed PuTTY in the default folder.) Confirm that PuTTY is in your path by typing: %ECHO PATH% at the command line. Navigate to the directory (folder) that holds the file you want to transfer The pscp command has the format pscp <Path_to_FileToBeTransfered> <Path_To_NewLocation> For example to use pscp to copy the file myMatlabScript.m from my current directory (folder) to my home directory on the Supercloud system I would type If I wanted to copy myResults.dat from my home directory on the Supercloud system to a folder called goodData on my local machine I would first change directories so that I was in goodDate, using: cd goodData . Then, secure copy the file from the LLSC system to my local system using: pscp <myUserID>@txe1-login.mit.edu:/home/gridsan/<myUserID>/myResults.dat . Note the \\' . \\' at the end of the line - that is equivalent to \\\"place it here\\\". Also note that in Windows you can use \\' \\ \\' or \\' / \\' but Linux only understands \\' / \\'. Cygwin \uf0c1 Cygwin also supports the secure copy protocol. This works similarly to scp in a Linux/Mac. When using Cygwin, you will be able to run scp , however it can be tricky to determine to the path to your files. This is of the form /cygdrive/C/<your_File_Path> . Say you are using scp to copy the file myScript.sh from your current directory (folder) to your home directory on the Supercloud system. Type: scp /cygdrive/C/<path_to_Folder>/mScript.sh <myUserID>@txe1-login.mit.edu:/home/gridsan/<myUserID> If you wanted to copy myResults.tx t from your home directory on the Supercloud system to a folder called goodData on your local machine, first change directories so that you are in goodData , using: cd goodData. Secure copy the file from the Supercloud system to the local system using: scp <myUserID>@txe1-login.mit.edu:/home/gridsan/<myUserID>/myResults.txt /cyqdrive/c/<path_to_goodData>/ Note that in Windows you can use \\'\\' or \\'/\\' but Linux only understands \\'/\\' Downloading Files through the Web Portal {##transferring-portal} \uf0c1 The web portal can be accessed at: https://txe1-portal.mit.edu/gridsan/USERNAME/ Where USERNAME is your username. When you navigate to the portal page, you will be prompted for your username and password. Through the portal, you can navigate through your directories and download files. You cannot upload files this way. Uploading and Downloading Files Using Jupyter {##transferring-jupyter} \uf0c1 You can both upload and download files using the Jupyter interface through your browser. If you do not already have a Jupyter instance running, start one up <jupyter> {.interpreted-text role=\"ref\"}. The Jupyter portal is located at: https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php Once you have an instance running, follow the \\\"Open Notebook\\\" link to open Jupyter. You will see the files and directories in your home directory. Here you can right click on a file and select \\\"Save Link As\\\" to download a file. The \\\"Upload\\\" button is located in the top right of the page, right next to the \\\"New\\\" button. You can select multiple files to upload at a time.","title":"Transferring Files"},{"location":"using-the-system/files-and-data/transferring-files/#transferring-files-transferring_files","text":"There are number of ways to access and transfer data and files between your computer and the MIT Supercloud System depending on your OS ( Mac/Linux <#MacLinux> {.interpreted-text role=\"ref\"} or Windows <#Windows> {.interpreted-text role=\"ref\"}) and your comfort with Linux/Unix commands. Regardless of OS, files can be downloaded, but not uploaded, through the web portal <#transferring-portal> {.interpreted-text role=\"ref\"} and both uploaded and downloaded through the Jupyter <#transferring-jupyter> {.interpreted-text role=\"ref\"} web interface. For Mac/Linux or Windows with Cygwin, you have the additional options of using scp and rsync from a terminal window. If you are using PuTTY on Windows, there is a way to transfer through that program as well. All of these are described below.","title":"Transferring Files {#transferring_files}"},{"location":"using-the-system/files-and-data/transferring-files/#maclinux-and-most-windows-maclinux","text":"","title":"Mac/Linux and Most Windows {##MacLinux}"},{"location":"using-the-system/files-and-data/transferring-files/#via-scp","text":"You can use scp (secure copy) to copy files to the MIT Supercloud. The scp command expects a path to the file or directory name that you would like to transfer followed by a space and then the destination. Note, it is best to copy to or from your local machine. In addition, the location on the system is the login node followed by a \\':\\' (colon) followed by the path to the directory where you want to put the file. If you do not specify the directory location scp will place it in your top level directory, which on the Supercloud system is your home directory. An example of copying a directory to TX-E1 is shown below. Note that the scp command is followed by -r , to recursively copy all the files in the directory, and the directory name is followed by a \\'/\\' so that scp copies all of the files in the directory. myLocalMachine UserName > scp -r testCodes/ <username>@txe1-login.mit.edu:/home/gridsan/<username>/snippets If you want to copy data from TX-E1 to your computer, you just swap the \\\"from\\\" and \\\"to\\\": myLocalMachine UserName > scp -r \\<username>\\@txe1-login.mit.edu:/home/gridsan/\\<username>/results/ results","title":"Via scp"},{"location":"using-the-system/files-and-data/transferring-files/#via-rsync","text":"The rsync (remote synchronization) command is similar to the scp command. However, after the first time that you copy files, excuting the rsync command will copy only the files that have changed since the last rsync. This can often save time. Note that if you have the slash at the end, it means \\\"all contents in the directory\\\". If not, it means the directory itself and its contents. As shown in case A below, the directory, myDir , and its all contents will be copied over to the snippets directory on the remote host. However, in case B, only the contents in the myDir directory will be copied to the snippets directory on the remote host. CaseA: rsync -rlug myDir <username>@txe1-login.mit.edu:/home/gridsan/<username>/snippets CaseB: rsync -rlug myDir/ <username>@txe1-login.mit.edu:/home/gridsan/<username>/snippets There are a number of options that can be used with rsync. Among the most commonly used are: r to recursively copy files in all sub-directories l to copy and retain symbolic links. u is needed if you have modified files on the destination and you don\\'t want the old file to overwrite over the newer version on the destination. g is used to preserve group attributes associated with files in a shared group. h human readable v verbose so that you get any error or warning information Again, if you want to copy data from TX-E1 to your computer, you just swap the \\\"from\\\" and \\\"to\\\": CaseA: rsync -rlug <username>@txe1-login.mit.edu:/home/gridsan/<username>/results myDir CaseB: rsync -rlug <username>@txe1-login.mit.edu:/home/gridsan/<username>/results/ myDir","title":"Via rsync"},{"location":"using-the-system/files-and-data/transferring-files/#windows-windows","text":"Depending on which ssh client you use you will transfer data differently. If you use Bash on Windows 10, Mobaxterm, Cygwin, or other Linux-like command line environment, you can use scp or rsync directly and should follow the instructions above for Mac/Linux <#MacLinux> {.interpreted-text role=\"ref\"}. If you use PuTTY, you can follow the directions below to use pscp. You run these commands from your own computer, not from the MIT SuperCloud login node (if you have aready ssh\\'d in, you\\'ve gone too far). IMPORTANT: MIT SuperCloud runs a Linux operating system. Two big differences between Windows and Linux are the direction that the folder separators face (Windows uses \\\"\\\" and Linux uses \\\"/\\\") and case sensitivity (Linux file and directory names are case sensitive, while Windows file and folder names are not). Whether you are using scp, rsync, or pscp you must use the Linux slashes \\\"/\\\" in your path and the correct case in your filenames (Documents =/= documents). It will also be much easier to transfer data if you avoid using spaces in your file and directory/folder names. HINT: Windows paths sometimes specify different \\\"drives\\\" (such as the C: drive, where most local documents are kept). Different ssh clients treat these drives differently. Using the example of a folder in the C drive (say C:myFolder), the translations for each are: Bash for Windows 10: /mnt/c/myFolder Mobaxterm: /drives/c/myFolder Cygwin: /cygdrive/C/myFolder When in doubt, you can navigate to the directory that you want to transfer and issue the \\\"pwd\\\" command, which should give you the full path to your current location.","title":"Windows {##Windows}"},{"location":"using-the-system/files-and-data/transferring-files/#putty","text":"When transferring files from a Windows machine to the Supercloud system, which are linux machines, you need to use a version of scp , or secure copy. The software package, PuTTY, which you installed to provide ssh , includes scp . To transfer a file between your Windows desktop and the Supercloud linux system: Open a command window by typing run in the search box. The scp command must be run from a command window. Set the PATH variable so that your system sees the putty command: To do it for this session only, type set PATH=C:\\Program Files\\PuTTY at the command prompt To make this change more permanent, click on the start button, in the search box type Environment Variables: Select Add/Modify Environment Variables for user Click on Path and then click to edit Path. Add C:\\Program File\\PuTTY (This assumes that you installed PuTTY in the default folder.) Confirm that PuTTY is in your path by typing: %ECHO PATH% at the command line. Navigate to the directory (folder) that holds the file you want to transfer The pscp command has the format pscp <Path_to_FileToBeTransfered> <Path_To_NewLocation> For example to use pscp to copy the file myMatlabScript.m from my current directory (folder) to my home directory on the Supercloud system I would type If I wanted to copy myResults.dat from my home directory on the Supercloud system to a folder called goodData on my local machine I would first change directories so that I was in goodDate, using: cd goodData . Then, secure copy the file from the LLSC system to my local system using: pscp <myUserID>@txe1-login.mit.edu:/home/gridsan/<myUserID>/myResults.dat . Note the \\' . \\' at the end of the line - that is equivalent to \\\"place it here\\\". Also note that in Windows you can use \\' \\ \\' or \\' / \\' but Linux only understands \\' / \\'.","title":"PuTTY"},{"location":"using-the-system/files-and-data/transferring-files/#cygwin","text":"Cygwin also supports the secure copy protocol. This works similarly to scp in a Linux/Mac. When using Cygwin, you will be able to run scp , however it can be tricky to determine to the path to your files. This is of the form /cygdrive/C/<your_File_Path> . Say you are using scp to copy the file myScript.sh from your current directory (folder) to your home directory on the Supercloud system. Type: scp /cygdrive/C/<path_to_Folder>/mScript.sh <myUserID>@txe1-login.mit.edu:/home/gridsan/<myUserID> If you wanted to copy myResults.tx t from your home directory on the Supercloud system to a folder called goodData on your local machine, first change directories so that you are in goodData , using: cd goodData. Secure copy the file from the Supercloud system to the local system using: scp <myUserID>@txe1-login.mit.edu:/home/gridsan/<myUserID>/myResults.txt /cyqdrive/c/<path_to_goodData>/ Note that in Windows you can use \\'\\' or \\'/\\' but Linux only understands \\'/\\'","title":"Cygwin"},{"location":"using-the-system/files-and-data/transferring-files/#downloading-files-through-the-web-portal-transferring-portal","text":"The web portal can be accessed at: https://txe1-portal.mit.edu/gridsan/USERNAME/ Where USERNAME is your username. When you navigate to the portal page, you will be prompted for your username and password. Through the portal, you can navigate through your directories and download files. You cannot upload files this way.","title":"Downloading Files through the Web Portal {##transferring-portal}"},{"location":"using-the-system/files-and-data/transferring-files/#uploading-and-downloading-files-using-jupyter-transferring-jupyter","text":"You can both upload and download files using the Jupyter interface through your browser. If you do not already have a Jupyter instance running, start one up <jupyter> {.interpreted-text role=\"ref\"}. The Jupyter portal is located at: https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php Once you have an instance running, follow the \\\"Open Notebook\\\" link to open Jupyter. You will see the files and directories in your home directory. Here you can right click on a file and select \\\"Save Link As\\\" to download a file. The \\\"Upload\\\" button is located in the top right of the page, right next to the \\\"New\\\" button. You can select multiple files to upload at a time.","title":"Uploading and Downloading Files Using Jupyter {##transferring-jupyter}"},{"location":"using-the-system/submitting-jobs/job-array-triples/","text":"Job Arrays with LLsub Triples in 3 Steps {#job_array_triples} \uf0c1 If you are currently running a Job Array, you can take advantage of SuperCloud\\'s triples mode submission by submitting your job array with LLsub. In most cases, this can be done in a few easy steps. What is triples mode? It\\'s a different way to submit your job by specifying three numbers: Nodes: The number of nodes you want to use up NPPN: The number of processes that should run per node NT: The number of threads per process Why would you want to use triples mode to submit your job? Triples mode provides a few advantages. Since it does whole-node scheduling you don\\'t have to worry about other jobs impacting yours (or your job impacting someone else\\'s). We also do task-pinning when you request resources with a triple, so your processes are arranged in the best way possible for the layout of the hardware architecture. Finally, submitting a Job Array with LLsub triples mode the way we describe here will greatly reduce the startup time for your jobs, over a job array with many pending tasks. Step 1: Batch Up your Array \uf0c1 The first thing you want to do is set up your job so that your script splits up your inputs among each process/task and each process/task iterates a subset of your inputs. This way you can change your input set without changing the number of tasks or processes that you schedule. This step alone will save you on startup time. Anytime you submit more jobs than your allocation allows, those additional jobs will remaining pending until some of your first running jobs complete. Then, when one of your first jobs complete, the scheduler now has to find the pending job some resources and start it running. If you batch up your job array, you only have to go through the scheduling process once. If you\\'ve set up a job array following our instructions in the past you may already be doing this, and you can skip to the next section. First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you\\'ll first want to enumerate all the combinations of what you are iterating over so you have one big loop. If your code is written so it uses the $SLURM_ARRAY_TASK_ID and uses that to run a single thing, first add a for loop that iterates over the full set of things you want to run your code on. If you can\\'t rewrite your code in such a way that it iterates over multiple inputs, you can use LLMapReduce to submit your job with triples mode, see this example . Then you add a few lines to your code to take in two arguments, a process/task ID and the number of processes/tasks, and use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, fnames . In python I would add: # Grab the arguments that are passed in my_task_id = int(sys.argv[1]) num_tasks = int(sys.argv[2]) # Assign indices to this process/task my_fnames = fnames[my_task_id:len(fnames):num_tasks] for f in my_fnames: ... Notice that I am iterating over my_fnames , which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this). Step 2: Changing Your Submission Script \uf0c1 If you have been running your Job Arrays with sbatch, you most likely have a few environment variables in your submission script. To submit with LLsub triples, you can just replace these: $SLURM_ARRAY_TASK_ID -> $LLSUB_RANK $SLURM_ARRAY_TASK_COUNT -> $LLSUB_SIZE If you have any SBATCH flags in your submission script, remove those as well (LLsub will see these and submit with sbatch, ignoring any command line arguments you give it). For example, if you are running a python script, your final submission script will look something like this: #!/bin/bash # Initialize and Load Modules source /etc/profile module load anaconda/2020a echo \"My task ID: \" $LLSUB_RANK echo \"Number of Tasks: \" $LLSUB_SIZE python top5each.py $LLSUB_RANK $LLSUB_SIZE You will also need to make your script executable. You can do that with this simple command line command: chmod u+x submit_LLsub.sh Step 3: Submit your Job with LLsub Triples \uf0c1 Now when you submit your job, you can run: LLsub ./submit.sh [NODES,NPPN,NT] where NODES is the number of nodes you want to use up NPPN is the number of processes that should run per node NT is the number of threads per process The total number of threads per node, or NT*NPPN , should not be more than the number of cores on the node, otherwise you may overwhelm the node with too many running processes and/or threads. For example, if you are running on the 48-core Xeon-P8 nodes, if you are running with NT =1, you should not set NPPN more than 48. If NT =2, NPPN should be at most 24, etc. The numbers you choose depend on your application, if it is multithreaded it may be worth increasing NT and decreasing NPPN . If your application consumes a lot of memory, you may need to decrease NPPN so each process has the memory it needs to proceed. The best way to determine what numbers to choose is to tune your triples <#tuning> {.interpreted-text role=\"ref\"}, which is a relatively quick exercise and can improve your speedup in the long run by helping you select the ideal numbers for your triple. So if you want to run on 2 nodes, 10 processes per node, and 4 threads per process, you would run: LLsub ./submit.sh [2,10,4] If you were to run LLstat after running this command, you would see what looks like a 2 task job array, for example: $ LLstat LLGrid: txe1 (running slurm-wlm 20.11.3) JOBID ARRAY_JOB_ NAME USER START_TIME PARTITION CPUS FEATURES MIN_MEMORY ST NODELIST(REASON) 9651412_1 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-2 9651412_2 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-1 However, you are still running 2*10 = 20 total processes. Triples mode uses slurm to request full nodes, then takes care of launching the processes on each node. This is why you\\'ll always see one task for each node in the LLstat output, rather than each process. One advantage of this is it makes for a more compact LLstat output that is easier to read, instead of having to scroll through tons of tasks. You can check on your individual processes by looking at the log files. LLsub with Triples mode will create a directory with the prefix \\\"LLSUB\\\" followed by a unique number to hold all the log files. Within this directory will be one launch log file, which will capture any errors that occur during launch ,and one directory per node and put the log files for each process in its node\\'s directory. These subdirectories are labeled with the process ID range and the node name. For example, here are the log files from a triples run using [2,4,10]: $ ls LLSUB.23004 README.md helpers.py submit_LLsub.sh submit_sbatch.sh top5each.py $ ls LLSUB.23004/ llsub-triple-mode-launch.log-10006591 p0-p3_d-19-4-1 p4-p7_d-19-3-4 $ ls LLSUB.23004/p0-p3_d-19-4-1/ submit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7 $ ls LLSUB.23004/p4-p7_d-19-3-4/ submit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7 In this example, LLSUB.23004 is the directory containing the log files, llsub-triple-mode-launch.log-10006591 is the log file for the job launch. p0-p3_d-19-4-1 and p4-p7_d-19-3-4 are the directories for each node, and submit_LLsub.sh.log.0 , submit_LLsub.sh.log.1 , ..., submit_LLsub.sh.log.7 are the log files for each process.","title":"Job Array Triples"},{"location":"using-the-system/submitting-jobs/job-array-triples/#job-arrays-with-llsub-triples-in-3-steps-job_array_triples","text":"If you are currently running a Job Array, you can take advantage of SuperCloud\\'s triples mode submission by submitting your job array with LLsub. In most cases, this can be done in a few easy steps. What is triples mode? It\\'s a different way to submit your job by specifying three numbers: Nodes: The number of nodes you want to use up NPPN: The number of processes that should run per node NT: The number of threads per process Why would you want to use triples mode to submit your job? Triples mode provides a few advantages. Since it does whole-node scheduling you don\\'t have to worry about other jobs impacting yours (or your job impacting someone else\\'s). We also do task-pinning when you request resources with a triple, so your processes are arranged in the best way possible for the layout of the hardware architecture. Finally, submitting a Job Array with LLsub triples mode the way we describe here will greatly reduce the startup time for your jobs, over a job array with many pending tasks.","title":"Job Arrays with LLsub Triples in 3 Steps {#job_array_triples}"},{"location":"using-the-system/submitting-jobs/job-array-triples/#step-1-batch-up-your-array","text":"The first thing you want to do is set up your job so that your script splits up your inputs among each process/task and each process/task iterates a subset of your inputs. This way you can change your input set without changing the number of tasks or processes that you schedule. This step alone will save you on startup time. Anytime you submit more jobs than your allocation allows, those additional jobs will remaining pending until some of your first running jobs complete. Then, when one of your first jobs complete, the scheduler now has to find the pending job some resources and start it running. If you batch up your job array, you only have to go through the scheduling process once. If you\\'ve set up a job array following our instructions in the past you may already be doing this, and you can skip to the next section. First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you\\'ll first want to enumerate all the combinations of what you are iterating over so you have one big loop. If your code is written so it uses the $SLURM_ARRAY_TASK_ID and uses that to run a single thing, first add a for loop that iterates over the full set of things you want to run your code on. If you can\\'t rewrite your code in such a way that it iterates over multiple inputs, you can use LLMapReduce to submit your job with triples mode, see this example . Then you add a few lines to your code to take in two arguments, a process/task ID and the number of processes/tasks, and use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, fnames . In python I would add: # Grab the arguments that are passed in my_task_id = int(sys.argv[1]) num_tasks = int(sys.argv[2]) # Assign indices to this process/task my_fnames = fnames[my_task_id:len(fnames):num_tasks] for f in my_fnames: ... Notice that I am iterating over my_fnames , which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this).","title":"Step 1: Batch Up your Array"},{"location":"using-the-system/submitting-jobs/job-array-triples/#step-2-changing-your-submission-script","text":"If you have been running your Job Arrays with sbatch, you most likely have a few environment variables in your submission script. To submit with LLsub triples, you can just replace these: $SLURM_ARRAY_TASK_ID -> $LLSUB_RANK $SLURM_ARRAY_TASK_COUNT -> $LLSUB_SIZE If you have any SBATCH flags in your submission script, remove those as well (LLsub will see these and submit with sbatch, ignoring any command line arguments you give it). For example, if you are running a python script, your final submission script will look something like this: #!/bin/bash # Initialize and Load Modules source /etc/profile module load anaconda/2020a echo \"My task ID: \" $LLSUB_RANK echo \"Number of Tasks: \" $LLSUB_SIZE python top5each.py $LLSUB_RANK $LLSUB_SIZE You will also need to make your script executable. You can do that with this simple command line command: chmod u+x submit_LLsub.sh","title":"Step 2: Changing Your Submission Script"},{"location":"using-the-system/submitting-jobs/job-array-triples/#step-3-submit-your-job-with-llsub-triples","text":"Now when you submit your job, you can run: LLsub ./submit.sh [NODES,NPPN,NT] where NODES is the number of nodes you want to use up NPPN is the number of processes that should run per node NT is the number of threads per process The total number of threads per node, or NT*NPPN , should not be more than the number of cores on the node, otherwise you may overwhelm the node with too many running processes and/or threads. For example, if you are running on the 48-core Xeon-P8 nodes, if you are running with NT =1, you should not set NPPN more than 48. If NT =2, NPPN should be at most 24, etc. The numbers you choose depend on your application, if it is multithreaded it may be worth increasing NT and decreasing NPPN . If your application consumes a lot of memory, you may need to decrease NPPN so each process has the memory it needs to proceed. The best way to determine what numbers to choose is to tune your triples <#tuning> {.interpreted-text role=\"ref\"}, which is a relatively quick exercise and can improve your speedup in the long run by helping you select the ideal numbers for your triple. So if you want to run on 2 nodes, 10 processes per node, and 4 threads per process, you would run: LLsub ./submit.sh [2,10,4] If you were to run LLstat after running this command, you would see what looks like a 2 task job array, for example: $ LLstat LLGrid: txe1 (running slurm-wlm 20.11.3) JOBID ARRAY_JOB_ NAME USER START_TIME PARTITION CPUS FEATURES MIN_MEMORY ST NODELIST(REASON) 9651412_1 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-2 9651412_2 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-1 However, you are still running 2*10 = 20 total processes. Triples mode uses slurm to request full nodes, then takes care of launching the processes on each node. This is why you\\'ll always see one task for each node in the LLstat output, rather than each process. One advantage of this is it makes for a more compact LLstat output that is easier to read, instead of having to scroll through tons of tasks. You can check on your individual processes by looking at the log files. LLsub with Triples mode will create a directory with the prefix \\\"LLSUB\\\" followed by a unique number to hold all the log files. Within this directory will be one launch log file, which will capture any errors that occur during launch ,and one directory per node and put the log files for each process in its node\\'s directory. These subdirectories are labeled with the process ID range and the node name. For example, here are the log files from a triples run using [2,4,10]: $ ls LLSUB.23004 README.md helpers.py submit_LLsub.sh submit_sbatch.sh top5each.py $ ls LLSUB.23004/ llsub-triple-mode-launch.log-10006591 p0-p3_d-19-4-1 p4-p7_d-19-3-4 $ ls LLSUB.23004/p0-p3_d-19-4-1/ submit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7 $ ls LLSUB.23004/p4-p7_d-19-3-4/ submit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7 In this example, LLSUB.23004 is the directory containing the log files, llsub-triple-mode-launch.log-10006591 is the log file for the job launch. p0-p3_d-19-4-1 and p4-p7_d-19-3-4 are the directories for each node, and submit_LLsub.sh.log.0 , submit_LLsub.sh.log.1 , ..., submit_LLsub.sh.log.7 are the log files for each process.","title":"Step 3: Submit your Job with LLsub Triples"},{"location":"using-the-system/submitting-jobs/submitting-jobs/","text":"Submitting Jobs {#submitting_jobs} \uf0c1 For most job types, there are two ways to start the job: using the commands provided by the scheduler, Slurm, or using wrapper command, LLsub, that we have provided. LLsub creates a scheduler command based on the arguments you feed it, and will output that command to show you what it is running. The scheduler commands may provide more flexibility, and the wrapper commands may be easier to use in some cases and are scheduler agnostic. We show some of the more commonly used options. More Slurm options can be seen on the Slurm documentation page, and more LLsub options can be seen by running LLsub -h at the command line. There are two main types of jobs that you can run: interactive and batch jobs. Interactive jobs allow you to run interactively on a compute node in a shell. Batch jobs, on the other hand, are for running a pre-written script or executable. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Batch jobs are the traditional jobs you see on an HPC system and should be used when you want to run a script that doesn\\'t require that you interact with it. On this page we will go over: How to start an Interactive Job with LLsub <#interactive> {.interpreted-text role=\"ref\"} How to submit a Basic Serial job with LLsub and sbatch <#serial> {.interpreted-text role=\"ref\"} How to request more resources with sbatch <#sbatch> {.interpreted-text role=\"ref\"} How to request more resources with LLsub <#llsub> {.interpreted-text role=\"ref\"} How to submit an LLMapReduce Job <#llmapreduce> {.interpreted-text role=\"ref\"} How to submit a job with pMatlab, sbatch, or LaunchFunctionOnGrid <#matlab> {.interpreted-text role=\"ref\"} How to get the most performance out of LLsub, LLMapReduce, and pMatlab using Triples Mode <#triples> {.interpreted-text role=\"ref\"} You can find examples of several job types in the Teaching Examples github repository. They are also in the bwedx shared group directory and anyone with a Supercloud account can copy them to their home directory and use them as a starting point. How to start an Interactive Job with LLsub {##interactive} \uf0c1 Interactive jobs allow you to run interactively on a compute node in a shell. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Starting an interactive job with LLsub is very simple. To request a single core, run at the command line: LLsub -i As mentioned earlier on this page, when you run an LLsub command, you\\'ll see the Slurm command that is being run in the background when you submit the job. Once your interactive job has started, you\\'ll see the command line prompt has changed. It\\'ll say something like: USERNAME@d-14-13-1:~$ Where USERNAME is your username, and d-14-13-1 is the hostname of the machine you are on. This is how you know you are now on a compute node in an interactive job. By default you will be allocated a single CPU core. We have a number of options that allow you to request additional resources. You can always view these options and more by running LLsub -h . We\\'ll go over a few of those here. Note that these can (and often should) be combined. Full Exclusive Node: Add the word full to request an exclusive node. No one else will be on the machine with you: <!-- --> A number of cores: Use the -s option to request a certain number of CPU cores, or slots. Here, for example, we are requesting 4 cores: <!-- --> GPUs: Use the -g option to request a GPU. You need to specify the GPU type and the number of GPUs you want. You can request up to the number of GPUs on a single node. Refer to the systems_and_software {.interpreted-text role=\"ref\"} page to see how many GPUs are available per node. Remember you may want to also allocate some number of CPUs in addition to your GPUs. To get 20 CPUs and 1 Volta GPU (half the resources on our Xeon-G6 nodes), you would run: Submitting a Simple Serial Batch Job {##serial} \uf0c1 Submitting a batch job to the scheduler is the same for most languages. This starts by writing a submission script. This script should be a bash script (it should start with #!/bin/bash ) and contain the command(s) you need to run your code from the command line. It can also contain scheduler flags at the beginning of the script, or load modules or set environment variables you need to run your code. A job submission script for a simple, serial, batch job (for example, running a python script) looks like this: #!/bin/bash # Loading the required module source /etc/profile module load anaconda/2020a # Run the script python myScript.py The first line is the #!/bin/bash mentioned earlier. It looks like a comment, but it isn\\'t. This tells the machine how to interpret the script, that it is a bash script. Lines 3 and 4 demonstrate how to load a module in a submission script. The final line of the script runs your code. This should be the command you use to run your code from the command line, including any input arguments. This example is running a python script, therefore we have python myScript.py . Submitting with LLsub \uf0c1 To submit a simple batch job, you can use the LLsub command: LLsub myScript.sh Here myScript.sh can be a job submission script, or could be replaced by a compiled executable. The LLsub command, with no arguments, creates a scheduler command with some default options. If your submission script is myScript.sh , your output file will be myScript.sh.log-%j , where %j is a unique numeric identifier, the JobID for your job. The output file is where all the output for your job gets written. Anything that normally is written to the screen when you run your code, including any errors or print statements, will be printed to this file. When you run this command, the scheduler will find available resources to launch your job to. Then myScript.sh will run to completion, and the job will finish when the script is complete. Submitting with Slurm Scheduler Commands \uf0c1 To submit a simple batch job with the same default behavior as LLsub above, you would run: sbatch -o myScript.sh.log-%j myScript.sh Here myScript.sh can be a job submission script, or could be replaced by a compiled executable. The -o flag states the name of the file where any output will be written, the %j portion indicates job ID. If you do not include this flag, any output will be written to slurm-JOBID.out , which may make it difficult differentiate between job outputs. You can also incorporate this flag into your job submission script by adding lines starting with #SBATCH followed by the flag right after the first #!/bin/bash line: #!/bin/bash # Slurm sbatch options #SBATCH -o myScript.sh.log-%j # Loading the required module source /etc/profile module load anaconda/2020a # Run the script python myScript.py Like #!/bin/bash , these lines starting with #SBATCH look like comments, but they are not. As you add more flags to specify what resources your job needs, it becomes easier to specify them in your submission script, rather than having to type them out at the command line. If you incorporate Slurm flags in your script like this, you can submit it by running: sbatch myScript.sh When you run these commands, the scheduler will find available resources to launch your job to. Then myScript.sh will run to completion, and the job will finish when the script is complete. Note that when you start adding additional resources you need to make a choice between using LLsub and sbatch . If you have sbatch options in your submission script and submit it with LLsub , LLsub will ignore any additional command line arguments you give it and use those described in the script. Requesting Additional Resources with sbatch {##sbatch} \uf0c1 By default you will be allocated a single core for your job. This is fine for testing, but usually you\\'ll want more than that. For example you may want: Additional cores on multiple nodes (distributed) <#slurm-dist> {.interpreted-text role=\"ref\"} Additional cores on the same node (shared memory or threading) <#slurm-shared> {.interpreted-text role=\"ref\"} Multiple independent tasks (job array/throughput) <#slurm-jobarray> {.interpreted-text role=\"ref\"} Exclusive node(s) <#slurm-exclusive> {.interpreted-text role=\"ref\"} More memory or cores per process/task/worker <#slurm-memcores> {.interpreted-text role=\"ref\"} GPUs <#slurm-gpus> {.interpreted-text role=\"ref\"} Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want. How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python\\'s multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce <#llmapreduce> {.interpreted-text role=\"ref\"}). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU. Additional Cores on Multiple Nodes {##slurm-dist} \uf0c1 The flag to request a certain number of cores that can be on more than one node is --ntasks , or -n for short. A task is Slurm\\'s terminology for an individual process or worker. For example, to request 4 tasks you can add the following to your submission script: #SBATCH -n 4 You can control how many nodes these tasks are split onto using the --nodes , or -N . Your tasks will be split evenly across the nodes you request. For example, if I were to have the following in my script: #SBATCH -n 4 #SBATCH -N 2 I would have four tasks on two nodes, two tasks on each node. Specify the number of nodes like this does not ensure that you have exclusive access to those nodes. It will by default allocate one core for each task, so in this case you\\'d get a total of four cores, two on each node. If you need more than one core for each task, take a look at the cpus-per-task <#slurm-memcores> {.interpreted-text role=\"ref\"} option, and if you need exclusive access to those nodes see the exclusive <#slurm-exclusive> {.interpreted-text role=\"ref\"} option. Additional Cores on the Same Node {##slurm-shared} \uf0c1 There are technically two ways to do this. You can use the same options as requesting tasks on multiple nodes and setting the number of Nodes to 1, say we want four cores: #SBATCH -n 4 #SBATCH -N 1 Or you can use -c , or the --cpus-per-task option by itself: #SBATCH -c 4 As far as the number of cores you get, the result will be the same. You\\'ll get the four cores on a single node. There is a bit of a nuance on how Slurm sees it. The first allocates four tasks all on one node. The second allocates a single task with four CPUs or cores. You don\\'t need to worry too much about this, choose whichever makes the most sense to you. Job Arrays {##slurm-jobarray} \uf0c1 NOTE: We encourage everyone who runs a job array to use LLsub with Triples mode. See the page job_array_triples {.interpreted-text role=\"ref\"} to see how to set this up. A simple way to run the same script or command with different parameters or on different files in parallel is by using a Job Array. With a Job Array, the parallelism happens at the Scheduler level and is completely language agnostic. The best way to use a Job Array is to batch up your parameters so you have a finite number of tasks each running a set of parameters, rather than one task for each parameter. In your submission script you specify numeric indices, corresponding to the number of tasks that you want running at once. Those indices, or Task IDs are captured in environment variables, along with the total number of tasks, and passed into your script. Your script then has the information it needs to split up the work among tasks. This process is described in the Teaching Examples github repository, with examples in Julia and Python . First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you\\'ll first want to enumerate all the combinations of what you are iterating over so you have one big loop. Then you want to add a few lines to your code to take in two arguments, the Task ID and the number of tasks, use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, fnames . In python I would add: # Grab the arguments that are passed in my_task_id = int(sys.argv[1]) num_tasks = int(sys.argv[2]) # Assign indices to this process/task my_fnames = fnames[my_task_id-1:len(fnames):num_tasks] for f in my_fnames: ... Notice that I am iterating over my_fnames , which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this). The submission script will look like this: #!/bin/bash #SBATCH -o myScript.sh.log-%j-%a #SBATCH -a 1-4 python top5each.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT The -a (or --array ) option is where you specify your array indices, or task IDs. Here I am creating an array with four tasks by specifying 1 \\\"through\\\" 4. When the scheduler starts your job, it will start up four independent tasks, each will run this script, and each will have #SLURM_ARRAY_TASK_ID set to its task ID. Similarly, $SLURM_ARRAY_TASK_COUNT will be set to the total number of tasks, in this case 4. You may have noticed that there is an additional %a in the output file name. There will be one output file for each task in the array, and the %a puts the task ID on at the end of the filename, so you know which file goes with which task. By default you will get one core for each task in the array. If you need more than one core for each task, take a look at the cpus-per-task <#slurm-memcores> {.interpreted-text role=\"ref\"} option, and if you need to add a GPU to each task, check out the the GPUs <#slurm-gpus> {.interpreted-text role=\"ref\"} section. Exclusive Nodes {##slurm-exclusive} \uf0c1 Requesting an exclusive node ensures that there will be no other users on the node with you. You might want to do this when you know you need to make use of the full node, when you are running performance tests, or when you think your program might affect other users. There is some software that have not been designed for a shared HPC environment, and so use all the cores on the node, whether you have allocated them or not. You can look through their documentation to see if there is a way to limit the number of cores it uses, or you can request an exclusive node. Another situation where you might affect other users is when you don\\'t yet know what resources your code requires. For these first few runs it makes sense to request an exclusive node, and then look at the resources that your job used, and request those resources in the future. To request an exclusive node or nodes, you can add the following option: #SBATCH --exclusive This will ensure that wherever the tasks in your job land, those nodes will be exclusive. If you have four tasks, for example, specified with either -n ( --ntasks ) or in a job array, and those four tasks fall on the same node, you will get that one node exclusively. It will not force each task onto its own exclusive node without adding other options. Adding More Memory or Cores per Task {##slurm-memcores} \uf0c1 You can ensure that each task has more than one core or the default amount of memory the same way. By default, each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the systems_and_software {.interpreted-text role=\"ref\"} page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, with the Xeon-P8 nodes, they have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do this is using the --cpus-per-task , or -c option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I\\'d want to request five cores for each task: #SBATCH -c 5 This works nicely with both the -n ( --ntasks ) and -a ( --array ) options. As the flag name implies, you will get 5 cpu cores for every task in your job. If you are already using the -c option for a shared memory or threaded job, you can either use the -n and -N 1 alternative and save -c for adding additional memory, or you can increase what you put for -c . For example, if I know I\\'m going to use 4 cores in my code, but each will need 20 GB of RAM, I can request a total of 4*5 = 20 cores. How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first in [exclusive \\<#slurm-exclusive>]{.title-ref} mode so your job can have access to the maximum amount of memory. Then you can use the sacct slurm command to get the memory used: sacct -j JOBID -o JobID,JobName,State,AllocCPUS,MaxRSS,MaxVMSize --units=G where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job array job, while MaxVMSize is the maximum memory that was requested by the process (the peak memory usage). In other words, MaxVMSize is the high-watermark of memory that was allocated by the process, regardless of whether it was used or not. The MaxRSS size is the maximum physical memory that was actually used. If the MaxVMSize value is larger than the per-slot/core memory limit for the compute node (again, check the systems_and_software {.interpreted-text role=\"ref\"} page to get this for the resource type you are requesting), you will have to request additional memory for your job. This formatting for the accounting data prints out a number of memory datapoints for the job. They are all described in the sacct man page . Reqesting GPUs {##slurm-gpus} \uf0c1 Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA. Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, add the following line to your submission script: #SBATCH --gres=gpu:volta:1 This flag will give you a single GPU. For multi-node jobs, it\\'ll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task. Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our systems_and_software {.interpreted-text role=\"ref\"} page. Requesting Additional Resources with LLsub {##llsub} \uf0c1 By default you will be allocated a single core for your job. This is fine for testing, but usually you\\'ll want more than that. For example you may want: Additional cores on the same node (shared memory or threading) <#llsub-shared> {.interpreted-text role=\"ref\"} Multiple independent tasks (job array/throughput) <#llsub-jobarray> {.interpreted-text role=\"ref\"} More memory or cores per process/task/worker <#llsub-memcores> {.interpreted-text role=\"ref\"} GPUs <#llsub-gpus> {.interpreted-text role=\"ref\"} Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want. How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python\\'s multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce <#llmapreduce> {.interpreted-text role=\"ref\"}). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU. If you are submitting your job with LLsub, you should be aware of its behavior. If you have any Slurm options in your submission script (any lines starting with #SBATCH ) LLsub will ingore any command line arguments you give it and only use those you specify in your script. You can still submit this script with LLsub, but it won\\'t add any extra command line arguments you pass it. Additional Cores on the Same Node[]{#additional-cores-on-the-same-node-1} {##llsub-shared} \uf0c1 Libraries that use shared memory or threading to handle parallelism require that all cores be on the same node. In this case you are constrained to the number of cores on a single machine. Check the systems_and_software {.interpreted-text role=\"ref\"} page to see the number of cores available on the current hardware. To request multiple cores on the same node for your job you can use the -s option in LLsub . This stands for \\\"slots\\\". For example, if I am running a job and I\\'d like to allocate 4 cores to it, I would run: LLsub myScript.sh -s 4 Job Array {##llsub-jobarray} \uf0c1 Take a look at the Slurm instructions above for how to set up a Job Array <#slurm-jobarray> {.interpreted-text role=\"ref\"}. You\\'ll still set up your code the same, and pass the two environment variables #SLURM_ARRAY_TASK_ID and $SLURM_ARRAY_TASK_COUNT into your script. When you submit, rather than adding #SBATCH lines to your submission script, you would use the -t option: LLsub myScript.sh -t 1-4 If you need more cores or memory for each task, you can add the -s option as described below <#llsub-memcores> {.interpreted-text role=\"ref\"}. Adding More Memory or Cores {##llsub-memcores} \uf0c1 If you anticipate that your job will use more than \\~4 GB of RAM, you may need to allocate more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. Each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the systems_and_software {.interpreted-text role=\"ref\"} page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, the Xeon-P8 nodes have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do with LLsub is the -s (for slots) option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I\\'d want to request five cores for each task: LLsub myScript.sh -s 5 If you are already using the -s option for a shared memory or threaded job, you should increase what you put for -s . For example, if I know I\\'m going to use 4 cores in my code, but each will need 20 GB of RAM, I can reqest a total of 4*5 = 20 cores: LLsub myScript.sh -s 20 How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first (you can request the maximum number of cores per node for this step). Then you can use the sacct slurm command to get the memory used: sacct -j JOBID -o JobID,JobName,State,AllocCPUS,MaxRSS,MaxVMSize --units=G where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job array job, while MaxVMSize is the maximum memory that was requested by the process (the peak memory usage). In other words, MaxVMSize is the high-watermark of memory that was allocated by the process, regardless of whether it was used or not. The MaxRSS size is the maximum physical memory that was actually used. If the MaxVMSize value is larger than the per-slot/core memory limit for the compute node (again, check the systems_and_software {.interpreted-text role=\"ref\"} page to get this for the resource type you are requesting), you will have to request additional memory for your job. This formatting for the accounting data prints out a number of memory data points for the job. They are all described in the sacct man page . Requesting GPUs {##llsub-gpus} \uf0c1 Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA. Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, use the following command: LLsub myScript.sh -g volta:1 This flag will give you a single GPU. For multi-node jobs, it\\'ll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task. Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our systems_and_software {.interpreted-text role=\"ref\"} page. To request 20 cores and 1 GPU, run: LLsub myScript.sh -s 20 -g volta:1 LLMapReduce {##llmapreduce} \uf0c1 The LLMapReduce command scans the user-specified input directory and translates each individual file as a computing task for the user-specified application. Then, the computing tasks will be submitted to scheduler for processing. If needed, the results can be post-processed by setting up a user-specified reduce task, which is dependent on the mapping task results. The reduce task will wait until all the results become available. You can view the most up-to-date options for the LLMapReduce command by running the command LLMapReduce -h. You can see examples of how to use LLMapReduce jobs in /usr/local/examples directory on the Supercloud system nodes. Some of these may be in the examples directory in your home directory. You can copy any that are missing from /usr/local/examples to your home directory. We also have an example in the Teaching Examples github repository, with examples in Julia and Python . These examples are also available in the bwedx shared group directory and can be copied to your home directory from there. LLMapReduce can work with any programs and we have a couple of examples for Java, Matlab, Julia, and Python. By default, it cleans up the temporary directory, .MAPRED.PID. However, there is an option to keep (--keep true) the temporary directory if you want it for debugging. The current version also supports a nested LLMapReduce call. Matlab/Octave Tools {##matlab} \uf0c1 pMatlab \uf0c1 pMatlab was created at MIT Lincoln Laboratory to provide easy access to parallel computing for engineers and scientists using the MATLAB(R) language. pMatlab provides the interfaces to the communication libraries necessary for distributed computation. In addition to MATLAB(R), pMatlab works seamlessly with Octave, and open source Matlab toolkit. MATLAB(R) is the primary development language used by Laboratory staff, and thus the place to start when developing an infrastructure aimed at removing the traditional hurdles associated with parallel computing. In an effort to develop a tool that will enable the researcher to seamlessly move from desktop (serial) to parallel computing, pMatlab has adopted the use of Global Array Semantics. Global Array Semantics is a parallel programming model in which the programmer views an array as a single global array rather than multiple subarrays located on different processors. The ability to access and manipulate related data distributed across processors as a single array more closely matches the serial programming model than the traditional parallel approach, which requires keeping track of which data resides on any given individual processor. Along with global array semantics, pMatlab uses the message-passing capabilities of MatlabMPI to provide a global array interface to MATLAB(R)) programmers. The ultimate goal of pMatlab is to move beyond basic messaging (and its inherent programming complexity) towards higher level parallel data structures and functions, allowing MATLAB(R)) users to parallelize their existing programs by simply changing and adding a few lines. Any pMatlab code can be run on the MIT Supercloud using standard pMatlab submission commands. The Practical High Performance Computing course on our online course platform provides a very good introduction for how to use pMatlab. There is also an examples directory in your home directory that provides several examples. The Param_Sweep example is a good place to start. There is an in-depth explanation of this example in the Teaching Examples github repository. If you anticipate that your job will use more than \\~10 GB of RAM, you need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our nodes have 40 cores and 384 GB of RAM, therefore each core represents about 10 GB. So if your job needs \\~20 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with someone else\\'s job. To do this with pMatlab, you can add the following line to your run script, before you the eval(pRUN(...)) command: setenv('GRIDMATLAB_MT_SLOTS','2') Submitting with LLsub or Sbatch \uf0c1 You can always submit a Matlab(R) script with a submission script through sbatch or LLsub. The basic submission script looks like this: #!/bin/bash # Run the script matlab -nodisplay -r \"myScript; exit\" Where myScript is the name of the Matlab script that you want to run. When running a Matlab script through a submission script, you do need to specify that Matlab should exit after it runs your code. Otherwise it will continue to run, waiting for you to give it the next command. LaunchFunctionOnGrid and LaunchParforOnGrid \uf0c1 If you want to launch your serial MATLAB scripts or functions on LLSC systems, you can use the LaunchFunctionOnGrid() function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. Its usage, in Matlab, is as follows: launch_status = LaunchFunctionOnGrid(m_file) launch_status = LaunchFunctionOnGrid(m_file,variables) Where m_file is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants. If you want to launch your MATLAB scripts or functions that call the parfor() function on LLSC systems, you can use the LaunchParforOnGrid() function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. While LaunchParforOnGrid() will work functionally, it has significant limitations in performance, both at the node level and the cluster level; it might be better to use pMatlab instead. To use the LaunchParforOnGrid() function in MATLAB: launch_status = LaunchParforOnGrid(m_file) launch_status = LaunchParforOnGrid(m_file,variables) Where m_file is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants. If you anticipate that your job will use more than \\~10 GB of RAM, you need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our nodes have 40 cores and 384 GB of RAM, therefore each core represents about 10 GB. So if your job needs \\~20 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with another person\\'s job. To do this with LaunchFunctionOnGrid or LaunchParforOnGrid, you can add the following line to your run script, before you use the LaunchFunctionOnGrid() or LaunchParforOnGrid() command: setenv('GRIDMATLAB_MT_SLOTS','2') Triples Mode {##triples} \uf0c1 Triples mode is a way to launch pMatlab, LLsub Job Array <job_array_triples> {.interpreted-text role=\"ref\"}, and LLMapReduce jobs that gives you better performance and more flexibility to manage memory and threads. Unless you are requesting a small number of cores for your job, we highly encourage you to migrate to this model. With triples mode, you specify the resources for your job by providing 3 parameters: [Nodes NPPN NThreads] where | Nodes is number of compute nodes | NPPN is number of processes per node | NThreads is number of threads per process (default is 1) With triples mode your job will have exclusive use of each of the nodes that you request, so the total number of cores consumed against your allocation will be Nodes * 40. LLsub \uf0c1 A brief introduction to LLsub is provided above <#llsub-jobarray> {.interpreted-text role=\"ref\"}. To use triples mode to launch LLsub job on Supercloud, run as follows: LLsub ./submit.sh [Nodes,NPPN,NThreads] A more in-depth guide on how to convert an existing Job Array to an LLsub Triples submission is provided on the page job_array_triples {.interpreted-text role=\"ref\"}. LLMapReduce \uf0c1 A brief introduction to LLMapReduce is provided above <#llmapreduce> {.interpreted-text role=\"ref\"}. To use triples mode to launch your LLMapReduce job on Supercloud, use the --np option with the triple as its parameter, as follows: --np=[Nodes,NPPN,NThreads] pMatlab \uf0c1 A brief introduction to pMatlab is provided above <#matlab> {.interpreted-text role=\"ref\"}. To use triples mode to launch your pMatlab job on Supercloud, you use the pRUN() function. Its usage, in Matlab, is as follows: eval(pRUN('mfile', [Nodes NPPN OMP_NUM_THREADS], 'grid')) Triples Mode Tuning {##tuning} \uf0c1 Triples mode tuning provides greater efficiency by allowing you to better tune your resource requests to your application. This one-time tuning process typically takes \\~1 hour: Instrument your code to print a rate (work/time) giving a sense of the speed from a \\~1 minute run. Determine best number of threads ( NThreadsBest ) by examining rate from runs with varying numbers of threads: [1,1,1], [1,1,2], [1,1,4] , ... Determine best number of processes per node ( NPPNbest ) by examining rate from runs with varying numbers of processes: [1,1,NThreadsBest], [1,2,NThreadsBest], [1,4,NThreadsBest] , ... Determine best number of nodes ( NodesBest ) by examining rate from runs of with varying numbers of nodes: [1,NPPNbest,NThreadsBest], [2,NPPNbest NThreadsBest], [4,NPPNbest NThreadsBest] , ... Run your production jobs using [NodesBest NPPNbest NThreadsBest] You could tune NPPN first, then NThreads . This would be a better approach if you are memory bound. You can find the max NPPN that will fit, then keep increasing NThreads until you stop getting more performance. \\\"Good\\\" NPPN values for Xeon-P8: 1, 2, 4, 8, 16, 24, 32, 48 \\\"Good\\\" NPPN values for Xeon-G6: 1, 2, 4, 8, 16, 20, 32, 40 Triples mode tuning results in a \\~2x increase efficiency for many users. Once the best settings have been found, they can be reused as long as the code remains roughly similar. Recording the rates from the above process can often result in a publishable IEEE HPEC paper. We are happy to work with you to guide you through this tuning process.","title":"Submitting Jobs"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-jobs-submitting_jobs","text":"For most job types, there are two ways to start the job: using the commands provided by the scheduler, Slurm, or using wrapper command, LLsub, that we have provided. LLsub creates a scheduler command based on the arguments you feed it, and will output that command to show you what it is running. The scheduler commands may provide more flexibility, and the wrapper commands may be easier to use in some cases and are scheduler agnostic. We show some of the more commonly used options. More Slurm options can be seen on the Slurm documentation page, and more LLsub options can be seen by running LLsub -h at the command line. There are two main types of jobs that you can run: interactive and batch jobs. Interactive jobs allow you to run interactively on a compute node in a shell. Batch jobs, on the other hand, are for running a pre-written script or executable. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Batch jobs are the traditional jobs you see on an HPC system and should be used when you want to run a script that doesn\\'t require that you interact with it. On this page we will go over: How to start an Interactive Job with LLsub <#interactive> {.interpreted-text role=\"ref\"} How to submit a Basic Serial job with LLsub and sbatch <#serial> {.interpreted-text role=\"ref\"} How to request more resources with sbatch <#sbatch> {.interpreted-text role=\"ref\"} How to request more resources with LLsub <#llsub> {.interpreted-text role=\"ref\"} How to submit an LLMapReduce Job <#llmapreduce> {.interpreted-text role=\"ref\"} How to submit a job with pMatlab, sbatch, or LaunchFunctionOnGrid <#matlab> {.interpreted-text role=\"ref\"} How to get the most performance out of LLsub, LLMapReduce, and pMatlab using Triples Mode <#triples> {.interpreted-text role=\"ref\"} You can find examples of several job types in the Teaching Examples github repository. They are also in the bwedx shared group directory and anyone with a Supercloud account can copy them to their home directory and use them as a starting point.","title":"Submitting Jobs {#submitting_jobs}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#how-to-start-an-interactive-job-with-llsub-interactive","text":"Interactive jobs allow you to run interactively on a compute node in a shell. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Starting an interactive job with LLsub is very simple. To request a single core, run at the command line: LLsub -i As mentioned earlier on this page, when you run an LLsub command, you\\'ll see the Slurm command that is being run in the background when you submit the job. Once your interactive job has started, you\\'ll see the command line prompt has changed. It\\'ll say something like: USERNAME@d-14-13-1:~$ Where USERNAME is your username, and d-14-13-1 is the hostname of the machine you are on. This is how you know you are now on a compute node in an interactive job. By default you will be allocated a single CPU core. We have a number of options that allow you to request additional resources. You can always view these options and more by running LLsub -h . We\\'ll go over a few of those here. Note that these can (and often should) be combined. Full Exclusive Node: Add the word full to request an exclusive node. No one else will be on the machine with you: <!-- --> A number of cores: Use the -s option to request a certain number of CPU cores, or slots. Here, for example, we are requesting 4 cores: <!-- --> GPUs: Use the -g option to request a GPU. You need to specify the GPU type and the number of GPUs you want. You can request up to the number of GPUs on a single node. Refer to the systems_and_software {.interpreted-text role=\"ref\"} page to see how many GPUs are available per node. Remember you may want to also allocate some number of CPUs in addition to your GPUs. To get 20 CPUs and 1 Volta GPU (half the resources on our Xeon-G6 nodes), you would run:","title":"How to start an Interactive Job with LLsub {##interactive}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-a-simple-serial-batch-job-serial","text":"Submitting a batch job to the scheduler is the same for most languages. This starts by writing a submission script. This script should be a bash script (it should start with #!/bin/bash ) and contain the command(s) you need to run your code from the command line. It can also contain scheduler flags at the beginning of the script, or load modules or set environment variables you need to run your code. A job submission script for a simple, serial, batch job (for example, running a python script) looks like this: #!/bin/bash # Loading the required module source /etc/profile module load anaconda/2020a # Run the script python myScript.py The first line is the #!/bin/bash mentioned earlier. It looks like a comment, but it isn\\'t. This tells the machine how to interpret the script, that it is a bash script. Lines 3 and 4 demonstrate how to load a module in a submission script. The final line of the script runs your code. This should be the command you use to run your code from the command line, including any input arguments. This example is running a python script, therefore we have python myScript.py .","title":"Submitting a Simple Serial Batch Job {##serial}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-with-llsub","text":"To submit a simple batch job, you can use the LLsub command: LLsub myScript.sh Here myScript.sh can be a job submission script, or could be replaced by a compiled executable. The LLsub command, with no arguments, creates a scheduler command with some default options. If your submission script is myScript.sh , your output file will be myScript.sh.log-%j , where %j is a unique numeric identifier, the JobID for your job. The output file is where all the output for your job gets written. Anything that normally is written to the screen when you run your code, including any errors or print statements, will be printed to this file. When you run this command, the scheduler will find available resources to launch your job to. Then myScript.sh will run to completion, and the job will finish when the script is complete.","title":"Submitting with LLsub"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-with-slurm-scheduler-commands","text":"To submit a simple batch job with the same default behavior as LLsub above, you would run: sbatch -o myScript.sh.log-%j myScript.sh Here myScript.sh can be a job submission script, or could be replaced by a compiled executable. The -o flag states the name of the file where any output will be written, the %j portion indicates job ID. If you do not include this flag, any output will be written to slurm-JOBID.out , which may make it difficult differentiate between job outputs. You can also incorporate this flag into your job submission script by adding lines starting with #SBATCH followed by the flag right after the first #!/bin/bash line: #!/bin/bash # Slurm sbatch options #SBATCH -o myScript.sh.log-%j # Loading the required module source /etc/profile module load anaconda/2020a # Run the script python myScript.py Like #!/bin/bash , these lines starting with #SBATCH look like comments, but they are not. As you add more flags to specify what resources your job needs, it becomes easier to specify them in your submission script, rather than having to type them out at the command line. If you incorporate Slurm flags in your script like this, you can submit it by running: sbatch myScript.sh When you run these commands, the scheduler will find available resources to launch your job to. Then myScript.sh will run to completion, and the job will finish when the script is complete. Note that when you start adding additional resources you need to make a choice between using LLsub and sbatch . If you have sbatch options in your submission script and submit it with LLsub , LLsub will ignore any additional command line arguments you give it and use those described in the script.","title":"Submitting with Slurm Scheduler Commands"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-additional-resources-with-sbatch-sbatch","text":"By default you will be allocated a single core for your job. This is fine for testing, but usually you\\'ll want more than that. For example you may want: Additional cores on multiple nodes (distributed) <#slurm-dist> {.interpreted-text role=\"ref\"} Additional cores on the same node (shared memory or threading) <#slurm-shared> {.interpreted-text role=\"ref\"} Multiple independent tasks (job array/throughput) <#slurm-jobarray> {.interpreted-text role=\"ref\"} Exclusive node(s) <#slurm-exclusive> {.interpreted-text role=\"ref\"} More memory or cores per process/task/worker <#slurm-memcores> {.interpreted-text role=\"ref\"} GPUs <#slurm-gpus> {.interpreted-text role=\"ref\"} Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want. How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python\\'s multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce <#llmapreduce> {.interpreted-text role=\"ref\"}). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU.","title":"Requesting Additional Resources with sbatch {##sbatch}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#additional-cores-on-multiple-nodes-slurm-dist","text":"The flag to request a certain number of cores that can be on more than one node is --ntasks , or -n for short. A task is Slurm\\'s terminology for an individual process or worker. For example, to request 4 tasks you can add the following to your submission script: #SBATCH -n 4 You can control how many nodes these tasks are split onto using the --nodes , or -N . Your tasks will be split evenly across the nodes you request. For example, if I were to have the following in my script: #SBATCH -n 4 #SBATCH -N 2 I would have four tasks on two nodes, two tasks on each node. Specify the number of nodes like this does not ensure that you have exclusive access to those nodes. It will by default allocate one core for each task, so in this case you\\'d get a total of four cores, two on each node. If you need more than one core for each task, take a look at the cpus-per-task <#slurm-memcores> {.interpreted-text role=\"ref\"} option, and if you need exclusive access to those nodes see the exclusive <#slurm-exclusive> {.interpreted-text role=\"ref\"} option.","title":"Additional Cores on Multiple Nodes {##slurm-dist}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#additional-cores-on-the-same-node-slurm-shared","text":"There are technically two ways to do this. You can use the same options as requesting tasks on multiple nodes and setting the number of Nodes to 1, say we want four cores: #SBATCH -n 4 #SBATCH -N 1 Or you can use -c , or the --cpus-per-task option by itself: #SBATCH -c 4 As far as the number of cores you get, the result will be the same. You\\'ll get the four cores on a single node. There is a bit of a nuance on how Slurm sees it. The first allocates four tasks all on one node. The second allocates a single task with four CPUs or cores. You don\\'t need to worry too much about this, choose whichever makes the most sense to you.","title":"Additional Cores on the Same Node {##slurm-shared}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#job-arrays-slurm-jobarray","text":"NOTE: We encourage everyone who runs a job array to use LLsub with Triples mode. See the page job_array_triples {.interpreted-text role=\"ref\"} to see how to set this up. A simple way to run the same script or command with different parameters or on different files in parallel is by using a Job Array. With a Job Array, the parallelism happens at the Scheduler level and is completely language agnostic. The best way to use a Job Array is to batch up your parameters so you have a finite number of tasks each running a set of parameters, rather than one task for each parameter. In your submission script you specify numeric indices, corresponding to the number of tasks that you want running at once. Those indices, or Task IDs are captured in environment variables, along with the total number of tasks, and passed into your script. Your script then has the information it needs to split up the work among tasks. This process is described in the Teaching Examples github repository, with examples in Julia and Python . First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you\\'ll first want to enumerate all the combinations of what you are iterating over so you have one big loop. Then you want to add a few lines to your code to take in two arguments, the Task ID and the number of tasks, use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, fnames . In python I would add: # Grab the arguments that are passed in my_task_id = int(sys.argv[1]) num_tasks = int(sys.argv[2]) # Assign indices to this process/task my_fnames = fnames[my_task_id-1:len(fnames):num_tasks] for f in my_fnames: ... Notice that I am iterating over my_fnames , which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this). The submission script will look like this: #!/bin/bash #SBATCH -o myScript.sh.log-%j-%a #SBATCH -a 1-4 python top5each.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT The -a (or --array ) option is where you specify your array indices, or task IDs. Here I am creating an array with four tasks by specifying 1 \\\"through\\\" 4. When the scheduler starts your job, it will start up four independent tasks, each will run this script, and each will have #SLURM_ARRAY_TASK_ID set to its task ID. Similarly, $SLURM_ARRAY_TASK_COUNT will be set to the total number of tasks, in this case 4. You may have noticed that there is an additional %a in the output file name. There will be one output file for each task in the array, and the %a puts the task ID on at the end of the filename, so you know which file goes with which task. By default you will get one core for each task in the array. If you need more than one core for each task, take a look at the cpus-per-task <#slurm-memcores> {.interpreted-text role=\"ref\"} option, and if you need to add a GPU to each task, check out the the GPUs <#slurm-gpus> {.interpreted-text role=\"ref\"} section.","title":"Job Arrays {##slurm-jobarray}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#exclusive-nodes-slurm-exclusive","text":"Requesting an exclusive node ensures that there will be no other users on the node with you. You might want to do this when you know you need to make use of the full node, when you are running performance tests, or when you think your program might affect other users. There is some software that have not been designed for a shared HPC environment, and so use all the cores on the node, whether you have allocated them or not. You can look through their documentation to see if there is a way to limit the number of cores it uses, or you can request an exclusive node. Another situation where you might affect other users is when you don\\'t yet know what resources your code requires. For these first few runs it makes sense to request an exclusive node, and then look at the resources that your job used, and request those resources in the future. To request an exclusive node or nodes, you can add the following option: #SBATCH --exclusive This will ensure that wherever the tasks in your job land, those nodes will be exclusive. If you have four tasks, for example, specified with either -n ( --ntasks ) or in a job array, and those four tasks fall on the same node, you will get that one node exclusively. It will not force each task onto its own exclusive node without adding other options.","title":"Exclusive Nodes {##slurm-exclusive}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#adding-more-memory-or-cores-per-task-slurm-memcores","text":"You can ensure that each task has more than one core or the default amount of memory the same way. By default, each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the systems_and_software {.interpreted-text role=\"ref\"} page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, with the Xeon-P8 nodes, they have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do this is using the --cpus-per-task , or -c option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I\\'d want to request five cores for each task: #SBATCH -c 5 This works nicely with both the -n ( --ntasks ) and -a ( --array ) options. As the flag name implies, you will get 5 cpu cores for every task in your job. If you are already using the -c option for a shared memory or threaded job, you can either use the -n and -N 1 alternative and save -c for adding additional memory, or you can increase what you put for -c . For example, if I know I\\'m going to use 4 cores in my code, but each will need 20 GB of RAM, I can request a total of 4*5 = 20 cores. How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first in [exclusive \\<#slurm-exclusive>]{.title-ref} mode so your job can have access to the maximum amount of memory. Then you can use the sacct slurm command to get the memory used: sacct -j JOBID -o JobID,JobName,State,AllocCPUS,MaxRSS,MaxVMSize --units=G where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job array job, while MaxVMSize is the maximum memory that was requested by the process (the peak memory usage). In other words, MaxVMSize is the high-watermark of memory that was allocated by the process, regardless of whether it was used or not. The MaxRSS size is the maximum physical memory that was actually used. If the MaxVMSize value is larger than the per-slot/core memory limit for the compute node (again, check the systems_and_software {.interpreted-text role=\"ref\"} page to get this for the resource type you are requesting), you will have to request additional memory for your job. This formatting for the accounting data prints out a number of memory datapoints for the job. They are all described in the sacct man page .","title":"Adding More Memory or Cores per Task {##slurm-memcores}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#reqesting-gpus-slurm-gpus","text":"Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA. Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, add the following line to your submission script: #SBATCH --gres=gpu:volta:1 This flag will give you a single GPU. For multi-node jobs, it\\'ll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task. Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our systems_and_software {.interpreted-text role=\"ref\"} page.","title":"Reqesting GPUs {##slurm-gpus}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-additional-resources-with-llsub-llsub","text":"By default you will be allocated a single core for your job. This is fine for testing, but usually you\\'ll want more than that. For example you may want: Additional cores on the same node (shared memory or threading) <#llsub-shared> {.interpreted-text role=\"ref\"} Multiple independent tasks (job array/throughput) <#llsub-jobarray> {.interpreted-text role=\"ref\"} More memory or cores per process/task/worker <#llsub-memcores> {.interpreted-text role=\"ref\"} GPUs <#llsub-gpus> {.interpreted-text role=\"ref\"} Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want. How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python\\'s multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce <#llmapreduce> {.interpreted-text role=\"ref\"}). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU. If you are submitting your job with LLsub, you should be aware of its behavior. If you have any Slurm options in your submission script (any lines starting with #SBATCH ) LLsub will ingore any command line arguments you give it and only use those you specify in your script. You can still submit this script with LLsub, but it won\\'t add any extra command line arguments you pass it.","title":"Requesting Additional Resources with LLsub {##llsub}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#additional-cores-on-the-same-nodeadditional-cores-on-the-same-node-1-llsub-shared","text":"Libraries that use shared memory or threading to handle parallelism require that all cores be on the same node. In this case you are constrained to the number of cores on a single machine. Check the systems_and_software {.interpreted-text role=\"ref\"} page to see the number of cores available on the current hardware. To request multiple cores on the same node for your job you can use the -s option in LLsub . This stands for \\\"slots\\\". For example, if I am running a job and I\\'d like to allocate 4 cores to it, I would run: LLsub myScript.sh -s 4","title":"Additional Cores on the Same Node[]{#additional-cores-on-the-same-node-1} {##llsub-shared}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#job-array-llsub-jobarray","text":"Take a look at the Slurm instructions above for how to set up a Job Array <#slurm-jobarray> {.interpreted-text role=\"ref\"}. You\\'ll still set up your code the same, and pass the two environment variables #SLURM_ARRAY_TASK_ID and $SLURM_ARRAY_TASK_COUNT into your script. When you submit, rather than adding #SBATCH lines to your submission script, you would use the -t option: LLsub myScript.sh -t 1-4 If you need more cores or memory for each task, you can add the -s option as described below <#llsub-memcores> {.interpreted-text role=\"ref\"}.","title":"Job Array {##llsub-jobarray}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#adding-more-memory-or-cores-llsub-memcores","text":"If you anticipate that your job will use more than \\~4 GB of RAM, you may need to allocate more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. Each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the systems_and_software {.interpreted-text role=\"ref\"} page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, the Xeon-P8 nodes have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do with LLsub is the -s (for slots) option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I\\'d want to request five cores for each task: LLsub myScript.sh -s 5 If you are already using the -s option for a shared memory or threaded job, you should increase what you put for -s . For example, if I know I\\'m going to use 4 cores in my code, but each will need 20 GB of RAM, I can reqest a total of 4*5 = 20 cores: LLsub myScript.sh -s 20 How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first (you can request the maximum number of cores per node for this step). Then you can use the sacct slurm command to get the memory used: sacct -j JOBID -o JobID,JobName,State,AllocCPUS,MaxRSS,MaxVMSize --units=G where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job array job, while MaxVMSize is the maximum memory that was requested by the process (the peak memory usage). In other words, MaxVMSize is the high-watermark of memory that was allocated by the process, regardless of whether it was used or not. The MaxRSS size is the maximum physical memory that was actually used. If the MaxVMSize value is larger than the per-slot/core memory limit for the compute node (again, check the systems_and_software {.interpreted-text role=\"ref\"} page to get this for the resource type you are requesting), you will have to request additional memory for your job. This formatting for the accounting data prints out a number of memory data points for the job. They are all described in the sacct man page .","title":"Adding More Memory or Cores {##llsub-memcores}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-gpus-llsub-gpus","text":"Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA. Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, use the following command: LLsub myScript.sh -g volta:1 This flag will give you a single GPU. For multi-node jobs, it\\'ll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task. Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our systems_and_software {.interpreted-text role=\"ref\"} page. To request 20 cores and 1 GPU, run: LLsub myScript.sh -s 20 -g volta:1","title":"Requesting GPUs {##llsub-gpus}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#llmapreduce-llmapreduce","text":"The LLMapReduce command scans the user-specified input directory and translates each individual file as a computing task for the user-specified application. Then, the computing tasks will be submitted to scheduler for processing. If needed, the results can be post-processed by setting up a user-specified reduce task, which is dependent on the mapping task results. The reduce task will wait until all the results become available. You can view the most up-to-date options for the LLMapReduce command by running the command LLMapReduce -h. You can see examples of how to use LLMapReduce jobs in /usr/local/examples directory on the Supercloud system nodes. Some of these may be in the examples directory in your home directory. You can copy any that are missing from /usr/local/examples to your home directory. We also have an example in the Teaching Examples github repository, with examples in Julia and Python . These examples are also available in the bwedx shared group directory and can be copied to your home directory from there. LLMapReduce can work with any programs and we have a couple of examples for Java, Matlab, Julia, and Python. By default, it cleans up the temporary directory, .MAPRED.PID. However, there is an option to keep (--keep true) the temporary directory if you want it for debugging. The current version also supports a nested LLMapReduce call.","title":"LLMapReduce {##llmapreduce}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#matlaboctave-tools-matlab","text":"","title":"Matlab/Octave Tools {##matlab}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#pmatlab","text":"pMatlab was created at MIT Lincoln Laboratory to provide easy access to parallel computing for engineers and scientists using the MATLAB(R) language. pMatlab provides the interfaces to the communication libraries necessary for distributed computation. In addition to MATLAB(R), pMatlab works seamlessly with Octave, and open source Matlab toolkit. MATLAB(R) is the primary development language used by Laboratory staff, and thus the place to start when developing an infrastructure aimed at removing the traditional hurdles associated with parallel computing. In an effort to develop a tool that will enable the researcher to seamlessly move from desktop (serial) to parallel computing, pMatlab has adopted the use of Global Array Semantics. Global Array Semantics is a parallel programming model in which the programmer views an array as a single global array rather than multiple subarrays located on different processors. The ability to access and manipulate related data distributed across processors as a single array more closely matches the serial programming model than the traditional parallel approach, which requires keeping track of which data resides on any given individual processor. Along with global array semantics, pMatlab uses the message-passing capabilities of MatlabMPI to provide a global array interface to MATLAB(R)) programmers. The ultimate goal of pMatlab is to move beyond basic messaging (and its inherent programming complexity) towards higher level parallel data structures and functions, allowing MATLAB(R)) users to parallelize their existing programs by simply changing and adding a few lines. Any pMatlab code can be run on the MIT Supercloud using standard pMatlab submission commands. The Practical High Performance Computing course on our online course platform provides a very good introduction for how to use pMatlab. There is also an examples directory in your home directory that provides several examples. The Param_Sweep example is a good place to start. There is an in-depth explanation of this example in the Teaching Examples github repository. If you anticipate that your job will use more than \\~10 GB of RAM, you need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our nodes have 40 cores and 384 GB of RAM, therefore each core represents about 10 GB. So if your job needs \\~20 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with someone else\\'s job. To do this with pMatlab, you can add the following line to your run script, before you the eval(pRUN(...)) command: setenv('GRIDMATLAB_MT_SLOTS','2')","title":"pMatlab"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-with-llsub-or-sbatch","text":"You can always submit a Matlab(R) script with a submission script through sbatch or LLsub. The basic submission script looks like this: #!/bin/bash # Run the script matlab -nodisplay -r \"myScript; exit\" Where myScript is the name of the Matlab script that you want to run. When running a Matlab script through a submission script, you do need to specify that Matlab should exit after it runs your code. Otherwise it will continue to run, waiting for you to give it the next command.","title":"Submitting with LLsub or Sbatch"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#launchfunctionongrid-and-launchparforongrid","text":"If you want to launch your serial MATLAB scripts or functions on LLSC systems, you can use the LaunchFunctionOnGrid() function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. Its usage, in Matlab, is as follows: launch_status = LaunchFunctionOnGrid(m_file) launch_status = LaunchFunctionOnGrid(m_file,variables) Where m_file is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants. If you want to launch your MATLAB scripts or functions that call the parfor() function on LLSC systems, you can use the LaunchParforOnGrid() function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. While LaunchParforOnGrid() will work functionally, it has significant limitations in performance, both at the node level and the cluster level; it might be better to use pMatlab instead. To use the LaunchParforOnGrid() function in MATLAB: launch_status = LaunchParforOnGrid(m_file) launch_status = LaunchParforOnGrid(m_file,variables) Where m_file is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants. If you anticipate that your job will use more than \\~10 GB of RAM, you need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our nodes have 40 cores and 384 GB of RAM, therefore each core represents about 10 GB. So if your job needs \\~20 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with another person\\'s job. To do this with LaunchFunctionOnGrid or LaunchParforOnGrid, you can add the following line to your run script, before you use the LaunchFunctionOnGrid() or LaunchParforOnGrid() command: setenv('GRIDMATLAB_MT_SLOTS','2')","title":"LaunchFunctionOnGrid and LaunchParforOnGrid"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#triples-mode-triples","text":"Triples mode is a way to launch pMatlab, LLsub Job Array <job_array_triples> {.interpreted-text role=\"ref\"}, and LLMapReduce jobs that gives you better performance and more flexibility to manage memory and threads. Unless you are requesting a small number of cores for your job, we highly encourage you to migrate to this model. With triples mode, you specify the resources for your job by providing 3 parameters: [Nodes NPPN NThreads] where | Nodes is number of compute nodes | NPPN is number of processes per node | NThreads is number of threads per process (default is 1) With triples mode your job will have exclusive use of each of the nodes that you request, so the total number of cores consumed against your allocation will be Nodes * 40.","title":"Triples Mode {##triples}"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#llsub","text":"A brief introduction to LLsub is provided above <#llsub-jobarray> {.interpreted-text role=\"ref\"}. To use triples mode to launch LLsub job on Supercloud, run as follows: LLsub ./submit.sh [Nodes,NPPN,NThreads] A more in-depth guide on how to convert an existing Job Array to an LLsub Triples submission is provided on the page job_array_triples {.interpreted-text role=\"ref\"}.","title":"LLsub"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#llmapreduce","text":"A brief introduction to LLMapReduce is provided above <#llmapreduce> {.interpreted-text role=\"ref\"}. To use triples mode to launch your LLMapReduce job on Supercloud, use the --np option with the triple as its parameter, as follows: --np=[Nodes,NPPN,NThreads]","title":"LLMapReduce"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#pmatlab_1","text":"A brief introduction to pMatlab is provided above <#matlab> {.interpreted-text role=\"ref\"}. To use triples mode to launch your pMatlab job on Supercloud, you use the pRUN() function. Its usage, in Matlab, is as follows: eval(pRUN('mfile', [Nodes NPPN OMP_NUM_THREADS], 'grid'))","title":"pMatlab"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#triples-mode-tuning-tuning","text":"Triples mode tuning provides greater efficiency by allowing you to better tune your resource requests to your application. This one-time tuning process typically takes \\~1 hour: Instrument your code to print a rate (work/time) giving a sense of the speed from a \\~1 minute run. Determine best number of threads ( NThreadsBest ) by examining rate from runs with varying numbers of threads: [1,1,1], [1,1,2], [1,1,4] , ... Determine best number of processes per node ( NPPNbest ) by examining rate from runs with varying numbers of processes: [1,1,NThreadsBest], [1,2,NThreadsBest], [1,4,NThreadsBest] , ... Determine best number of nodes ( NodesBest ) by examining rate from runs of with varying numbers of nodes: [1,NPPNbest,NThreadsBest], [2,NPPNbest NThreadsBest], [4,NPPNbest NThreadsBest] , ... Run your production jobs using [NodesBest NPPNbest NThreadsBest] You could tune NPPN first, then NThreads . This would be a better approach if you are memory bound. You can find the max NPPN that will fit, then keep increasing NThreads until you stop getting more performance. \\\"Good\\\" NPPN values for Xeon-P8: 1, 2, 4, 8, 16, 24, 32, 48 \\\"Good\\\" NPPN values for Xeon-G6: 1, 2, 4, 8, 16, 20, 32, 40 Triples mode tuning results in a \\~2x increase efficiency for many users. Once the best settings have been found, they can be reused as long as the code remains roughly similar. Recording the rates from the above process can often result in a publishable IEEE HPEC paper. We are happy to work with you to guide you through this tuning process.","title":"Triples Mode Tuning {##tuning}"}]}