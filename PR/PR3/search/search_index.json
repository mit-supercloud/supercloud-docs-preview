{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SuperCloud Documentation","text":""},{"location":"acknowledging-us/","title":"Acknowledging Us","text":"<p>The canonical article about the MIT Supercloud is:</p> <p>Interactive Supercomputing on 40,000 Cores for Machine Learning and Data Analysis, Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor, Bill Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin, Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Peter Michaleas, paper presented at the 2018 IEEE High Performance Extreme Computing Conference (HPEC), July 2018.</p> <p>When referencing the MIT Supercloud, please use this reference. The link above is to IEEE Xplore, the paper can also be found on arXiv.org.</p> <p>For convenience, here is a bibtex listing you can copy and paste into your paper:</p> <p><code>@inproceedings{reuther2018interactive,   title={Interactive supercomputing on 40,000 cores for machine learning and data analysis},   author={Reuther, Albert and Kepner, Jeremy and Byun, Chansup and Samsi, Siddharth and Arcand, William and Bestor, David and Bergeron, Bill and Gadepally, Vijay and Houle, Michael and Hubbell, Matthew and Jones, Michael and Klein, Anna and Milechin, Lauren and Mullen, Julia and Prout, Andrew and Rosa, Antonio and Yee, Charles and Michaleas, Peter},   booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)},   pages={1--6},   year={2018},   organization={IEEE}   }</code></p> <p>If you would like to acknowledge the MIT Supercloud in your paper or report, we recommend the following (be sure to select the applicable resource(s) from among the listed resources that we provide):</p> <p>The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing (HPC, database, consultation) resources that have contributed to the research results reported within this paper/report.</p> <p>Thank you for acknowledging us -- we appreciate it.</p>"},{"location":"faqs/","title":"Frequently Asked Questions","text":"<p>This page contains the answers to a few questions that we receive often. As more questions are asked, this page may be updated.</p>"},{"location":"faqs/#how-do-i-get-an-account","title":"How do I get an account?","text":"<p>To request an account, follow the instructions and answer the questions on our Account Request page. We will reach out to you once your account is created, or if we have any questions for you.</p>"},{"location":"faqs/#i-would-like-to-log-in-from-a-new-computer-can-i-add-a-new-ssh-key","title":"I would like to log in from a new computer. Can I add a new ssh key?","text":"<p>If you have a new computer, or want to add keys for additional computers that you use, you can add your own key on our web portal. Instructions on how to generate a new ssh key and add it to your account are on our Account Request page. In summary, log in with your credentials (for MIT and other educational institutions this is the middle option when you go to https://txe1-portal.mit.edu) and then click on the \"sshkeys\" link. Scroll to the bottom and paste your key in the box.</p>"},{"location":"faqs/#how-much-storage-do-i-have-for-my-account","title":"How much storage do I have for my account?","text":"<p>We have set some guardrails on home and group directory storage. Home directories are set to 10TB and group directories are set to 50TB. It is recommended that users not use their accounts as primary storage. Further, we do not back up the storage on the system, so we strongly recommend transferring your code, data, and any other important files to another machine for backup.</p>"},{"location":"faqs/#how-can-i-share-filescodedata-with-my-colleagues","title":"How can I share files/code/data with my colleagues?","text":"<p>If you would like to share files with others you can request a shared group directory. Shared group directories are located at /home/gridsan/groups and we will put a symlink in your home directory to use as a shortcut to your shared group directory. To request one, send email to supercloud\\@mit.edu and let us know:</p> <ol> <li>What the group should be called. Short, descriptive names are best.</li> <li>Who should be the owner/approver for the group. We will ask this     person for approval whenever we receive a request to join a group.</li> <li>Who should be in the group. Supercloud usernames are helpful, but     not required.</li> <li>Whether you plan to store any non-public data in the group. If you     do, let us know what requirements, restrictions, or agreements are     associated with the data. See why we ask     here.</li> </ol> <p>To learn more about Shared Groups and best practices using them, see the page on Shared Groups.</p>"},{"location":"faqs/#how-do-i-setchange-my-password","title":"How do I set/change my password?","text":"<p>We do not use passwords on SuperCloud. If you have an active MIT Kerberos or login from another University, you can log in using your institution's credentials. On the Supercloud Web Portal Login page, select the middle option \"MIT Touchstone/InCommon Federation\". You may have to select your institution from the dropdown list, which should take you to your institution's login page. After you log in, you should see the Portal main page. If you have trouble logging in this way, please contact us and we can help.</p> <p>If you cannot log in using \"MIT Touchstone/InCommon Federation\" or one of the other options on the portal, we may set you up with a password. If you have not yet reset your password, or remember your previous password, then follow the instructions on the Web Porta page. If you have previously set your password and cannot remember it, contact us and we will help you reset your password.</p>"},{"location":"faqs/#are-there-any-resource-limits","title":"Are there any resource limits?","text":"<p>New accounts are created with a small starting resource allocation. Once you have completed the Practical HPC course you can send your certificate  to supercloud@mit.edu to request to be moved to the standard allocation. The starting and standard allocations are listed on the Systems and Software page.</p> <p>If you have a deadline and need additional resources you can request more by contacting us. If you looking to request more GPUs, please read through this page on Optimizing your GPU Jobs first. Please state the number of additional processors you need, the length of time for which you need it, and tell us about the jobs you are running and how you are submitting them. If you plan to run many independent jobs we will ask you to convert your job to use triples before giving and increased allocation. Remember this is a shared system, so during busy times we may not be able to grant your request. We will also only grant increase requests if you have completed the Practical HPC course.</p> <p>It is also important to keep in mind what your fair share of memory is for each process and request additional resources if needed. For example, if there are 40 cores and 384GB of RAM on the machine you are using, each processor's fair share would be about 9GB. Check the Systems and Software page to see how many cores and how much memory each node type has. If you think your processes will go over this, request additional slots as needed. This ensures you have sufficient memory without killing your job.</p>"},{"location":"faqs/#what-do-i-do-if-my-job-wont-be-deleted","title":"What do I do if my job won't be deleted?","text":"<p>Occasionally this will happen if the node where your job is running goes down, or your job does not exit gracefully. If this happens, contact us with the Job ID, and we'll delete the job and reboot the node if needed.</p>"},{"location":"faqs/#why-do-i-get-an-error-when-i-try-to-install-a-package","title":"Why do I get an error when I try to install a package?","text":"<p>There are two common reasons you get an error when you try to install a package. If you get a \"Permission Denied\" or similar error, it is because you are trying to install the package system-wide, rather than your own home directory. See the Software and Package Management page for more information on how to install packages.</p> <p>If you get a \"Network Error\", or similar, this is because we don't have internet/network connection on the compute nodes, this includes Jupyter and any interactive jobs. You will have to install the package on one of the login nodes.</p> <p>If you get an error like <code>Could not install packages due to an EnvironmentError: [Errno 122] Disk quota exceeded</code> when installing a package with pip or something like <code>ERROR: could not download https://pkg.julialang.org/registry/...</code> installing a package with Julia, even though you are on the login node, this is because it is filling up your quota in the <code>/tmp</code> directory. We have set quotas on this directory to prevent a single person from inadvertently filling it up, as when this happens it can cause issues for everyone using the node, including preventing anyone from installing packages. This can be fixed by setting the <code>TMPIDR</code> environment variable like so:</p> <pre><code>mkdir /state/partition1/user/$USER\nexport TMPDIR=/state/partition1/user/$USER\n</code></pre> <p>After you have installed your package you can clean up any lingering files by removing the temporary directory you have created:</p> <p><code>rm -rf /state/partition1/user/$USER</code></p>"},{"location":"faqs/#how-can-i-set-up-vscode-to-edit-files-remotely-on-supercloud","title":"How can I set up VSCode to edit files remotely on Supercloud?","text":"<p>You can use VSCode to remotely connect to Supercloud via the Remote-SSH extension. The default settings in the VSCode Remote - SSH extension will fail to connect. This is due to it trying to lock files in your home directory, which is disabled for performance reasons.</p> <p>The solution is to have it use the local filesystem. To get it to work, go to your VS Code settings, click \"Extensions\" and then \"Remote - SSH\". Once you're in the settings for Remote - SSH, check the box next to \"Remote.SSH: Lockfiles in Tmp\". What this will do is put any lockfiles in /tmp, rather than your home directory.</p> <p>A side note: we have seen VS Code clutter up /tmp in the past, which we keep fairly small. Disconnecting occasionally should clean these up, however we do not know for sure. If you can check it once in a while and clean up any files that are yours in /tmp, that would be really helpful.</p>"},{"location":"faqs/#how-can-i-use-tensorboard-on-supercloud","title":"How can I use Tensorboard on Supercloud?","text":"<p>Take a look at this page on how to run Tensorboard in an interactive job.</p>"},{"location":"faqs/#i-got-an-out-of-memory-error-how-can-i-figure-out-how-much-memory-my-job-needs-and-request-more","title":"I got an Out of Memory error. How can I figure out how much memory my job needs and request more?","text":"<p>This is described on the Submitting Jobs page. If you submit your jobs with <code>sbatch</code>, check out this section, and if you use <code>LLsub</code> take a look at this section. As described in those links, you can check how much memory your job used using the <code>sacct</code> command, then request enough additional cores for the memory you need. Keep in mind that if your job was killed due to high memory use, your job may not have gotten to the point of highest memory use. To get an accurate measurement you can run your job on an exclusive node long enough to reach the part of the job that would consume the most memory, then stop the job and check the memory use with <code>sacct</code>.</p>"},{"location":"faqs/#my-pythonjulia-job-is-running-but-i-dont-see-any-output-in-the-log-files-what-is-going-on","title":"My Python/Julia job is running, but I don't see any output in the log files. What is going on?","text":"<p>Julia and Python will buffer output in batch jobs. This means they will hold on to the output and print it out all at once, sometimes this isn't until the end of a loop or the end of the program. You can force both to print the output when it is produced. In Python you can do this by using the <code>-u</code> flag when you call Python in your submission script (ex: <code>python -u myscript.py</code>). In Julia you can do this by adding <code>flush(stdout)</code> after the print statements in your Julia script that you'd like to print immediately (ex: <code>println(\"Hello World!\"); flush(stdout)</code>).</p>"},{"location":"faqs/#what-does-the-underutilizingoversubscribing-the-node-warning-message-mean","title":"What does the Underutilizing/Oversubscribing the node warning message mean?","text":"<p>When you launch a job in triples mode, the second and third numbers you provide are the number of processes per node (NPPN) and the number of threads per process (NT). You can multiply these two numbers to get the total number of threads you will have running on the node. You will usually get the best performance if you have the same total number of threads as cores on the node. See the Systems and Software page for a list of how many cores are on each node type.</p> <p>Oversubscribing: When you have more threads than the number of cores on the node. Oversubscribing can overwhelm the node, which can slow your job down or even cause it to fail. It can also be harmful for the node. Reduce the number of threads per process or number of processes per node so that the total number of threads is less than or equal to the number of cores.</p> <p>Underutilizing: When you have fewer threads than the number of cores on the node you may be undersubscribing. This means you may not be taking full advantage of the node. Many underlying packages and libraries can take advantage of multithreading. You might try increasing the number of threads per process, while avoiding oversubscribing, to see if you get improved performance.</p> <p>For more information on how to pick the best triple for your job, take a look at the tuning process and recommendations.</p>"},{"location":"faqs/#how-can-i-get-more-help","title":"How can I get more help?","text":"<p>If you have a question that is not answered here, send email to supercloud@mit.edu for more help.</p>"},{"location":"getting-help/","title":"Getting Help","text":""},{"location":"getting-help/#additional-documentation","title":"Additional Documentation","text":"<p>If you haven't found your answer elsewhere in this wiki, you may find it here. We provide a list of Frequently Asked Questions and external Resources.</p>"},{"location":"getting-help/#email","title":"Email","text":"<p>If none of these resources are answering your question, please contact us at supercloud@mit.edu. In this email, please provide, where applicable:</p> <ul> <li>Description of your issue or request</li> <li>Job ID(s)</li> <li>What you tried</li> <li>The full error message you are receiving</li> <li>Any supporting files (code, submission scripts, screenshots, etc)</li> </ul>"},{"location":"getting-help/#office-hours","title":"Office Hours","text":"<p>We also host weekly office hours. A reminder email is sent out weekly with the exact location and time. If you would like to attend office hours and don't think you are getting these reminder emails, please email\u00a0supercloud@mit.edu.</p> <p>SuperCloud Office hours are also listed on the ORCD webpage: https://orcd.mit.edu/orcd-public-calendar/. The office hours are listed under:</p> <ul> <li>HPC Help Office Hours</li> <li>HPC Help Virtual Office Hours</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This page contains the most common steps for setting up and getting started with your SuperCloud account. We provide this page as a convenient reference to get started. To learn how to use your SuperCloud account, complete the Practical HPC course, which contains the material below and more. The course will walk you through these steps in more detail and often with videos to see how it is done. The course is self paced, can be accessed anytime, and is kept up to date. More reference material is available throughout site (some of this material links to those pages).</p> <p>When your account is first created you will have a small startup allocation. Once you complete the Practical HPC course you can request your account be updated to the standard allocation by sending email to supercloud@mit.edu. Resource allocations are listed on the Systems and Software page.</p>"},{"location":"getting-started/#logging-in-via-ssh","title":"Logging in Via ssh","text":"<p>The first thing you should do when you get a new account is verify that you can log in. The primary way to access the MIT SuperCloud system is through ssh. Instructions for different operating systems are below. Keep in mind that you will only be able to access the system from ssh from the machine where you generated your ssh key. You will not be able to log in until we have sent you an email stating that we have created your account, which will contain your username.</p> <p>First, add your ssh key to the web portal. Go to\u00a0https://txe1-portal.mit.edu/. If you affiliated with MIT or another institution/university select the middle option \"MIT Touchstone/InCommon Federation\" to log in. Select your institution from the dropdown (be aware they are spelled out, MIT is listed as Massachusetts Institute of Technology, for example), click \"Remember my choice\" box and then the \"Select\" button. Then log in with your institution credentials. Once you are logged in, click on the \"sshkeys\" link and paste your ssh key in the box at the bottom of the page and click \"Update Keys\".</p> <p>For instructions on how to generate ssh keys, retrieve your public key, and additional troubleshooting tips, watch the videos in the \"Account Setup and SSH Keys\" section in the \"Getting Started\" module of the Practical HPC course, or see this page.</p> <p>First open a command line terminal window where you generated your ssh keys. Enter the following command, where <code>USERNAME</code> is your username on the MIT SuperCloud system:</p> <p><code>ssh USERNAME@txe1-login.mit.edu</code></p> <p>If you generated your keys using PuTTY, open a PuTTY window. In the box labeled \"Host Name\" enter<code>USERNAME@txe1-login.mit.edu</code>, where<code>USERNAME</code>is your username on the MIT SuperCloud system. Keep the ssh box checked (this should be default) and Port should be set to 22. Click \"Open\" to start the session. You may also need to indicate your private key on the Connection -&gt; SSH -&gt; Auth page.</p>"},{"location":"getting-started/#shared-hpc-clusters","title":"Shared HPC Clusters","text":"<p>The MIT SuperCloud is an HPC-style shared cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended.</p> <p>Being a cluster, there are several machines connected together with a network. We refer to these as nodes. Most nodes in the cluster are referred to as compute nodes, this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node. The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes.</p> <p>Each job is started using a piece of software called the scheduler, which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.</p>"},{"location":"getting-started/#software-and-packages","title":"Software and Packages","text":"<p>The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review our page on Software and Package Management, paying particular attention to the section on modules and installing packages for the language that you use. If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.</p>"},{"location":"getting-started/#linux-command-line","title":"Linux Command Line","text":"<p>The MIT SuperCloud runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher.</p> <p>Most Linux commands deal with directories and files. A directory, synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path. In Linux, directories on a path are separated by forward slashes <code>/</code>. It is also important to note that everything in Linux is case sensitive, so a file <code>myScript.sh</code> is not the same as the file <code>myscript.sh</code>. When you first log in you are in you home directory. Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you. The path to your home directory on SuperCloud is <code>/home/gridsan/[USERNAME]</code>, where <code>[USERNAME]</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here.</p> <p>Finally, below is a list of Linux Commands. Try them out for yourself at the command line.</p> <ul> <li>Creating, navigating and viewing directories:<ul> <li><code>pwd</code>: tells you the full path of the directory you are     currently in</li> <li><code>mkdir dirname</code>: creates a directory with the name \"dirname\"</li> <li><code>cd dirname</code>: change directory to directory \"dirname\"<ul> <li><code>cd ../</code>: takes you up one level</li> </ul> </li> <li><code>ls</code>: lists the files in the directory<ul> <li><code>ls -a</code>: lists all files including hidden files</li> <li><code>ls -l</code>: lists files in \"long format\" including ownership     and date of last update</li> <li><code>ls -t</code>: lists files by date stamp, most recently updated     file first</li> <li><code>ls -tr</code>: lists files by dates stamp in reverse order, most     recently updated file is listed last (this is useful if you     have a lot of files, you want to know which file you changed     last and the list of files results in a scrolling window)</li> <li><code>ls dirname</code>: lists the files in the directory \"dirname\"</li> </ul> </li> </ul> </li> <li>Viewing files<ul> <li><code>more filename</code>: shows the first part of a file, hitting the     space bar allows you to scroll through the rest of the file, q     will cause you to exit out of the file.</li> <li><code>less filename</code>: allows you to scroll through the file, forward     and backward, using the arrow keys.</li> <li><code>tail filename</code>: shows the last 10 lines of a file (useful when     you are monitoring a logfile or output file to see that the     values are correct)<ul> <li>t<code>ail &lt;number&gt; filename</code>: show you the last &lt;number&gt;     lines of a file.</li> <li><code>tail -f filename</code>: shows you new lines as they are written     to the end of the file. Press CMD+C or Control+C to exit.     This is helpful to monitor the log file of a batch job.</li> </ul> </li> </ul> </li> <li>Copying, moving, renaming, and deleting files<ul> <li><code>mv filename dirname</code>: moves filename to directory dirname.<ul> <li><code>mv filename1 filename2</code>: moves filename1 to filename2, in     essence renames the file. The date and time are not changed     by the mv command.</li> </ul> </li> <li><code>cp filename dirname</code>: copies to directory dirname.<ul> <li><code>cp filename1 filename2</code>: copies filename1 to filename2. The     date stamp on filename2 will be the date/time that the file     was moved</li> <li><code>cp -r dirname1 dirname2</code>: copies directory dirname1 and its     contents to dirname2.</li> </ul> </li> <li><code>rm filename</code>: removes (deletes) the file</li> </ul> </li> </ul>"},{"location":"getting-started/#transferring-files-to-mit-supercloud","title":"Transferring Files to MIT SuperCloud","text":"<p>One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands scp or rsync. Read the page on Transferring Files to learn how to use these commands and transfer what you need to your home directory.</p>"},{"location":"getting-started/#testing-your-code","title":"Testing your Code","text":"<p>At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command:</p> <p><code>LLsub -i</code></p> <p>After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, or uses a lot of memory, you should request a full node to be sure you don't impact other jobs on the system:</p> <p><code>LLsub -i full</code></p> <p>These commands by no means represent the full use of the system, and most likely won't be the primary way you run your code. In our tutorial we go over much more on how to submit jobs and will make sure you have the tools you need to get the most out of the MIT SuperCloud.</p>"},{"location":"getting-started/#supercloud-downtimes","title":"SuperCloud Downtimes","text":"<p>Note that SuperCloud has Monthly Downtimes which are scheduled for the Third Thursday of each month. During downtimes the system is not available. Downtimes usually last about a day and emails are sent when they are complete. We also send out a reminder email a few days before each downtime.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>The following is a list of commonly used terms and acronyms, and their definitions when used in the context of the MIT SuperCloud.</p> <p>First is a visual labeling the portions of the system with the terminology we tend to use for each piece.</p> <p></p> <p>Accelerators A piece of hardware used to speed up computation, usually for a   specific operation. GPUs are used as an accelerator for certain matrix   operations.</p> <p>Bandwidth A theoretical measure of how much data could be transferred from   source to destination in a given amount of time.</p> <p>Bash shell A specific shell and language used at the command line. This is the   shell used on SuperCloud.</p> <p>Bash script/Shell script A script using bash command syntax.</p> <p>Batch job A job for running a pre-written script or executable. Resources are   requested through the scheduler, the schedule allocates the resources   when they are available, runs the script, and then exits.</p> <p>Cluster Many nodes connected via a fast network interconnect.</p> <p>Command Line A text-based user interface that allows a user to type commands that   the computer then executes.</p> <p>Compute Nodes Nodes where the computation is done on the system (where you will run   your code). Compute nodes are managed by the scheduler.</p> <p>Core A core is the smallest computation unit that can run a program.</p> <p>CPU The Central Processing Unit (CPU) is the part of a computer which   executes software programs. CPU refers to an individual silicon chip,   such as Intel's Xeon-E5 or AMD's Opteron. \u00a0A CPU contains one or   more cores. \u00a0Also known as a processor or socket.</p> <p>Data Server Also called an Object Storage Server. A component of a parallel file   system which stores all of the data of the files on the file system.</p> <p>(Job) Dependency Defer the start of a job until the specified dependencies have been   satisfied completed. This is usually the completion of another job.</p> <p>Distributed Memory (see \"Memory Models\") In\u00a0a distributed memory system, each CPU has its own private memory.   Processes can only operate on local data. If remote data is required,   the process must communicate with the remote process over an   interconnect.</p> <p>Downtime A regular maintenance day during which the system is unavailable.</p> <p>Environment Variable Environment variables allow you to customize the environment in which   programs run. They become part of the environment in which the   programs run and can be queried by running programs. For example, you   can set an environment variable to contain the path to your data   files. Your running process can query this environment variable to get   the location of the files.</p> <p>File Permissions Properties of a file that determine who can read, write, or execute   (run) a file.</p> <p>Filesystem The system that controls how and where data is stored on storage disk.   See Shared/Central Filesystem and Local Filesystem.</p> <p>GPU A Graphics Processing Unit (GPU) is a specialized device originally   used to generate computer output. \u00a0Each compute node can host one or   more GPUs. \u00a0Modern GPUs have many simple compute cores and have been   used for parallel processing.</p> <p>Group Shared Directory A directory, created upon user request, where members of the group   shared directory can share files with other members of the group.   \u00a0Since a user's home directory is accessible only to the user, a group   shared directory is the only mechanism for users to share files.</p> <p>GUI Graphical User Interface- these are interfaces that allow the user to   interact with a program with a mouse through visual icons, as opposed   to a command line interface.</p> <p>Home Directory Where the user keeps their files. Each user has their own home   directory.</p> <p>HPC High Performance Computing (HPC) refers to the practice of aggregating   computing power to achieve higher performance that would not possible   by using a typical computer.\u00a0 The community often used\u00a0concurrent   computing\u00a0to mean programs running at the same time v in serial one   after another.</p> <p>Hub A networking component that takes an incoming message and broadcasts   it across all of the other ports of the hub.</p> <p>Independent (Tasks/Processes) Tasks/processes that can operate by themselves without needing data   from another.</p> <p>Interactive Job An interactive job allows you to actually log in to a compute node.   This is useful for when you need to compile software, test jobs and   scripts, or run software that requires keyboard inputs and user   interaction, such as a graphical interface\u00a0.</p> <p>Interconnects The connections between components of the computer (this interconnect   is called the System Network), and the computer to the Internet   network (this interconnect is called the Network Connection).</p> <p>I/O (Input/Output) Refers operations that involve a transfer of data, particularly   reading from and writing to the filesystem.</p> <p>Job A job is a separately executable unit of work whose resources are   allocated and shared. \u00a0Users create job submission scripts to ask the   scheduler for resources (cores, a specific processor type, etc). \u00a0The   scheduler places the requests in a queue and allocates the requested   resources.</p> <p>Job Array According to the Slurm documentation: \u00a0\"Job arrays offer a mechanism   for submitting and managing collections of similar jobs quickly and   easily\". \u00a0 Job arrays are useful for applying the same processing   routine to a collection of multiple inputs, data, or files. \u00a0Job   arrays offer a very simple way to submit a large number of independent   or\u00a0High Throughput\u00a0processing jobs.</p> <p>Job Slot A computational resource unit that is roughly equivalent to a   processor core. One or more job slots can be used to execute a   process.</p> <p>Jupyter Notebook An interactive browser-based programming environment.</p> <p>Latency The delay before a transfer of data begins following an instruction   for its transfer.</p> <p>Lgpn Average observed Load per GPU on the node.</p> <p>LLGrid Beta LLGrid Beta is a collection of software packages that are released as   a beta test on the SuperCloud. \u00a0The beta software packages are ones   that SuperCloud users have requested but are not included in the   SuperCloud system image.</p> <p>LLMapReduce A language-agnostic command for running loosely coupled or MapReduce   applications.</p> <p>LLx A course platform containing online courses that use the SuperCloud   system for exercises.</p> <p>Lnode Average observed Load on the node.</p> <p>Local Filesystem Each node in the cluster has its own local filesystem that is only   accessible from that node. The system image and software stack is on   the local filesystem. It also contains space that can be used during   jobs for fast file access.</p> <p>Login Node The login node controls user access to a parallel computer. \u00a0Users   usually connect to login nodes via SSH to compile and debug their   code, review their results, do some simple tests, and submit their   interactive and batch jobs to the scheduler.</p> <p>Loosely Coupled Applications that involve an independent (map) step where the same   operation can be performed by many processes on different inputs,   followed by a serial step that uses the output of the first step as   its input. Also called MapReduce.</p> <p>Lppn Average observed Load per process on the node.</p> <p>Man page Short for \"manual page\". Documentation for a command or program.</p> <p>MapReduce See \"Loosely Coupled\".</p> <p>Mbpc Memory Bytes Per Core.</p> <p>Mbpn Memory Bytes Per Node.</p> <p>Mbpp Memory Bytes Per Process.</p> <p>Memory See \"Volatile Memory\".</p> <p>Memory Models (see \"Distributed Memory\" and \"Shared Memory\")</p> <p></p> <p>Metadata Server A component of a parallel file system which maintains the state of all   files and folders within the file system, and the list of data servers   where it can find the data for the files.</p> <p>MIMO Mode Multiple input, multiple output is an application mode for use with   LLMapReduce. In MIMO mode your application iterates through multiple   inputs. LLMapReduce calls and loads your application once in order to   process multiple assigned inputs.</p> <p>Modules Here we are referring to \"environment modules\", but we often refer   to them just as \"modules\". An open source software management tool   used in most HPC facilities. \u00a0Using modules enable users to   selectively pick the software that they want and add them to their   environment. \u00a0Using the module command, you can manipulate your   environment to gain access to new software or different versions of a   package.</p> <p>MPI The Message Passing Interface (MPI) is a library for passing messages   between processes and between compute nodes within a parallel job   running on a cluster. There are a variety of open source and   commercial versions of MPI that have\u00a0been developed over the past   several decades including mpich,\u00a0 OpenMPI, and Intel MPI.</p> <p>Multi-Threaded Describes an application that uses multiple threads. See \"Shared   Memory\".\u00a0</p> <p>Ncpn Number of hardware Cores Per Node.</p> <p>Ngpn Number of hardware GPUs Per Node.</p> <p>Nnode Number of Nodes.</p> <p>Node A stand-alone computer where jobs are run. \u00a0Each node is connected to   other compute nodes via a fast network interconnect. \u00a0While accessible   via interactive jobs, compute nodes are not meant to be accessed   directly by users.</p> <p>Non-Volatile Memory Storage device where the information stored on it remains intact even   when the computer is shut down or restarted, e.g., disk drives.</p> <p>Np Number of Processes = Nnode * Nppn.</p> <p>Nppn Number of Processes Per Node.</p> <p>Ntpn Number of Threads Per Node = Nppn * Ntpp.</p> <p>Ntpp Number of Threads Per Process.</p> <p>Operating System (OS) The software that manages how each of the applications running on the   computer interact with the hardware of the computer to accomplish   tasks.</p> <p>Path A list of directories separated by \"/\" characters that shows the   location of a file or directory in the directory structure.</p> <p>Absolute Path The full path from the root of the filesystem, /. For example, the   absolute path to the home directory for studentx would be:   /home/gridsan/studentx.</p> <p>Relative\u00a0Path The path to a file or directory from the current location.</p> <p>Process An independent computation running on a computer. \u00a0Processes have   their own address space and may create threads that will share their   address space. \u00a0Processes must use interprocess communication to   communicate with other processes.</p> <p>Router A networking component that acts as a special switch that moves   messages across defined network boundaries.</p> <p>Rsync A command for transferring and syncing files between systems.</p> <p>Scheduler The scheduler receives job and task execution requests from users and   manages how and where they are executed across the many compute nodes   in the HPC system. Before starting a job, it ensures that the needed   resources are available for the job. The scheduler monitors running   jobs, can stop jobs, and can provide information about completed jobs   and the status of the system (e.g. what resources are currently   available).</p> <p>Shared/Central Filesystem The shared filesystem is the filesystem that is available to all nodes   in the cluster. Home and group directories are on the shared   filesystem.</p> <p>Shared Memory\u00a0(see \"Memory Models\") In\u00a0a shared memory system, there is shared memory that can be   simultaneously accessed by multiple CPUs in a multiprocessor CPU.   Communication or data passing among threads or processes in a shared   memory system is via memory.</p> <p>Shell Another term for the Linux command line interface.</p> <p>SISO Mode Single input, single output is an application mode for use   with\u00a0LLMapReduce. In SISO mode, your application runs on a single   input. LLMapReduce calls and loads your application once in order to   process one assigned input.</p> <p>Slurm Simple Linux Utility for Resource Management (SLURM) is a job   scheduler which coordinates the running of many programs on a shared   facility. \u00a0Slurm is used on the MIT SuperCloud system. \u00a0It replaced   the SGE scheduler.</p> <p>Socket A computational unit packaged as one, and usually made of a single   chip often called processor. \u00a0Modern sockets carry many cores.</p> <p>SPMD Single Program Multiple Data</p> <p>SSH Secure Shell (SSH) is a protocol to securely access remote computers.   \u00a0Based on the client-server model, users with an SSH client can access   a remote computer. \u00a0Some operating systems such as Linux and Mac OS   have a built-in SSH client and others can use one of many publicly   available clients. \u00a0For Windows, we recommend PuTTY or Cygwin for ssh.</p> <p>SSH Keys Credentials used as an authentication method for ssh. These come in   pairs: a public and private key. Public keys are placed on the system   you need to access, private keys are placed on your computer. When you   ssh in the ssh program checks to see whether the public key fits your   private key.</p> <p>Submission/batch script A script for submitting a batch job to the scheduler. It is a bash   script that tells the scheduler how to run your job, and may include   the resources you are requesting for you job.</p> <p>Switch A networking component that is more efficient than a hub. It takes an   incoming network message and sends it out only onto the switch port on   which its destination will be reached. Switches only transmit messages   within a defined network.</p> <p>Symlink Short for symbolic link. A file that acts as a shortcut by pointing to   another file or directory on the filesystem. If you are in a group you   may see a symbolic link to the shared group directory in your home   directory.</p> <p>Terminal (Window) A window containing a command line prompt.</p> <p>Third-party software According to Wikipedia: a third-party software component is a reusable   software component developed to be either freely distributed or sold   by an entity other than the original vendor of the development   platform.\u00a0</p> <p>Examples of third-party software on the SuperCloud system include MATLAB and TensorFlow.\u00a0</p> <p>Thread Threads are lightweight processes which exist within a single   operating system process. \u00a0Threads share the address space of the   process that created them and can communicate directly with other   threads in the same process.</p> <p>Throughput An actual measure of how much data is successfully transferred from   source to destination in a given amount of time.</p> <p>Throughput (Workflow) A throughput application is one that is fully independent. Often this   means it is running the same operation on a number of different inputs   or parameters, and the result of an operation on one input does not   depend on the result of another.</p> <p>Triples (Mode) A job submission mode that allows you to request resources in a   triple: Number of Nodes, Number of Processes per Node, and Number of   Threads per Process. Available for LLsub job arrays, LLMapReduce, and   pMatlab jobs.</p> <p>Ubpn Average observed Used bytes per node.</p> <p>Ubpp Average observed Used bytes per process.</p> <p>Unix, Linux Unix is a family of portable, multi-tasking, multi-user operating   systems. Linux is an open source, Unix-like operating system that is   derived from Unix. The SuperCloud system runs the Ubuntu version of   the Linux operating system.</p> <p>User space User space is a set of locations where normal user processes (i.e.   everything other than the kernel, the lowest part of the operating   system) run.</p> <p>Volatile Memory Storage device where applications and data are loaded so that the   processors can actively work with them, e.g. RAM and cache. The   information stored on it does not remain intact when the computer is   shut down or restarted.</p> <p>Web Portal A web page for SuperCloud where you can access your SuperCloud   account. On the Web Portal you can add ssh keys, access the files in   your home directory, and start Jupyter Notebooks.</p>"},{"location":"online-courses/","title":"Online Courses","text":""},{"location":"online-courses/#some-available-online-courses","title":"Some Available Online Courses","text":"<p>Practical HPC: An introductory course that:</p> <ul> <li>Includes an introduction to HPC, canonical HPC Workflows, and the     SuperCloud system.</li> <li>Walks you through setting up your account, installing software,     running your first test job, submitting your first batch job.</li> <li>Describes how to scale up efficiently and measure your performance.</li> </ul> <p>Mathematics of Big Data and Machine Learning: Available through OCW.</p>"},{"location":"online-courses/#accessing-the-llx-online-course-site","title":"Accessing the LLx Online Course Site","text":"<p>Navigate to LLx and follow the instructions below to create an account, or click \"Sign In\" if you have an account. Click the \"Courses\" tab to register for courses. Once you have logged in hover over your username in the top right corner and click on the \"Dashboard\" link to access the courses you are enrolled in.</p>"},{"location":"online-courses/#creating-an-online-course-account","title":"Creating an Online Course Account","text":"<p>Below, you will find instructions on registering for an online course account:</p> <ol> <li>To sign up for an LLx account, go to the the LLx     Platform</li> <li>In the upper right corner, click on \"Register\".</li> <li>Complete the Registration Form to create your LLx account. Note, the     following items are required:<ul> <li>A valid and accessible email address</li> <li>Your name</li> <li>A public username of your choosing, it cannot include spaces</li> <li>A password, you can change this later</li> </ul> </li> <li>Once you have completed the registration form, click on the button     to create your account.</li> <li>Once you click on the button to create your account, you should see     a message stating that an activation email has been sent to your     email address.</li> <li>Check your email. When you receive the activation email, click on     the link to activate your account. Note you will not be able to log     back into the course site if you have not activated your account.</li> <li>Once you have activated your account you can register for courses.</li> </ol> <p>NOTE: If you forget your login password you may request a new password by clicking on \"Need help logging in?\" to the right above the password box. A password will be sent to your specified email. Follow the instructions for changing your password.</p>"},{"location":"online-courses/#questions","title":"Questions?","text":"<p>If you have any questions about our online courses or are having trouble with the platform, please contact us at\u00a0llx-help@mit.edu. If you have any questions about the MIT SuperCloud System, contact us at supercloud@mit.edu.</p>"},{"location":"requesting-account/","title":"Requesting an Account","text":"<p>The MIT SuperCloud is intended to support research and collaboration between MIT Lincoln Laboratory and students, faculty and researchers at MIT and other academic institutions.\u00a0It is our practice to allow access from within the United States.</p>"},{"location":"requesting-account/#account-request-process","title":"Account Request Process","text":"<p>The account request, approval, and creation process is:</p> <ol> <li>Request: There are two steps to the request:<ol> <li>Fill out all fields of our Account Request     Form. In this     form we ask if you are using non-public data, see why     below. If you     are not part of an MGHPCC institution, list your MIT or Lincoln     Laboratory collaborator. Do not submit this form without     answering these question, it will cause significant delay in the     process. If you have any questions about the form, ask us by     sending email to supercloud@mit.edu.</li> <li>Ask your faculty advisor or PI to send us a short confirmation     email for your account verifying that you will be using your     Supercloud account for your work. This email should be sent to     supercloud@mit.edu.\u00a0We     will not email your advisor for you. We will not proceed with     the account creation process until we receive an email from your     advisor/PI.</li> </ol> </li> <li>Faculty Advisor/PI Confirmation: Once we receive an email from your     faculty advisor/PI we can continue the next step. This confirmation     must come from a faculty member or PI on the project that you are     using Supercloud for.</li> <li>Approval: This usually happens behind the scenes. You may receive an     email with additional questions before you are approved. While you     are waiting you can start learning to use your account by working     through the Practical     HPC course.</li> <li>Creation: When your account is created, you will receive an email     with your username and further instructions to set up your account.     When your account is first created you will have a small startup     allocation. Once you complete steps 5 and 6 you can request your     account be updated to the standard allocation.</li> <li>Set up your account:     Create an ssh key and     add it to     your account, then     make sure you can log in through ssh. The Practical     HPC\u00a0course also     has a section with videos that walks you through this process.</li> <li>Learn to use your account: Work through the Practical     HPC course. This     course:<ol> <li>Includes an Introduction to HPC, canonical HPC Workflows, and     the SuperCloud system.</li> <li>Walks you through setting up your account, installing software,     running your first test job, submitting your first batch job.</li> <li>Describes how to scale up efficiently and measure your     performance.</li> </ol> </li> </ol> <p>The account creation process is manual and can take approximately\u00a0two weeks. You can make this process smoother by making sure you have fully filled out your request form before submitting it and making sure your advisor has sent us an email confirmation. While you are waiting for your account you can get a head start learning how to use the account by reviewing the Practical HPC\u00a0course.</p>"},{"location":"requesting-account/#why-do-we-ask-if-you-are-using-data-that-is-not-publicly-available","title":"Why do we ask if you are using data that is not publicly available?","text":"<p>Not all data is appropriate for Supercloud. If your data is not publicly available, we ask for any agreements or requirements you have for your data to make sure Supercloud is the right place to be putting the data. Please be as detailed as you can. To get a general idea of the sorts of data that may or may not be appropriate for Supercloud, take a look at MIT IS&amp;T's guidance for storing data in Dropbox, OneDrive, and Google Drive here.</p>"},{"location":"requesting-account/#generating-ssh-keys","title":"Generating ssh Keys","text":"<p>If you have any issues or questions regarding the generation of ssh keys, please contact the team at supercloud@mit.edu.\u00a0To access the system you will need ssh keys. For additional security you can create a passphrase when you generate your key, which you must enter every time you log in. Since you set this yourself on your own computer, we cannot help you reset it if you forget it. If you can't remember your passphrase you'll have to generate a new key and re-add it using the Web Portal.</p> <p>If you cannot generate ssh keys on your system, let us know and we can help you.</p> <p>If you have no existing ssh keys, from the command line in a terminal window, follow the steps below. On Mac and Linux, open your standard terminal window. On Windows 10 and higher, you can use the Windows command prompt. If the command prompt does not recognize the ssh-keygen command, you can install OpenSSH by following the instructions on this page. If your Windows operating system is older than Windows 10, see the note below.</p> <p>If you already have ssh keys then you can use those. You will need your public key, <code>id_rsa.pub.</code></p> <p><code>[user1234@yourMachine]$ ssh-keygen -t rsa</code></p> <p>You will see the following:</p> <p><code>Generating public/private rsa key pair.</code></p> <p>When answering the 3 prompts (first 3 lines) hit return to create passwordless keys and save them in the default location. Alternatively, for extra security you can create a passphrase for your key that you'll have to enter every time you log in. To do this, instead of pressing \"enter\" or \"return\", enter the passphrase you've chosen when prompted.</p> <pre><code>Enter file in which to save the key (/home/user1234/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/user1234/.ssh/id_rsa.\nYour public key has been saved in /home/user1234/.ssh/id_rsa.pub.\nThe key fingerprint is:\n88:90:6a:dc:f1:bd:ed:fb:b1:aa:46:14:34:5e:b9:70 user1234@yourMachine\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|\u00a0 \u00a0 \u00a0 .o ..\u00a0 \u00a0 \u00a0 |\n| \u00a0 .\u00a0 .ooE \u00a0 \u00a0 \u00a0 |\n|\u00a0 o. \u00a0 .+ .\u00a0 \u00a0 \u00a0 |\n|....o..o . \u00a0 \u00a0 \u00a0 |\n|.o ...o.S\u00a0 \u00a0 \u00a0 \u00a0 |\n|.\u00a0 \u00a0 \u00a0 .o\u00a0 \u00a0 \u00a0 \u00a0 |\n|\u00a0 \u00a0 \u00a0 .. . . \u00a0 \u00a0 |\n| \u00a0 \u00a0 \u00a0 .. \u00a0 o\u00a0 \u00a0 |\n|\u00a0 \u00a0 \u00a0 ...++o \u00a0 \u00a0 |\n+\u2014\u2014\u2014\u2014\u2014------------+`\n</code></pre> <p>To view your public ssh key, go to your .ssh directory.</p> <p><code>[user1234@yourMachine]$ cd .ssh</code></p> <p>In <code>~/.ssh</code> you would see two files <code>id_rsa</code> and <code>id_rsa.pub</code>. The <code>id_rsa.pub</code> file contains your public key.</p> <p><code>[user1234@yourMachine]$ ls   id_rsa\u00a0 id_rsa.pub</code></p> <p>This is the <code>id_rsa.pub</code> file content after generating a public SSH key\u00a0that we would require. To view it, type\u00a0<code>cat id_rsa.pub</code>\u00a0at the command line.</p> <pre><code>[user1234@yourMachine]$ cat id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA1NAD8v4nFzQ6G7KIEzkDLOnlH7t/4zmw0vVXlJjjFW4kLBgLJa0tkk61jHCxO2CurDr4zdEs2NeHG9agZJgMKMJZdIVaxtPcEBVVaNutvn/ZDRe3VsrRjToKEoR0xlAUdoef++AwiwI6K6vBOGIq6whLIlY5L9tZJfaLF3xMwmQRRhf4C+al/yZ5hX7BfGba2fqZmugTPpeSbLnFMVPKK/wy6XZasBSAKgLBA141EMXIKuGrpXpxLMECPBN5GDd/xmjmD0pC2o2z5OdfdYJj/FRWL2sC8hWTZSPa4p/n7Qc9ErFW5wM7FkynwguN4t/A+QOCa+p8C/nrOcTQKugrtw user1234@yourMachine\n</code></pre> <p>Copy the entire output, including the ssh-rsa at the beginning.</p>"},{"location":"requesting-account/#adding-your-ssh-keys-to-your-account","title":"Adding your SSH Keys to your Account","text":"<p>Once you have created your ssh keys and copied your public key, you can add your key to your account using the Web Portal:</p> <ol> <li>Go to https://txe1-portal.mit.edu.</li> <li>Log in. If you are an MIT affiliate or an affiliate at another     university or institution you can log in with your MIT or     institutional credentials. Click on MIT Touchstone/InCommon.<ol> <li>Select your institution (note these are spelled out, MIT is     listed as Massachusetts Institute of Technology, for example).</li> <li>Click the checkbox next to \"Remember my Choice\" and click the     \"Select\" button.</li> <li>Log in with your institutional credentials.</li> </ol> </li> <li>Click on the \"sshkeys\" link.</li> <li>Paste your public ssh key in the box at the bottom of the page,     click \"Update Keys\".</li> <li>Verify you can log in by running <code>ssh USERNAME@txe1-login.mit.edu</code>     in the terminal where you created your ssh keys, where <code>USERNAME</code> is     the username we sent you in your new account email.</li> </ol>"},{"location":"requesting-account/#pre-windows-10","title":"Pre-Windows 10","text":"<p>NOTE:\u00a0For other Windows users there are a number of ssh clients you can use. Some ssh clients like Moba Xterm and Cygwin give you a Linux-like environment, and so once you start the program (which should look include a command line window), you can follow the instructions for creating an ssh key in above once you install the client.</p> <p>To install Moba Xterm, follow the instructions here through the section \"Create Local Shell\". Anytime you are instructed to open a terminal window, you can follow the instructions to create a local shell. Once you have installed, follow the instructions above for creating an ssh key.</p> <p>Instructions for installing PuTTY are\u00a0here.\u00a0(Please note, the link will open in a new window.) Once PuTTY is installed please follow the instructions at this\u00a0link\u00a0to manually generate your ssh-keys, only follow the instructions in the \"Generating an SSH Key\" section.</p>"},{"location":"requesting-account/#current-approver-list","title":"Current Approver List","text":"<ul> <li>Boston University: Wayne Gilmore</li> <li>Harvard: Scott Yockel</li> <li>MIT: Jeremy Kepner, Vijay Gadepally, Chris Hill, Lauren Milechin</li> <li>Lincoln Laboratory: Jeremy Kepner, Vijay Gadepally, Albert Reuther</li> <li>Northeastern: David Kaeli</li> <li>UMass Amherst: John Griffin</li> <li>UMass Dartmouth:\u00a0Geoffrey Cowles</li> <li>UMass Lowell: Anne Maglia</li> <li>UMass Medical: Paul Langlois</li> <li>University of Rhode Island:\u00a0Gaurav Khanna</li> </ul>"},{"location":"systems-and-software/","title":"Systems and Software","text":"<p>This page lists information about the system and available software, languages, compilers, modules, etc. This is only a partial list, so if there is anything you are interested in that isn't listed here, please contact us.</p>"},{"location":"systems-and-software/#mghpcc-tx-e1-specifications","title":"MGHPCC TX-E1 Specifications","text":"Summary Number of Nodes 704 Total CPU Cores 32000 Total GPUs 448 Distributed Storage 873 TB"},{"location":"systems-and-software/#individual-nodes","title":"Individual Nodes","text":"Processor Nodes CPU Cores Node RAM RAM/core GPU Type GPU RAM GPUs/node Local Disk Intel Xeon   Platinum 8260 480 48 192 GB 4 GB NA NA NA 4.4 TB Intel Xeon  Gold 6248 224 40 384 GB 9 GB NVidiaVolta V100 32 GB 2 3.8 TB"},{"location":"systems-and-software/#resource-allocations","title":"Resource Allocations","text":"Processor Partition Starting Standard Intel Xeon  Platinum 8260 xeon-p8 2 Nodes(96 Cores) 16 Nodes(768 Cores) Intel Xeon  Gold 6248 xeon-g6-volta 1 Node(40 Cores 2 GPUs) 4 Nodes(160 cores, 8 GPUs)"},{"location":"systems-and-software/#available-languages","title":"Available Languages","text":"<ul> <li>Julia</li> <li>Python (Anaconda available through modules)</li> <li>Matlab(R)/Octave</li> <li>R</li> <li>C/C++</li> <li>Fortran</li> <li>Java</li> <li>Perl 5</li> <li>Ruby</li> </ul>"},{"location":"systems-and-software/#modules","title":"Modules","text":"<p>To see the most up-to-date list of currently available modules, run the command <code>module avail</code>. For more information about modules, see the module section on the Software and Package Management page.</p>"},{"location":"systems-and-software/#software","title":"Software","text":""},{"location":"systems-and-software/#machine-learning-tools","title":"Machine Learning Tools","text":"<ul> <li>Tensorflow</li> <li>Pytorch</li> </ul>"},{"location":"systems-and-software/#big-data-software-stack","title":"Big Data Software Stack","text":"<ul> <li>Hadoop</li> <li>Zookeeper</li> <li>Accumulo</li> </ul>"},{"location":"systems-and-software/#middleware-software-stack","title":"Middleware Software Stack","text":"<ul> <li>ARPACK</li> <li>ATLAS</li> <li>Boost</li> <li>BLAS</li> <li>FFTW</li> <li>LAPAC</li> <li>OpenMPI</li> <li>OpenBLAS</li> </ul>"},{"location":"systems-and-software/#lincoln-laboratory-developed-software","title":"Lincoln Laboratory Developed Software","text":"<ul> <li>pMatlab</li> <li>D4M</li> <li>Graphulo</li> <li>LLMapReduce</li> </ul>"},{"location":"systems-and-software/#compilers","title":"Compilers","text":"<ul> <li>gcc</li> <li>g++</li> <li>gfortran</li> <li>icc</li> </ul>"},{"location":"tensorboard/","title":"Tensorboard","text":"<p>You can run Tensorboard as a job, this is the preferred method of   doing this. \u00a0 First start an interactive session with a reserved port:</p> <pre><code>[studentx@login-1 ~]$ LLsub -i --resv-ports 1     \u00a0     salloc --immediate=60 -p normal --constraint=xeon-e5 --cpus-per-task=4 --qos=high\u00a0 srun --resv-ports=1 --pty bash -i     salloc: Granted job allocation 355286     salloc: Waiting for resource configuration     salloc: Nodes node-052 are ready for job     \u00a0`\n</code></pre> <p>Then create your logging directory in TMPDIR:</p> <p><code>[studentx@node-052 ~]$ mkdir -p ${TMPDIR}/tensorboard</code></p> <p>Set up your forwarding name and file:</p> <pre><code>[studentx@node-052 ~]$ PORTAL_FWNAME=\"$(id -un tr '[A-Z]' '[a-z]')-tensorboard\"     [studentx@node-052 ~]$ PORTAL_FWFILE=\"/home/gridsan/portal-url-fw/${PORTAL_FWNAME}\"     [studentx@node-052 ~]$ echo $PORTAL_FWFILE     /home/gridsan/portal-url-fw/studentx-tensorboard    [studentx@node-052 ~]$ echo \"Portal URL is: https://${PORTAL_FWNAME}.fn.txe1-portal.mit.edu/\"     Portal URL is: https://studentx-tensorboard.fn.txe1-portal.mit.edu/`\n</code></pre> <p>Put the forward URL in the forwarding file (when you run \"cat   \\$PORTAL_FWFILE\" you should only see one line- if you see two or more,   delete all but the last line): \u00a0 <pre><code>[studentx@node-052 ~]$ echo \"http://$(hostname -s):${SLURM_STEP_RESV_PORTS}/\" &gt;&gt; $PORTAL_FWFILE     [studentx@node-052 ~]$ cat $PORTAL_FWFILE     http://node-052:12637/\n</code></pre></p> <p>Set the permissions on the forward file properly:</p> <p><code>[studentx@node-052 ~]$ chmod u+x ${PORTAL_FWFILE}</code></p> <p>Load an anaconda module and start tensorboard</p> <pre><code>[studentx@node-052 ~]$ module load anaconda/2023a\n[studentx@node-052 ~]$ tensorboard --logdir ${TMPDIR}/tensorboard --host \"$(hostname -s)\" --port ${SLURM_STEP_RESV_PORTS}     \u00a0</code></pre> <p>In the browser, go to the URL listed above (for example, mine is https://studentx-tensorboard.fn.txe1-portal.mit.edu/)</p>"},{"location":"using-the-system/databases/","title":"Databases","text":"<p>The MIT Supercloud allows users to launch their own databases through the database portal. The portal is located at:</p> <p>https://txe1-portal.mit.edu/db/dbstatus.php</p> <p>This page requires you to authenticate into the portal. From here you will see all the databases you have access to. There are Accumulo and Postgress databases available.</p> <p>To start up a database instance, press \"Start\". You can stop it by clicking on the \"Stop\" button, and can checkpoint a stopped database as well. Clicking on \"View Info\" for Accumulo databases will take you to the Accumulo Monitoring page, where you can view ingest/query plots and current tables.</p> <p>If you need an instance created and cannot use one of those already available, contact us. Let us know the type of database you need, what you are using it for, what it should be called, and who should have access to the database. Let us know if there are any special configurations you need.</p> <p>There are many ways to insert and query data. While this cannot be done from the portal page, you can query and insert data from any of the nodes on the MIT Supercloud system. We recommend using D4M, which has been installed and pre-configured to work on our system with very little effort. For more information about databases and how to use them with D4M, take a look at the Advanced Database Technologies course on our online course platform. This course contains a good introduction on how to set up a data pipeline, including parsing, ingesting, and querying data for Accumulo.</p>"},{"location":"using-the-system/jupyter-notebooks/","title":"Jupyter Notebooks","text":"<p>Jupyter notebooks are an interactive IDE environment. The environment allows you to start up notebooks in a variety of languages (Python, Matlab, Octave, Julia), terminal windows, and a simple text editor. You can also download and upload files to your home directory through this interface. This page will describe how to start up and use your own Jupyter instance and how to set environment variables for Jupyter.</p>"},{"location":"using-the-system/jupyter-notebooks/#starting-up-jupyter","title":"Starting Up Jupyter","text":"<p>When you start up Jupyter, the Jupyter instance gets submitted through the scheduler as a job. To do this, navigate to the following page:</p> <p>https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php</p> <p>To launch a notebook with default options, you can simply click on the \"Launch Notebook\" button.</p> <p>To see what the default options are or to change these options, click \"Show Advanced Launch Options\". A form will appear where you can select alternative options.</p> <ul> <li>Partitions: The partition the job is submitted to. You do not have     to change this option, it will adjust based on the resources you     select.</li> <li>CPU Type: The type of CPU you would like to launch to. You do not have     to change this option, it will adjust based on the resources you     select.</li> <li>GPU Resource Flag: If you would like to allocate GPUs to your     Jupyter instance you can select the GPU type here. Note your code or     packages must take advantage of GPUs, otherwise they will sit idle     if you request them.</li> <li>GPU Resource Count: If you select a GPU the number of GPUs to     allocate to your Jupyter instance.</li> <li>ncpu: The number of CPUs or cores allocated to your notebook.</li> <li>exclusive: Check this box if you would like exclusive use of a     compute node. You will be allocated all the CPUs on the node.</li> <li>Anaconda/Python Version: Select the Anaconda and/or Python version     that you want. Note some languages are not available on all Anaconda     versions. Refer to our How To pages to see which versions you should     select for the language you want to use.</li> <li>Application: Choose between Jupyter Notebook and Jupyter Lab. They     both have the same features but with different layouts. Jupyter     Notebook is the stable production application, Jupyter Lab is a beta     application. The application you choose is personal preference. In     this course we will be showing examples using Jupyter Notebook.</li> </ul> <p>Once you click on the \"Launch\" button, the scheduler launches your job, if the resources are available. When your Jupyter instance is ready, a link will appear on the page. This can take a minute or so.</p> <p>One very important thing to note is that once your Jupyter instance has launched, it will continue to run until you stop the job. This is particularly important if you are using a lot of resources when launching your job. Stopping the job can be done by clicking on the \"Shutdown\" button in the top right corner of the Jupyter interface page, by navigating back to the initial launch page and clicking the \"Shutdown\" button, or by using one of the scheduler commands (<code>LLkill</code>).</p>"},{"location":"using-the-system/jupyter-notebooks/#the-jupyter-environment","title":"The Jupyter Environment","text":"<p>When you first enter the Jupyter environment, you will see the files and directories in your home directory. You can navigate through these by clicking. By clicking the \"New\" button in the top right, you can:</p> <ul> <li>Start a new notebook (Julia, Python, Matlab, Octave, R)</li> <li>Open a terminal</li> <li>Create a new text file with a simple text editor</li> </ul> <p>In addition to these, you can upload and download data, and edit text files from Jupyter. Click on the checkbox next to the file you want to edit, and go to the top of the page and click \"Edit\". This will bring you to a simple text editor. For a bit of an intro to the Jupyter interface, see this page on Notebook Basics.</p>"},{"location":"using-the-system/jupyter-notebooks/#a-note-on-environment-variables-and-modules","title":"A Note on Environment Variables and Modules","text":"<p>Environment variables you have defined may not be set in the Jupyter environment, and modules you need may not be loaded. You can add these to the file <code>~/.jupyter/llsc_notebook_bashrc</code>, then shutdown and restart your Jupyter instance on the Jupyter portal (https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php). You can check that your environment is set the way you need it by opening a terminal and running the <code>env</code> command or by running the <code>module list</code> command to check which modules are loaded. Note that the Anaconda module is automatically loaded when starting up Jupyter, so you do not need to load it, and loading multiple Ananconda modules may cause unexpected behavior.</p>"},{"location":"using-the-system/monitoring-system-and-jobs/","title":"Monitoring System and Job Status","text":"<p>The four actions you may take the most are checking system status and starting, monitoring, and stopping jobs. Since scheduling jobs is a longer topic, see this page for an in-depth description of how to start your job. Here we describe how to check the status of the system for available resources, monitor a currently running job), and stop a running job.</p> <p>Each of these tasks is done through the scheduler, which is Slurm on the MIT Supercloud system. On this page and the job submission page we describe some of the basic options for submitting, monitoring, and stopping jobs. More advanced options are described in Slurm's documentation, and this handy two-page guide\u00a0gives a brief description of the commands and their options.</p>"},{"location":"using-the-system/monitoring-system-and-jobs/#checking-system-status","title":"Checking System Status","text":"<p>Our wrapper command, <code>LLGrid_status</code>, has a nicely formatted and easy to read output for checking system status:</p> <pre><code>[StudentX@login-0 ~]$ LLGrid_status\nLLGrid: txe1 (running slurm 16.05.8)\n============================================ Online Intel xeon-e5 nodes: 36\nUnclaimed nodes: 24\nClaimed slots: 172\nClaimed slots for exclusive jobs: 80\n-------------------------------------------- Available slots: 404\n</code></pre> <p>In the output, you can see the name of the system you are on (e1 here), the scheduler that's being used (Slurm), the number of unclaimed nodes, and the number of available slots.</p>"},{"location":"using-the-system/monitoring-system-and-jobs/#monitoring-jobs","title":"Monitoring Jobs","text":"<p>You can monitor your jobs using the <code>LLstat</code> command:</p> <pre><code>[StudentX@login-0 ~]$ LLstat   LLGrid: txe1 (running slurm 16.05.8)\nJOBID\u00a0\u00a0\u00a0\u00a0 ARRAY_J\u00a0\u00a0\u00a0 NAME\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 USER\u00a0\u00a0\u00a0\u00a0START_TIME\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PARTITION\u00a0 CPUS\u00a0 FEATURES\u00a0 MIN_MEMORY\u00a0 ST\u00a0 NODELIST(REASON)   40986\u00a0\u00a0\u00a0\u00a0 40986\u00a0\u00a0\u00a0\u00a0\u00a0 myJob\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Student\u00a0 2017-10-19T15:35:46 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2  40980_100 40980\u00a0\u00a0\u00a0\u00a0\u00a0 myArrayJob\u00a0Student\u00a0 2017-10-19T15:35:37 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2  40980_101 40980\u00a0\u00a0\u00a0\u00a0\u00a0 myArrayJob Student\u00a0 2017-10-19T15:35:37 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2  40980_102 40980\u00a0\u00a0\u00a0\u00a0\u00a0 myArrayJob Student\u00a0 2017-10-19T15:35:37 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2\n</code></pre> <p>The output of the LLstat command lists the job IDs of the jobs running, their names, the start time, the number of cpus per task, its status, and the node that it is running on. If it is in error state, it lists that as well.</p>"},{"location":"using-the-system/monitoring-system-and-jobs/#stopping-jobs","title":"Stopping Jobs","text":"<p>Jobs can be stopped using the <code>LLkill</code> command. You specify the list of job IDs, separated by commas that you would like to stop, for example:</p> <p><code>LLkill 40986,40980</code></p> <p>Stops the jobs with job IDs 40986 and 40980. You can also use the <code>LLkill</code> command to stop all of your currently running jobs:</p> <p><code>LLkill -u USERNAME</code></p>"},{"location":"using-the-system/software-packages/","title":"Software and Package Management","text":"<p>The standard environment on the MIT Supercloud System is sufficient for most. If it is not, first check to see if the tool you need is included in a module. Modules contain environment variables that set you up to use other software, packages, or compilers not included in the standard stack. If there is no module with what you need, you can often install your package or software in your home directory. Below we have instructions on how to install Julia, Python, and R packages. If you have explored both these options or are having trouble, contact us.</p>"},{"location":"using-the-system/software-packages/#modules","title":"Modules","text":"<p>Modules are a handy way to set up environment variables for particular work, especially in a shared environment. They provide an easy way to load a particular version of a language or compiler.</p> <p>To see what modules are available, type the command:</p> <p><code>module avail</code></p> <p>To load a module, use the command:</p> <p><code>module load moduleName</code></p> <p>Where <code>moduleName</code> can be any of the modules listed by the module avail command.</p> <p>If you want to list the modules you currently have loaded, you can use the module list command:</p> <p><code>module list</code></p> <p>If you want to change to a different version of the module you have loaded, you can switch the module you have loaded. This is important to do when loading a different version of a module you already have loaded, as environment variables from one version could interfere with those of another. To switch modules:</p> <p><code>module switch oldModuleName newModuleName</code></p> <p>Where <code>oldModuleName</code> is the name of the module you currently have loaded, and <code>newModuleName</code> is the new module that you would like to load.</p> <p>If you would like to unload the module, or remove the changes the module has made to your environment, use the following command:</p> <p><code>module unload moduleName</code></p> <p>Finally, in order to use the module command inside a script, you will need to initialize it first.</p> <p>The following shows a bash shell script example:</p> <pre><code>#!/bin/bash\n# Initialize the module command first\nsource /etc/profile\n\n# Then use the module command to load the module needed for your work\nmodule load anaconda/2023a\n</code></pre>"},{"location":"using-the-system/software-packages/#installing-software-or-packages-in-your-home-directory","title":"Installing Software or Packages in your Home Directory","text":"<p>Many packages and software can be installed in user space, meaning they are installed just for the user installing the package or software. The way to do this for Julia, Python, and R packages is described below. For other software, look at their installation documentation and see if they have instructions on how to install in your home directory. Sometimes this is described changing the installation location. Often you will have to download the source and build the software in your home directory to do this. Any dependencies can usually be installed in a similar way. If you run into trouble installing software you can reach out to us for help. Let us know what you have tried so far and we can often point you in the right direction.</p>"},{"location":"using-the-system/software-packages/#julia-packages","title":"Julia Packages","text":"<p>Adding new packages in Julia doesn't require doing anything special. On the login node, load a julia module and start Julia. You can enter package mode by pressing the <code>]</code> key and entering <code>add packagename</code>, where <code>packagename</code> is the name of your package. Or you can load <code>Pkg</code> and run <code>Pkg.add(\"packagename\")</code>.</p> <p>The easiest way to check if a package already exists is to try to load it by running <code>using packagename</code>. The <code>Pkg.status()</code> command will only show packages that you have added to your home environment. If you would like a list of the packages we have installed, the following lines should do the trick (where v1.# is your version number, for example v1.3):</p> <pre><code>using Pkg\nPkg.activate(DEPOT_PATH[2]*\"/environments/v1.3\")\ninstalled_pkgs = Pkg.installed()\nPkg.activate(DEPOT_PATH[1]*\"/environments/v1.3\")\ninstalled_pkgs\n</code></pre> <p>If you get an error trying to install a Julia package, first check to make sure you are on the login node, as the compute nodes don't have internet access. If you are already on the login node, it is possible that the installation is filling up the <code>/tmp</code> directory. The errors for this can be vague and differ between the different Julia versions. You can try changing the temporary directory that Julia uses to download its packages for installation by setting the <code>$TMPDIR</code> environment variable. You can create the new temporary directory and set the environment variable like this:</p> <pre><code>mkdir /state/partition1/user/$USER\nexport TMPDIR=/state/partition1/user/$USER\n</code></pre> <p>Once you have done this you can start up Julia and install packages as you normally would. Once you are done it is good practice to delete these temporary files.</p> <p>Note</p> <p>If you are using Jupyter there is an additional step you can optionally do so that Jupyter can find both our installed packages and your own. You can also run this if you are missing a Julia Kernel. First load a Julia module. Then, in a Julia shell, run:</p> <pre><code>using IJulia\ninstallkernel(\"Julia MyKernel\", env=Dict(\"JULIA_LOAD_PATH\"=&gt;ENV[\"JULIA_LOAD_PATH\"]))\n</code></pre> <p>The first part <code>Julia MyKernel</code> is what you want to call your kernel, so feel free to change this. The second part makes sure both our packages and any you've installed in your home directory show up on the load path when you use a Jupyter Notebook with this kernel.</p>"},{"location":"using-the-system/software-packages/#python-packages","title":"Python Packages","text":"<p>Many python packages are included in the Anaconda distribution. The quickest way to check if the package you want is in our module is to load the anaconda module, start python, and try to import the package you are interested in. Importing the packages in our Anaconda modules will also be much faster than importing packages that are installed in your home directory. This is because the packages in our Anaconda modules are installed on the local disk of every node, which is faster to access than packages installed in your home directory.</p> <p>If the package you are looking to install is not included in Anaconda, then you can install it in user space in your home directory- this allows you to install the package for you to use without affecting other users. We recommend that you use pip to do this, as pip allows you to only install the additional packages that you need in your home directory. Conda environments will result in installing all packages in your home directory, which can slow down the import process quite a bit.</p>"},{"location":"using-the-system/software-packages/#installing-packages-in-your-home-directory-with-pip","title":"Installing Packages in your Home Directory with pip","text":"<p>First, load the Anaconda module that you want to use if you haven't already:</p> <p><code>module load anaconda/2023a</code></p> <p>Here we are loading the 2021a module, the newer modules will have newer packages. Then, install the package with pip as you normally would, but with the <code>--user</code> flag:</p> <p><code>pip install --user packageName</code></p> <p>Where <code>packageName</code> is the name of the package that you are installing.</p> <p>If you get an error trying to install a package with pip, first check to make sure you are on the login node, as the compute nodes don't have internet access. If you are already on the login node, it is possible that the installation is filling up the <code>/tmp</code> directory, you may get a \"Disk quota exceeded\" error. You can change the temporary directory that pip uses to download its packages for installation by setting the <code>$TMPDIR</code> environment variable. You can create the new temporary directory, set the environment variable, and install your package like this:</p> <pre><code>mkdir /state/partition1/user/$USER\nexport TMPDIR=/state/partition1/user/$USER\n</code></pre> <p>Once you are done it is good practice to delete these temporary files.</p>"},{"location":"using-the-system/software-packages/#installing-packages-with-conda","title":"Installing Packages with Conda","text":"<p>As mentioned above, if at all possible we recommend you install packages in your home directory with pip rather than create a conda environment, as it'll be much faster. However, if you need to use a conda environment (usually this is because a package isn't available through pip or to help manage complex dependencies), you can do so by loading our anaconda module (this will give you access to the \"conda\" command) and then creating an environment the same way you would anywhere else. For example:</p> <pre><code>module load anaconda/2023a\nconda create -n my_env python=3.8 pkg1 pkg2 pkg3\n</code></pre> <p>In this example I am loading the <code>anaconda/2023a</code> module, then creating a conda environment named <code>my_env</code> with Python 3.8 and installing packages pkg1, pkg2, pkg3. We have found that conda creates more robust environments when you include all the packages you need when you create the environment. Then, whenever you want to activate the environment, first load the anaconda module, then activate with <code>source activate my_env</code>. Using <code>source activate</code>\u00a0instead of <code>conda activate</code> allows you to use your conda environment at the command line and in submission scripts without additional steps.</p> <p>If you would like to use your conda environment in Jupyter, simply install the \"jupyter\" package into your environment. Once you have done that, you should see your conda environment listed in the available kernels.</p> <p>Note:\u00a0If you are using a conda environment and would like to install the package with pip in that environment rather than in the standard home directory location, you should not include the <code>--user</code> flag. Further, if you are using a conda environment and want Python to use packages in your environment first, you can run the following two command:</p> <p><code>export PYTHONNOUSERSITE=True</code></p> <p>This will make sure your conda environment packages will be chosen before those that may be installed in your home directory. If you are using Jupyter, you will need to add this line to the\u00a0<code>.jupyter/llsc_notebook_bashrc</code>\u00a0file.\u00a0See the section on the bottom of the Jupyter\u00a0page for more information.</p>"},{"location":"using-the-system/software-packages/#r-libraries","title":"R Libraries","text":"<p>There are two different ways we recommend that you use R. First, is using a preset R environment that comes with the anaconda module, second would be to create your own R conda environment. This first way works best if you don't need to install any additional packages than what we already have.</p> <p>To use our R conda environment, log in and load an anaconda module. Then you can activate the R environment with <code>source activate</code>. You can see what packages are installed with the <code>conda list</code> command. Any packages that start with <code>r-</code> are R libraries.</p> <pre><code>module load anaconda/2023a\nsource activate R\nconda list\n</code></pre> <p>Then you can use R as you did before.</p> <p>If you need to install additional packages, it's best to do it in a new conda environment.</p> <p>First thing to know is that many R packages are available through conda, and some are not. What you want to do is include as many R libraries that you'll need as you can when you create your conda environment- this helps avoid version conflicts. Conda r libraries are all prefixed with <code>r-</code>, so for example if you need rjava, you'd search for <code>r-rjava</code>. You can check if conda has a library with the command: <code>conda\u00a0search\u00a0r-LIBNAME</code>, where <code>LIBNAME</code> is the name of the library you're looking for. You'll see a lot of versions, but as long as you see something you should be good to add it.</p> <p>Once you have a list of all the libraries available through conda, create your conda environment (I'm calling the environment myR, feel free to change that):</p> <p><code>conda create -n myR -c conda-forge r-essentials r-LIB1 r-LIB2\u2026</code></p> <p>Where LIB1, LIB2, etc are the additional R libraries you'd like to include. Sometimes this step takes a while. It'll tell you which new packages are going to be installed, and then you can confirm by typing <code>y</code>.</p> <p>If you have any other libraries that weren't available through conda, install them now. First activate your new environment and then start R:</p> <pre><code>source activate myR\nR\n</code></pre> <p>Then you can install your remaining libraries. You can do some test loads here as well, to make sure the libraries installed properly.</p> <pre><code>install.packages(\u201cPKGNAME\u201d)\nlibrary(PKGNAME)\n</code></pre> <p>In Jupyter, you should see your environment show up as a kernel. For a batch job, you'll have to activate the environment either in your submission script or before you submit the job.</p>"},{"location":"using-the-system/web-portal/","title":"Web Portal","text":"<p>You can get to the Supercloud Web Portal at this URL:</p> <p>https://txe1-portal.mit.edu</p> <p>On this page you will find links to a number of useful tools. These include</p> <ul> <li>Page to add or remove ssh keys</li> <li>Home directory file browser</li> <li>Dynamic database launching system</li> <li>Jupyter Notebook portal</li> </ul>"},{"location":"using-the-system/web-portal/#portal-authentication","title":"Portal Authentication","text":"<p>There are three ways you can authenticate into the Web Portal: PKI Certificate/Smart Card, MIT Touchstone/InCommon Federation, Username and Password.</p>"},{"location":"using-the-system/web-portal/#mit-touchstoneincommon-federation","title":"MIT Touchstone/InCommon Federation","text":"<p>Most Supercloud users can log in via the MIT Touchstone/InCommon Federation link. These include users with an active MIT Kerberos account or account at most other educational institutions. When you click the \"MIT Touchstone/InCommon Federation\" link for the first time you will be taken to a page where you can select your institution. Note most options are spelled out, for example \"MIT\" is listed as \"Massachusetts Institute of Technology\". UMass Lowell users should select \"University of Massachusetts System\". When you have selected your institution, you should check the box that says \"Remember my Choice\" and click select. You'll be taken to your institution's login page where you can log in. Then you will be taken to the Portal main page.</p> <p>If you get a message saying \"The MIT Touchstone / InCommon Federation principal you presented is not associated with an account on this system.\u00a0Please sign up at: https://supercloud.mit.edu/requesting-account.\" and you already have an account contact us and let us know.</p>"},{"location":"using-the-system/web-portal/#username-and-password","title":"Username and Password","text":"<p>If you do not have an active login for an educational institution you may need to log into the portal using your username and password. Let us know and we will let you know how to do this.</p>"},{"location":"using-the-system/web-portal/#pki-certificatesmart-card","title":"PKI Certificate/Smart Card","text":"<p>Those who have a Smart Card or PKI Certificate can log in using the first link on the page. If you would like to log in using your PKI Certificate or Smart Card, please let us know.</p>"},{"location":"using-the-system/web-portal/#addingremoving-ssh-keys","title":"Adding/Removing SSH Keys","text":"<p>The first link on the Portal Home page is \"/sshkeys/\". Clicking on this link takes you to a page where you can add new keys or remove old ones. To add a new key, paste the key in the box at the bottom of the page and click \"Update Keys\". You should see your new key populated in the table and display similarly to the other keys listed. To remove an old key, click the check box next to the key you would like to remove and click \"Update Keys\" at the bottom. Instructions for generating a new key are here.</p>"},{"location":"using-the-system/web-portal/#browsing-your-home-directory","title":"Browsing your Home Directory","text":"<p>The second link on the Portal Home page is \"/gridsan/\". Click this link and then select your username to see the files in your home directory. You can click on a file to download it or click on a directory to navigate to that directory.</p>"},{"location":"using-the-system/best-practices/filesystem/","title":"Best Practices for Using the Filesystem","text":""},{"location":"using-the-system/best-practices/filesystem/#installing-python-packages","title":"Installing Python Packages","text":"<ul> <li>Use our anaconda modules whenever possible. The newest anaconda     module should have the most up to date versions. Our anaconda     modules are on the local disk of all the nodes and so does not     affect the shared filesystem.</li> <li>If you need to install additional Python packages, install with pip     using the <code>--user</code> flag as described in our     documentation. Python     will then only go to your home directory for these installed     packages, and so should be less load on the shared filesystem. Only     use conda environments as a last resort, as this puts ALL packages     you use in your home directory, and creates many small files.</li> <li>DO NOT install anaconda or miniconda in your home directory. There     is no reason to do this and will slow your Supercloud experience     down significantly, as these installations contain many, many small     files. If you absolutely need to use conda to install a package     create a conda environment using our anaconda modules. If you have     previously installed anaconda or miniconda in your home directory,     delete it now.</li> </ul>"},{"location":"using-the-system/best-practices/filesystem/#submitting-jobs","title":"Submitting Jobs","text":"<ul> <li>Use Triples Mode for     submitting Job     Arrays and     LLMapReduce jobs. These create fewer log files and group them by     node, reducing the number of files per directory. It is also lighter     weight on the scheduler, as it creates fewer tasks/jobs that the     scheduler has to keep track of.</li> <li>DO NOT create very large Job Arrays. Each task in a Job Array     creates a log file, the more tasks in your array, the more files.     The best practice is to use Triples to submit your job arrays (see     bullet above).</li> <li>DO NOT submit many small jobs, most likely a Job Array or     LLMapReduce with Triples would be appropriate.</li> <li>Avoid doing things that actively stress the filesystem, for example     checking whether a file exists repeatedly over a long period of time     or across many tasks.</li> </ul>"},{"location":"using-the-system/best-practices/filesystem/#file-organization","title":"File Organization","text":"<ul> <li>Aim for less than ~1000 files per directory.</li> <li>Fewer, larger files are better than many small files (file size     should be a minimum of 1MB, target ~100MB).</li> <li>Within a job, you can use $TMPDIR for temporary or intermediate     files you don't need after the job. $TMPDIR points to a temporary     directory on the local filesystem that is set up at the start of     your job and removed at the end of your job. If your job requires a     lot of I/O you may see significant performance improvement by     copying the files you need to $TMPDIR at the start of your job and     copy any new files you need to your home directory at the end of     your job.</li> <li>Use shared directories to share data among team members rather than having a     separate copy in everyone's home directory.</li> <li>Check /home/gridsan/groups/datasets before downloading large public     datasets. If there is a public dataset that you are considering     downloading that you think others may also want to use, send us a     email to supercloud@mit.edu to suggest that we add it.<ul> <li>If you are using ImageNet, we have a special setup that will stress the filesystem significantly less and should be much faster. First load its modulefile: <code>module load /home/gridsan/groups/datasets/ImageNet/modulefile</code>. This will set up the <code>$IMAGENET_PATH</code> environment variable, which you can use in your code to point to ImageNet.</li> </ul> </li> </ul>"},{"location":"using-the-system/best-practices/gpu-jobs/","title":"Optimizing your GPU Jobs","text":"<p>The GPUs on Supercloud often are in very high demand. Before asking for more GPUs you'll want to do what you can to optimize your code to get the most out of the resources you do have.</p> <p>The first thing you need to do is profile your code.\u00a0Time your code both with and without the GPU. When you are running with a GPU, monitor the GPU Utilization and GPU Memory use. You can do this by going to the compute node where it is running (get the node name from LLstat, then ssh to the node with <code>ssh nodename</code>) and run the <code>nvidia-smi -l</code> command (press <code>Ctrl+C</code> to exit <code>nvidia-smi -l</code> when you are done).</p> <p>If your GPU utilization and GPU memory use is low, that means you aren't getting the full advantage out of the GPU. If this is the case, try out some of the suggestions below.</p> <p>If you are only getting a small speedup, say 2x or 4x, especially after trying the suggestions below, it may not be worthwhile using GPUs at all. In general, the CPU nodes on Supercloud are more available so we will likely be willing to increase your allocation of CPU nodes to make up for the small loss of speedup switching to CPUs.</p>"},{"location":"using-the-system/best-practices/gpu-jobs/#getting-more-out-of-the-gpus","title":"Getting More out of the GPUs","text":"<p>If your GPU utilization and memory use are low, usually this means you might be able to get some more performance out of the GPUs. There are a few things you can try if you are training machine learning models:</p> <ul> <li>Increase the batch size.</li> <li>Enable CUDA kernel tuning.<ul> <li>For Pytorch, add this before the training starts:     <code>torch.backends.cudnn.benchmark = True</code></li> <li>For Tensorflow, set this environment variable in your submission     script: <code>export TF_CUDNN_USE_AUTOTUNE=1</code></li> </ul> </li> <li>If you are using Tensorflow you can also try mixed-precision     training (we haven't played with this in Pytorch, but it could be     possible).<ul> <li>Tensorflow 2.4.1 and newer (anaconda/2021a+)<ul> <li>Add the following to the beginning of your Python code:<ul> <li><code>from tensorflow.keras import mixed_precision</code></li> <li><code>policy = mixed_precision.Policy('mixed_float16')</code></li> <li><code>mixed_precision.set_global_policy(policy)</code></li> </ul> </li> </ul> </li> <li>Pre-Tensorflow 2.4.1 (anaconda/2020b and older)<ul> <li>To do so set the following environment variables in your     submission script:<ul> <li><code>export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1</code></li> <li><code>export TF_ENABLE_AUTO_MIXED_PRECISION=1</code></li> </ul> </li> <li>And add this to your Python code, here <code>opt</code>\u00a0is the     optimizer object:<ul> <li><code>opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)</code></li> </ul> </li> </ul> </li> </ul> </li> <li>If the number of filters in the model layers is a multiple of 64,     you will see an additional speedup because the V100 tensor cores are     optimized for matmul/conv ops of these sizes.</li> <li>If both GPU utilization and memory are below 50% you could try to     train two models per GPU. This isn't too hard to do with Triples     Mode, simply set     the number of processes per node to 4.</li> </ul>"},{"location":"using-the-system/best-practices/gpu-jobs/#requesting-more-gpus","title":"Requesting More GPUs","text":"<p>If you have tried the above suggestions and you still find you need more GPUs you can put in a request. Send an email to supercloud@mit.edu and tell us what you need, what you need it for, and how long you need the allocation increase. Are you trying to run a large distributed training run, or are you doing hyperparameter searches? Justify your request with numbers: show us that you have compared CPU and GPU run times, that you have good GPU utilization, and that you have tried the above suggestions. Explain how you got to the number of GPUs you are requesting. If the system is not too busy we may grant your request. Keep in mind that others are likely to have the same paper deadlines as you and we may not be able to grant requests during these busy periods, so plan ahead.</p>"},{"location":"using-the-system/files-and-data/shared-groups/","title":"Shared Groups","text":""},{"location":"using-the-system/files-and-data/shared-groups/#overview","title":"Overview","text":"<p>This page summarizes Shared Groups, including how to request to join or create a group, some best practices for working with shared groups, and how to inspect and change file permissions and group ownership.</p> <p>Supercloud users are welcome to either join an existing group and receive the benefit of access to a shared file directory, or propose the creation of a group if there is some common interest amongst certain Supercloud account holders, perhaps they are members of the same lab.</p> <p>You can look at the current list of groups by listing the groups directory:</p> <p><code>studentx@login-3:~$ ls /home/gridsan/groups/</code></p> <p>You can see what groups you are currently in by running the \"groups\" command:</p> <p><code>studentx@login-3:~$ groups</code></p>"},{"location":"using-the-system/files-and-data/shared-groups/#joining-or-creating-a-group","title":"Joining or Creating a Group","text":"<p>If you would like to join a group, send an e-mail to supercloud@mit.edu with that request and CC the group owner for approval. The group owner must give approval before we can add you to the group. If you are not sure who the group owner/approver is, you can send in your request and we will reach out to the approver.</p> <p>If you would like to create a group, please email supercloud@mit.edu with the following info.</p> <ul> <li>What should the group be called?</li> <li>Who should the group owner/approver be? We will ask this person for     approval if anyone asks to be added.</li> <li>Who should be in the group, listing usernames is most helpful to us,     but not required.</li> <li>Whether you plan to store any non-public data in the group. If so,     please list any requirements, restrictions, or agreements associated     with the data. The more information you give us, the better.</li> </ul>"},{"location":"using-the-system/files-and-data/shared-groups/#using-shared-groups-effectively","title":"Using Shared Groups Effectively","text":"<p>Once you have been added to a group you will be able to access that group's shared directory. All group directories are located in the <code>/home/gridsan/groups</code> directory on the filesystem. Since this is part of the central filesystem along with your home directory, all nodes in Supercloud can access the group directories. We will also add a symlink in your home directory to your group shared directory, this symlink will have the suffix \"_shared\" to indicate it is linking to a group directory. If you are sharing code with other members of your team that includes paths to a shared group, it is good practice to use a path that does not include your home directory, otherwise your team members will get a permission denied error when they try to run your code. Instead, it is best to use the absolute path through <code>/home/gridsan/groups</code>.</p> <p>All of our Best Practices for using the Filesystem apply to the group directories. Additionally, NEVER use a GUI to drag and drop files into a group directory. Doing so can alter the permissions in the group directory, preventing others in your group from accessing the files you've moved into the shared group directory. When using <code>rsync</code> to transfer files into a group directory, be sure to use the <code>-g</code> flag, which will also help keep the group ownership set properly.</p>"},{"location":"using-the-system/files-and-data/shared-groups/#linux-file-permissions","title":"Linux File Permissions","text":"<p>Sometimes, despite your best efforts, the permissions on a group can be altered such that you or others in your group can't interact with a file the way they need to. If that happens, you can always contact us at supercloud@mit.edu and we can fix it. However, you may find it more convenient to fix it on your own. Here is a brief introduction to Linux File Permissions to help you learn what is going on and how to fix it.</p>"},{"location":"using-the-system/files-and-data/shared-groups/#inspecting-file-permissions","title":"Inspecting File Permissions","text":"<p>If you do a long form listing of the files in a directory using <code>ls -l</code>:</p> <pre><code>drwxrwx---\u00a0 \u00a0 2 studentz studentz\u00a0 \u00a0 \u00a04096 Jun 15 14:51\u00a0 mydirectory\nlrwxrwxrwx\u00a0 \u00a0 1 root\u00a0 \u00a0 \u00a0root\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 26 Jun 15 17:24\u00a0 files_shared -&gt; ../groups/fileshare\n-rw-------\u00a0 \u00a0 1 studenty studenty\u00a0 \u00a0 4096 Jun 30 09:02\u00a0 logfile1\n-rw-rw---\u00a0 \u00a0 \u00a01 studentx Alpha\u00a0 \u00a0 \u00a0 \u00a04096 Jun 30 09:02\u00a0 logfile2\n</code></pre> <p>You will see the file permissions of your various directories, symlinks, and files in the leftmost columns. The first column indicates whether the file is a directory (d), symlink (l), or a regular file. Columns 2 through 10 can be viewed as triplets that define access permissions for the file or folder. To explicitly define permissions you will need to reference the Permission Group and Permission Types:</p> <ul> <li>The Permission Groups are:\u00a0 u -- Owner\u00a0 \u00a0g -- Group\u00a0 o -- Others\u00a0\u00a0</li> <li>The Permission Types are:\u00a0 r -- Read\u00a0 w -- Write\u00a0 \u00a0x -- Execute</li> </ul> <p>The first of these triplets represent the Owner's permissions, the second the Group's, and the third Others'. An r,w, or x represent the ability to perform that action, and a \"-\" means that action is not permitted. For a file like <code>logfile1</code> above you can see that it is owned by user <code>studenty</code> (from group <code>studenty</code>) and only the owner has read and write permissions. The file named <code>logfile2</code> currently has the permissions set to <code>-rw-rw----</code>, which means that the owner and group have read and write permission. SuperCloud does not allow you to add read, write, or execute permissions for others, or all users. One important thing to note: in order to go into a directory you must have execute permissions on that directory. So if you get a \"Permission denied\" error when trying to enter or look at the files in a directory, check whether the directory has read and execute permissions.</p>"},{"location":"using-the-system/files-and-data/shared-groups/#changing-file-permissions","title":"Changing File Permissions","text":"<p>Now say we want to change permissions for a file. One of the easiest ways is to use the Assignment Operators, + (plus) and -- (minus). These are used to tell the system whether to add or remove the specific permissions.</p> <p>For example, to add group read and write permission for <code>logfile1</code>, you would invoke the command:</p> <p><code>chmod g+rw logfile1</code></p> <p>Now say you want your group to be able to read <code>logfile2</code>, but don't want anyone to accidentally modify it. To remove group write permissions you would invoke the command:</p> <p><code>chmod g-w logfile2</code></p> <p>It's very important to know that if you want to apply these changes recursively that you use the <code>-R</code> (with a capital R) flag. Using a lowercase <code>-r</code> flag like you do for other Linux commands like <code>cp</code> will remove write permissions for everyone, including yourself. If you make this mistake, it is not the end of the world, but you will need to send us an email and have us fix it.</p> <p>Alternately you can define the full permissions options with binary references like <code>chmod 750 logfile1</code> which would grant full privileges (7) to the owner, and rw privileges (5) to the group and nothing (0) to others in a single command. You can learn more options and about chmod either from an online tutorial or from your local man pages (<code>man chmod</code>, typing <code>q</code>\u00a0will exit) or with the quick cheat sheet you can display with <code>chmod --help</code>. The following is also pretty good tutorial, but be aware it talks about permissions in general, and not everything will be relevant to shared groups or Supercloud: How to use the chmod Command on Linux.</p>"},{"location":"using-the-system/files-and-data/shared-groups/#linux-file-ownership","title":"Linux File Ownership","text":"<p>If we take another look at the example directory above:</p> <pre><code>drwxrwx---\u00a0 \u00a0 2 studentz studentz\u00a0 \u00a0 \u00a04096 Jun 15 14:51\u00a0 mydirectory\nlrwxrwxrwx\u00a0 \u00a0 1 root\u00a0 \u00a0 \u00a0root\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 26 Jun 15 17:24\u00a0 files_shared -&gt; ../groups/fileshare\n-rw-rw----\u00a0 \u00a0 1 studenty studenty\u00a0 \u00a0 4096 Jun 30 09:02\u00a0 logfile1\n-rw-r----\u00a0 \u00a0 \u00a01 studentx Alpha\u00a0 \u00a0 \u00a0 \u00a04096 Jun 30 09:02\u00a0 logfile2\n-rw-r-x---\u00a0 \u00a0 1 studenty studenty\u00a0 \u00a0 4096 Jun 30 09:02\u00a0 myscript.py\n</code></pre> <p>the 12th and 13th column of the <code>ls -l</code> output is the owner of the file, listed first, and the group for the file, listed second. For example, <code>logfile2</code> is owned by <code>studentx</code> and its group is <code>Alpha</code>. Based on the permissions above, <code>studentx</code> can read and write to the file, and anyone in the <code>Alpha</code> group can read the file, but cannot write to it.</p> <p>In a group directory the group owner for a file should usually be the group associated with that directory. Sometimes it unintentionally gets set to the username of the person who created or put the file there. This can easily be remedied by using the <code>chgrp</code> command. For example, let's say we'd like everyone in the <code>Alpha</code> group to be able to read and run (execute) the file <code>myscript.py</code>, but not have write permissions. The group permissions are set properly, but the group is set to <code>studenty</code>\u00a0instead of <code>Alpha</code>. To fix this, we can run:</p> <p><code>chgrp Alpha myscript.py</code></p> <p>Again, if you would like to apply this change recursively, the flag is <code>-R</code> (with a capital R).</p>"},{"location":"using-the-system/files-and-data/transferring-files/","title":"Transferring Files","text":"<p>There are number of ways to access and transfer data and files between your computer and the MIT Supercloud System depending on your OS (Mac/Linux or Windows) and your comfort with Linux/Unix commands. Regardless of OS, files can be downloaded, but not uploaded, through the web portal and both uploaded and downloaded through the Jupyter web interface. For Mac/Linux or Windows with Cygwin, you have the additional options of using <code>scp</code> and <code>rsync</code> from a terminal window. If you are using PuTTY on Windows, there is a way to transfer through that program as well. All of these are described below.</p>"},{"location":"using-the-system/files-and-data/transferring-files/#maclinux-and-most-windows","title":"Mac/Linux and Most Windows","text":""},{"location":"using-the-system/files-and-data/transferring-files/#via-scp","title":"Via scp","text":"<p>You can use <code>scp</code> (secure copy) to copy files to the MIT Supercloud. The <code>scp</code> command expects a path to the file or directory name that you would like to transfer followed by a space and then the destination. Note, it is best to copy to or from your local machine. In addition, the location on the system is the login node followed by a <code>:</code> (colon) followed by the path to the directory where you want to put the file. If you do not specify the directory location <code>scp</code> will place it in your top level directory, which on the Supercloud system is your home directory. An example of copying a directory to TX-E1 is shown below. Note that the <code>scp</code> command is followed by <code>-r</code>, to recursively copy all the files in the directory, and the directory name is followed by a <code>/</code> so that <code>scp</code> copies all of the files in the directory.</p> <p><code>myLocalMachine UserName &gt; scp -r testCodes/ &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/snippets</code></p> <p>If you want to copy data from TX-E1 to your computer, you just swap the \"from\" and \"to\":</p> <p><code>myLocalMachine UserName &gt; scp -r &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/results/</code></p>"},{"location":"using-the-system/files-and-data/transferring-files/#via-rsync","title":"Via rsync","text":"<p>The <code>rsync</code> (remote synchronization) command is similar to the <code>scp</code> command. However, after the first time that you copy files, executing the <code>rsync</code> command will copy only the files that have changed since the last rsync. This can often save time. Note that if you have the slash at the end, it means \"all contents in the directory\". If not, it means the directory itself and its contents. As shown in case A below, the directory, <code>myDir</code>, and its all contents will be copied over to the snippets directory on the remote host. However, in case B, only the contents in the <code>myDir</code> directory will be copied to the snippets directory on the remote host.</p> <p>CaseA: <code>rsync -rlug myDir &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/snippets</code> CaseB: <code>rsync -rlug myDir/ &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/snippets</code></p> <p>There are a number of options that can be used with rsync. Among the most commonly used are:</p> <ul> <li><code>r</code> to recursively copy files in all sub-directories</li> <li><code>l</code> to copy and retain symbolic links.</li> <li><code>u</code> is needed if you have modified files on the destination and you     don't want the old file to overwrite over the newer version on the     destination.</li> <li><code>g</code> is used to preserve group attributes associated with files in a     shared group.</li> <li><code>h</code> human readable</li> <li><code>v</code> verbose so that you get any error or warning information</li> </ul> <p>Again, if you want to copy data from TX-E1 to your computer, you just swap the \"from\" and \"to\":</p> <p>CaseA: <code>rsync -rlug &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/results myDir</code> CaseB: <code>rsync -rlug &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/results/ myDir</code></p>"},{"location":"using-the-system/files-and-data/transferring-files/#windows","title":"Windows","text":"<p>Depending on which ssh client you use you will transfer data differently. If you use Bash on Windows 10, Mobaxterm, Cygwin, or other Linux-like command line environment, you can use scp or rsync directly and should follow the instructions above for Mac/Linux. If you use PuTTY, you can follow the directions below to use pscp. You run these commands from your own computer, not from the MIT SuperCloud login node (if you have already logged in with ssh, you've gone too far).</p> <p>IMPORTANT: MIT SuperCloud runs a Linux operating system. Two big differences between Windows and Linux are the direction that the folder separators face (Windows uses \"\\\" and Linux uses \"/\") and case sensitivity (Linux file and directory names are case sensitive, while Windows file and folder names are not). Whether you are using scp, rsync, or pscp you must use the Linux slashes \"/\" in your path and the correct case in your filenames (Documents =/= documents). It will also be much easier to transfer data if you avoid using spaces in your file and directory/folder names.</p> <p>HINT: Windows paths sometimes specify different \"drives\" (such as the C: drive, where most local documents are kept). Different ssh clients treat these drives differently. Using the example of a folder in the C drive (say C:myFolder), the translations for each are:</p> <ul> <li>Bash for Windows 10: <code>/mnt/c/myFolder</code></li> <li>Mobaxterm: <code>/drives/c/myFolder</code></li> <li>Cygwin: <code>/cygdrive/C/myFolder</code></li> </ul> <p>When in doubt, you can navigate to the directory that you want to transfer and issue the <code>pwd</code> command, which should give you the full path to your current location.</p>"},{"location":"using-the-system/files-and-data/transferring-files/#putty","title":"PuTTY","text":"<p>When transferring files from a Windows machine to the Supercloud system, which are linux machines, you need to use a version of <code>scp</code>, or secure copy. The software package, PuTTY, which you installed to provide <code>ssh</code>, includes <code>scp</code>. To transfer a file between your Windows desktop and the Supercloud linux system:</p> <ul> <li>Open a command window by typing <code>run</code> in the search box. The <code>scp</code>     command must be run from a command window.</li> <li>Set the <code>PATH</code> variable so that your system sees the putty command:</li> <li>To do it for this session only, type         <code>set PATH=C:\\Program Files\\PuTTY</code> at the command prompt</li> <li>To make this change more permanent, click on the start button,         in the search box type Environment Variables:<ul> <li>Select Add/Modify Environment Variables for user</li> <li>Click on Path and then click to edit Path.</li> <li>Add <code>C:\\Program File\\PuTTY</code> (This assumes that you installed         PuTTY in the default folder.)</li> <li>Confirm that PuTTY is in your path by typing: <code>%ECHO PATH%</code>         at the command line.</li> </ul> </li> <li>Navigate to the directory (folder) that holds the file you want to     transfer</li> <li>The <code>pscp</code> command has the format     <code>pscp &lt;Path_to_FileToBeTransfered&gt; &lt;Path_To_NewLocation&gt;</code></li> <li>For example to use <code>pscp</code> to copy the file <code>myMatlabScript.m</code> from     my current directory (folder) to my home directory on the Supercloud     system I would type</li> </ul> <p>If I wanted to copy <code>myResults.dat</code> from my home directory on the Supercloud system to a folder called <code>goodData</code> on my local machine I would first change directories so that I was in goodDate, using: <code>cd goodData</code>. Then, secure copy the file from SuperCloud to my local system using:</p> <p><code>pscp &lt;myUserID&gt;@txe1-login.mit.edu:/home/gridsan/&lt;myUserID&gt;/myResults.dat .</code></p> <p>Note the <code>.</code> at the end of the line - that is equivalent to \"place it here\". Also note that in Windows you can use <code>\\</code> or <code>/</code> but Linux only understands <code>/</code>.</p>"},{"location":"using-the-system/files-and-data/transferring-files/#cygwin","title":"Cygwin","text":"<p>Cygwin also supports the secure copy protocol. This works similarly to scp in a Linux/Mac. When using Cygwin, you will be able to run <code>scp</code>, however it can be tricky to determine to the path to your files. This is of the form <code>/cygdrive/C/&lt;your_File_Path&gt;</code>.</p> <p>Say you are using scp to copy the file myScript.sh from your current directory (folder) to your home directory on the Supercloud system. Type:</p> <p><code>scp /cygdrive/C/&lt;path_to_Folder&gt;/mScript.sh &lt;myUserID&gt;@txe1-login.mit.edu:/home/gridsan/&lt;myUserID&gt;</code></p> <p>If you wanted to copy <code>myResults.txt</code> from your home directory on the Supercloud system to a folder called <code>goodData</code> on your local machine, first change directories so that you are in <code>goodData</code>, using: <code>cd goodData.</code></p> <p>Secure copy the file from the Supercloud system to the local system using:</p> <p><code>scp &lt;myUserID&gt;@txe1-login.mit.edu:/home/gridsan/&lt;myUserID&gt;/myResults.txt  /cyqdrive/c/&lt;path_to_goodData&gt;/</code></p> <p>Note that in Windows you can use <code>\\</code> or <code>/</code> but Linux only understands <code>/</code>.</p>"},{"location":"using-the-system/files-and-data/transferring-files/#downloading-files-through-the-web-portal","title":"Downloading Files through the Web Portal","text":"<p>The web portal can be accessed at:</p> <p>https://txe1-portal.mit.edu/gridsan/USERNAME/</p> <p>Where USERNAME is your username. When you navigate to the portal page, you will be prompted for your username and password. Through the portal, you can navigate through your directories and download files. You cannot upload files this way.</p>"},{"location":"using-the-system/files-and-data/transferring-files/#uploading-and-downloading-files-using-jupyter","title":"Uploading and Downloading Files Using Jupyter","text":"<p>You can both upload and download files using the Jupyter interface through your browser. If you do not already have a Jupyter instance running, start one up. The Jupyter portal is located at:</p> <p>https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php</p> <p>Once you have an instance running, follow the \"Open Notebook\" link to open Jupyter. You will see the files and directories in your home directory. Here you can right click on a file and select \"Save Link As\" to download a file. The \"Upload\" button is located in the top right of the page, right next to the \"New\" button. You can select multiple files to upload at a time.</p>"},{"location":"using-the-system/submitting-jobs/job-array-triples/","title":"Job Arrays with LLsub Triples in 3 Steps","text":"<p>If you are currently running a Job Array, you can take advantage of SuperCloud's triples mode submission by submitting your job array with LLsub. In most cases, this can be done in a few easy steps.</p> <p>What is triples mode? It's a different way to submit your job by specifying three numbers:</p> <ul> <li>Nodes: The number of nodes you want to use up</li> <li>NPPN: The number of processes that should run per node</li> <li>NTPP: The number of threads per process</li> </ul> <p>Why would you want to use triples mode to submit your job? Triples mode provides a few advantages. Since it does whole-node scheduling you don't have to worry about other jobs impacting yours (or your job impacting someone else's). We also do task-pinning when you request resources with a triple, so your processes are arranged in the best way possible for the layout of the hardware architecture. Finally, submitting a Job Array with LLsub triples mode the way we describe here will greatly reduce the startup time for your jobs, over a job array with many pending tasks.</p>"},{"location":"using-the-system/submitting-jobs/job-array-triples/#step-1-batch-up-your-array","title":"Step 1: Batch Up your Array","text":"<p>The first thing you want to do is set up your job so that your script splits up your inputs among each process/task and each process/task iterates a subset of your inputs. This way you can change your input set without changing the number of tasks or processes that you schedule. This step alone will save you on startup time. Anytime you submit more jobs than your allocation allows, those additional jobs will remaining pending until some of your first running jobs complete. Then, when one of your first jobs complete, the scheduler now has to find the pending job some resources and start it running. If you batch up your job array, you only have to go through the scheduling process once. If you've set up a job array following our instructions in the past you may already be doing this, and you can skip to the next section.</p> <p>First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you'll first want to enumerate all the combinations of what you are iterating over so you have one big loop.</p> <p>If your code is written so it uses the <code>$SLURM_ARRAY_TASK_ID</code> and uses that to run a single thing, first add a for loop that iterates over the full set of things you want to run your code on. If you can't rewrite your code in such a way that it iterates over multiple inputs, you can use LLMapReduce to submit your job with triples mode, see this example.</p> <p>Then you add a few lines to your code to take in two arguments, a process/task ID and the number of processes/tasks, and use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, <code>fnames</code>. In python I would add:</p> PythonJulia <pre><code># Grab the arguments that are passed in\nmy_task_id = int(sys.argv[1])\nnum_tasks = int(sys.argv[2])\n# Assign indices to this process/task\nmy_fnames = fnames[my_task_id:len(fnames):num_tasks]\nfor f in my_fnames: ...\n</code></pre> <pre><code># Grab the arguments that are passed in\ntask_id = parse(Int,ARGS[1])\nnum_tasks = parse(Int,ARGS[2])\n# Assign indices to this process/task\nmy_fnames = fnames[task_id+1:num_tasks:length(fnames)]\nfor f in my_fnames\n...\n</code></pre> <p>Notice that I am iterating over <code>my_fnames</code>, which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this).</p>"},{"location":"using-the-system/submitting-jobs/job-array-triples/#step-2-changing-your-submission-script","title":"Step 2: Changing Your Submission Script","text":"<p>If you have been running your Job Arrays with sbatch, you most likely have a few environment variables in your submission script. To submit with LLsub triples, you can just replace these:</p> <ul> <li><code>$SLURM_ARRAY_TASK_ID</code> -&gt; <code>$LLSUB_RANK</code></li> <li><code>$SLURM_ARRAY_TASK_COUNT</code> -&gt; <code>$LLSUB_SIZE</code></li> </ul> <p>If you have any SBATCH flags in your submission script, remove those as well (LLsub will see these and submit with sbatch, ignoring any command line arguments you give it). For example, if you are running a python script, your final submission script will look something like this:</p> <pre><code>#!/bin/bash\n# Load Module(s)\nmodule load anaconda/2023a\n\necho \"My task ID: \" $LLSUB_RANK\necho \"Number of Tasks: \" $LLSUB_SIZE\npython top5each.py $LLSUB_RANK $LLSUB_SIZE\n</code></pre> <p>You will also need to make your script executable. You can do that with this simple command line command:</p> <p><code>chmod u+x submit_LLsub.sh</code></p>"},{"location":"using-the-system/submitting-jobs/job-array-triples/#step-3-submit-your-job-with-llsub-triples","title":"Step 3: Submit your Job with LLsub Triples","text":"<p>Now when you submit your job, you can run:</p> <p><code>LLsub ./submit.sh [NODES,NPPN,NTPP]</code></p> <p>where</p> <ul> <li><code>NODES</code> is the number of nodes you want to use up</li> <li><code>NPPN</code> is the number of processes that should run per node</li> <li><code>NTPP</code> is the number of threads per process</li> </ul> <p>The total number of threads per node, or <code>NTPP*NPPN</code>, should not be more than the number of cores on the node, otherwise you may overwhelm the node with too many running processes and/or threads. For example, if you are running on the 48-core Xeon-P8 nodes, if you are running with <code>NTPP</code>=1, you should not set <code>NPPN</code> more than 48. If <code>NTPP</code>=2, <code>NPPN</code> should be at most 24, etc. The numbers you choose depend on your application, if it is multithreaded it may be worth increasing <code>NTPP</code> and decreasing <code>NPPN</code>. If your application consumes a lot of memory, you may need to decrease <code>NPPN</code> so each process has the memory it needs to proceed. The best way to determine what numbers to choose is to tune your triples, which is a relatively quick exercise and can improve your speedup in the long run by helping you select the ideal numbers for your triple.</p> <p>So if you want to run on 2 nodes, 10 processes per node, and 4 threads per process, you would run:</p> <p><code>LLsub ./submit.sh [2,10,4]</code></p> <p>If you were to run LLstat after running this command, you would see what looks like a 2 task job array, for example:</p> <pre><code>$ LLstat\nLLGrid: txe1 (running slurm-wlm 20.11.3)\nJOBID ARRAY_JOB_ NAME USER START_TIME PARTITION CPUS FEATURES MIN_MEMORY ST NODELIST(REASON)\n9651412_1 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-2\n9651412_2 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-1\n</code></pre> <p>However, you are still running 2*10 = 20 total processes. Triples mode uses slurm to request full nodes, then takes care of launching the processes on each node. This is why you'll always see one task for each node in the <code>LLstat</code> output, rather than each process. One advantage of this is it makes for a more compact <code>LLstat</code> output that is easier to read, instead of having to scroll through tons of tasks.</p> <p>You can check on your individual processes by looking at the log files. LLsub with Triples mode will create a directory with the prefix \"LLSUB\" followed by a unique number to hold all the log files. Within this directory will be one launch log file, which will capture any errors that occur during launch ,and one directory per node and put the log files for each process in its node's directory. These subdirectories are labeled with the process ID range and the node name.</p> <p>For example, here are the log files from a triples run using [2,4,10]:</p> <pre><code>$ ls LLSUB.23004\nREADME.md helpers.py submit_LLsub.sh submit_sbatch.sh top5each.py\n\n$ ls LLSUB.23004/\nllsub-triple-mode-launch.log-10006591 p0-p3_d-19-4-1 p4-p7_d-19-3-4\n\n$ ls LLSUB.23004/p0-p3_d-19-4-1/\nsubmit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7\n\n$ ls LLSUB.23004/p4-p7_d-19-3-4/\nsubmit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7\n</code></pre> <p>In this example, <code>LLSUB.23004</code> is the directory containing the log files, <code>llsub-triple-mode-launch.log-10006591</code> is the log file for the job launch. <code>p0-p3_d-19-4-1</code> and <code>p4-p7_d-19-3-4</code> are the directories for each node, and <code>submit_LLsub.sh.log.0</code>, <code>submit_LLsub.sh.log.1</code>, ..., <code>submit_LLsub.sh.log.7</code> are the log files for each process.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/","title":"Submitting Jobs","text":"<p>For most job types, there are two ways to start the job: using the commands provided by the scheduler, Slurm, or using wrapper command, LLsub, that we have provided. LLsub creates a scheduler command based on the arguments you feed it, and will output that command to show you what it is running. The scheduler commands may provide more flexibility, and the wrapper commands may be easier to use in some cases and are scheduler agnostic. We show some of the more commonly used options. More Slurm options can be seen on the Slurm documentation page, and more LLsub options can be seen by running <code>LLsub -h</code> at the command line.</p> <p>There are two main types of jobs that you can run: interactive and batch jobs. Interactive jobs allow you to run interactively on a compute node in a shell. Batch jobs, on the other hand, are for running a pre-written script or executable. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Batch jobs are the traditional jobs you see on an HPC system and should be used when you want to run a script that doesn't require that you interact with it.</p> <p>On this page we will go over:</p> <ul> <li>How to start an Interactive Job with LLsub</li> <li>How to submit a Basic Serial job with LLsub and sbatch</li> <li>How to request more resources with sbatch</li> <li>How to request more resources with LLsub</li> <li>How to submit an LLMapReduce Job</li> <li>How to submit a job with pMatlab, sbatch, or LaunchFunctionOnGrid</li> <li>How to get the most performance out of LLsub, LLMapReduce, and pMatlab using Triples Mode</li> </ul> <p>You can find examples of several job types in the Teaching Examples github repository. They are also in the <code>bwedx</code>shared group directory and anyone with a Supercloud account can copy them to their home directory and use them as a starting point.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#how-to-start-an-interactive-job-with-llsub","title":"How to start an Interactive Job with LLsub","text":"<p>Interactive jobs allow you to run interactively on a compute node in a shell. Interactive jobs are mainly used for testing, debugging, and interactive data analysis.</p> <p>Starting an interactive job with LLsub is very simple. To request a single core, run at the command line:</p> <p><code>LLsub -i</code></p> <p>As mentioned earlier on this page, when you run an LLsub command, you'll see the Slurm command that is being run in the background when you submit the job. Once your interactive job has started, you'll see the command line prompt has changed. It'll say something like:</p> <p><code>USERNAME@d-14-13-1:~$</code></p> <p>Where <code>USERNAME</code> is your username, and <code>d-14-13-1</code> is the hostname of the machine you are on. This is how you know you are now on a compute node in an interactive job.</p> <p>By default you will be allocated a single CPU core. We have a number of options that allow you to request additional resources. You can always view these options and more by running <code>LLsub -h</code>. We'll go over a few of those here. Note that these can (and often should) be combined.</p> <ul> <li>Full Exclusive Node: Add the word <code>full</code> to request an exclusive     node. No one else will be on the machine with you:</li> </ul> <p><code>LLsub -i full</code></p> <ul> <li>A number of cores: Use the <code>-s</code> option to request a certain     number of CPU cores, or slots. Here, for example, we are requesting     4 cores:</li> </ul> <p><code>LLsub -i -s 4</code></p> <ul> <li>GPUs: Use the <code>-g</code> option to request a GPU. You need to specify     the GPU type and the number of GPUs you want. You can request up to     the number of GPUs on a single node. Refer to the     Systems and Software page to see how     many GPUs are available per node. Remember you may want to also     allocate some number of CPUs in addition to your GPUs. To get 20     CPUs and 1 Volta GPU (half the resources on our Xeon-G6 nodes), you     would run:</li> </ul> <p><code>LLsub -i -s 20 -g volta:1</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-a-simple-serial-batch-job","title":"Submitting a Simple Serial Batch Job","text":"<p>Submitting a batch job to the scheduler is the same for most languages. This starts by writing a submission script. This script should be a bash script (it should start with <code>#!/bin/bash</code>) and contain the command(s) you need to run your code from the command line. It can also contain scheduler flags at the beginning of the script, or load modules or set environment variables you need to run your code.</p> <p>A job submission script for a simple, serial, batch job (for example, running a python script) looks like this:</p> <pre><code>#!/bin/bash\n# Loading the required module\nmodule load anaconda/2023a\n\n# Run the script\npython myScript.py\n</code></pre> <p>The first line is the <code>#!/bin/bash</code> mentioned earlier. It looks like a comment, but it isn't. This tells the machine how to interpret the script, that it is a bash script. Lines 3 and 4 demonstrate how to load a module in a submission script. The final line of the script runs your code. This should be the command you use to run your code from the command line, including any input arguments. This example is running a python script, therefore we have <code>python myScript.py</code>.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-with-llsub","title":"Submitting with LLsub","text":"<p>To submit a simple batch job, you can use the LLsub command:</p> <p><code>LLsub myScript.sh</code></p> <p>Here <code>myScript.sh</code> can be a job submission script, or could be replaced by a compiled executable. The <code>LLsub</code> command, with no arguments, creates a scheduler command with some default options. If your submission script is <code>myScript.sh</code>, your output file will be <code>myScript.sh.log-%j</code>, where <code>%j</code> is a unique numeric identifier, the JobID for your job. The output file is where all the output for your job gets written. Anything that normally is written to the screen when you run your code, including any errors or print statements, will be printed to this file.</p> <p>When you run this command, the scheduler will find available resources to launch your job to. Then <code>myScript.sh</code> will run to completion, and the job will finish when the script is complete.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-with-slurm-scheduler-commands","title":"Submitting with Slurm Scheduler Commands","text":"<p>To submit a simple batch job with the same default behavior as LLsub above, you would run:</p> <p><code>sbatch -o myScript.sh.log-%j myScript.sh</code></p> <p>Here <code>myScript.sh</code> can be a job submission script, or could be replaced by a compiled executable. The <code>-o</code> flag states the name of the file where any output will be written, the <code>%j</code> portion indicates job ID. If you do not include this flag, any output will be written to <code>slurm-JOBID.out</code>, which may make it difficult differentiate between job outputs.</p> <p>You can also incorporate this flag into your job submission script by adding lines starting with <code>#SBATCH</code> followed by the flag right after the first <code>#!/bin/bash</code> line:</p> <pre><code>#!/bin/bash\n# Slurm sbatch options\n#SBATCH -o myScript.sh.log-%j\n# Loading the required module(s)\nmodule load anaconda/2023a\n\n# Run the script\npython myScript.py\n</code></pre> <p>Like <code>#!/bin/bash</code>, these lines starting with <code>#SBATCH</code> look like comments, but they are not. As you add more flags to specify what resources your job needs, it becomes easier to specify them in your submission script, rather than having to type them out at the command line. If you incorporate Slurm flags in your script like this, you can submit it by running:</p> <p><code>sbatch myScript.sh</code></p> <p>When you run these commands, the scheduler will find available resources to launch your job to. Then <code>myScript.sh</code> will run to completion, and the job will finish when the script is complete.</p> <p>Note that when you start adding additional resources you need to make a choice between using <code>LLsub</code> and <code>sbatch</code>. If you have <code>sbatch</code> options in your submission script and submit it with <code>LLsub</code>, <code>LLsub</code> will ignore any additional command line arguments you give it and use those described in the script.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-additional-resources-with-sbatch","title":"Requesting Additional Resources with sbatch","text":"<p>By default you will be allocated a single core for your job. This is fine for testing, but usually you'll want more than that. For example you may want:</p> <ul> <li>Additional cores on multiple nodes (distributed)</li> <li>Additional cores on the same node (shared memory or threading)</li> <li>Multiple independent tasks (job array/throughput)</li> <li>Exclusive node(s)</li> <li>More memory or cores per process/task/worker</li> <li>GPUs</li> </ul> <p>Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want.</p> <p>How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python's multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#additional-cores-on-multiple-nodes","title":"Additional Cores on Multiple Nodes","text":"<p>The flag to request a certain number of cores that can be on more than one node is <code>--ntasks</code>, or <code>-n</code> for short. A task is Slurm's terminology for an individual process or worker. For example, to request 4 tasks you can add the following to your submission script:</p> <p><code>#SBATCH -n 4</code></p> <p>You can control how many nodes these tasks are split onto using the <code>--nodes</code>, or <code>-N</code>. Your tasks will be split evenly across the nodes you request. For example, if I were to have the following in my script:</p> <p><code>#SBATCH -n 4 #SBATCH -N 2</code></p> <p>I would have four tasks on two nodes, two tasks on each node. Specify the number of nodes like this does not ensure that you have exclusive access to those nodes. It will by default allocate one core for each task, so in this case you'd get a total of four cores, two on each node. If you need more than one core for each task, take a look at the cpus-per-task option, and if you need exclusive access to those nodes see the exclusive option.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#additional-cores-on-the-same-node","title":"Additional Cores on the Same Node","text":"<p>There are technically two ways to do this. You can use the same options as requesting tasks on multiple nodes and setting the number of Nodes to 1, say we want four cores:</p> <p><code>#SBATCH -n 4 #SBATCH -N 1</code></p> <p>Or you can use <code>-c</code>, or the <code>--cpus-per-task</code> option by itself:</p> <p><code>#SBATCH -c 4</code></p> <p>As far as the number of cores you get, the result will be the same. You'll get the four cores on a single node. There is a bit of a nuance on how Slurm sees it. The first allocates four tasks all on one node. The second allocates a single task with four CPUs or cores. You don't need to worry too much about this, choose whichever makes the most sense to you.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#job-arrays","title":"Job Arrays","text":"<p>NOTE: We encourage everyone who runs a job array to use LLsub with Triples mode. See the page LLsub Job Array Triples to see how to set this up.</p> <p>A simple way to run the same script or command with different parameters or on different files in parallel is by using a Job Array. With a Job Array, the parallelism happens at the Scheduler level and is completely language agnostic. The best way to use a Job Array is to batch up your parameters so you have a finite number of tasks each running a set of parameters, rather than one task for each parameter. In your submission script you specify numeric indices, corresponding to the number of tasks that you want running at once. Those indices, or Task IDs are captured in environment variables, along with the total number of tasks, and passed into your script. Your script then has the information it needs to split up the work among tasks. This process is described in the Teaching Examples github repository, with examples in Julia and Python.</p> <p>First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you'll first want to enumerate all the combinations of what you are iterating over so you have one big loop. Then you want to add a few lines to your code to take in two arguments, the Task ID and the number of tasks, use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, <code>fnames</code>. In python I would add:</p> <pre><code># Grab the arguments that are passed in\nmy_task_id = int(sys.argv[1])\nnum_tasks = int(sys.argv[2])`\n# Assign indices to this process/task\nmy_fnames = fnames[my_task_id-1:len(fnames):num_tasks]\nfor f in my_fnames: ...\n</code></pre> <p>Notice that I am iterating over <code>my_fnames</code>, which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this).</p> <p>The submission script will look like this:</p> <pre><code>#!/bin/bash\n#SBATCH -o myScript.sh.log-%j-%a\n#SBATCH -a 1-4\n# Loading the required module(s)\nmodule load anaconda/2023a\n\npython top5each.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT\n</code></pre> <p>The <code>-a</code> (or <code>--array</code>) option is where you specify your array indices, or task IDs. Here I am creating an array with four tasks by specifying 1 \"through\" 4. When the scheduler starts your job, it will start up four independent tasks, each will run this script, and each will have <code>#SLURM_ARRAY_TASK_ID</code> set to its task ID. Similarly, <code>$SLURM_ARRAY_TASK_COUNT</code> will be set to the total number of tasks, in this case 4.</p> <p>You may have noticed that there is an additional <code>%a</code> in the output file name. There will be one output file for each task in the array, and the <code>%a</code> puts the task ID on at the end of the filename, so you know which file goes with which task.</p> <p>By default you will get one core for each task in the array. If you need more than one core for each task, take a look at the cpus-per-task option, and if you need to add a GPU to each task, check out the the GPUs section.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#exclusive-nodes","title":"Exclusive Nodes","text":"<p>Requesting an exclusive node ensures that there will be no other users on the node with you. You might want to do this when you know you need to make use of the full node, when you are running performance tests, or when you think your program might affect other users. There is some software that have not been designed for a shared HPC environment, and so use all the cores on the node, whether you have allocated them or not. You can look through their documentation to see if there is a way to limit the number of cores it uses, or you can request an exclusive node. Another situation where you might affect other users is when you don't yet know what resources your code requires. For these first few runs it makes sense to request an exclusive node, and then look at the resources that your job used, and request those resources in the future.</p> <p>To request an exclusive node or nodes, you can add the following option:</p> <p><code>#SBATCH --exclusive</code></p> <p>This will ensure that wherever the tasks in your job land, those nodes will be exclusive. If you have four tasks, for example, specified with either <code>-n</code> (<code>--ntasks</code>) or in a job array, and those four tasks fall on the same node, you will get that one node exclusively. It will not force each task onto its own exclusive node without adding other options.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#adding-more-memory-or-cores-per-task","title":"Adding More Memory or Cores per Task","text":"<p>You can ensure that each task has more than one core or the default amount of memory the same way. By default, each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the Systems and Software page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, with the Xeon-P8 nodes, they have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do this is using the <code>--cpus-per-task</code>, or <code>-c</code> option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I'd want to request five cores for each task:</p> <p><code>#SBATCH -c 5</code></p> <p>This works nicely with both the <code>-n</code> (<code>--ntasks</code>) and <code>-a</code> (<code>--array</code>) options. As the flag name implies, you will get 5 cpu cores for every task in your job. If you are already using the <code>-c</code> option for a shared memory or threaded job, you can either use the <code>-n</code> and <code>-N 1</code> alternative and save <code>-c</code> for adding additional memory, or you can increase what you put for <code>-c</code>. For example, if I know I'm going to use 4 cores in my code, but each will need 20 GB of RAM, I can request a total of 4*5 = 20 cores.</p> <p>How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first in exclusive  mode so your job can have access to the maximum amount of memory. Then you can use the <code>sacct</code> slurm command to get the memory used:</p> <p><code>sacct -j JOBID -o JobID,JobName,State,AllocCPUS,MaxRSS --units=G</code></p> <p>where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job.</p> <p>If the MaxRSS value is larger than the per-slot/core memory limit for the compute node (again, check the Systems and Software page to get this for the resource type you are requesting), you will have to request additional memory for your job.</p> <p>This formatting for the accounting data prints out a number of memory datapoints for the job. They are all described in the sacct man page.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-gpus","title":"Requesting GPUs","text":"<p>Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA.</p> <p>Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, add the following line to your submission script:</p> <p><code>#SBATCH --gres=gpu:volta:1</code></p> <p>This flag will give you a single GPU. For multi-node jobs, it'll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task.</p> <p>Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our Systems and Software page.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-additional-resources-with-llsub","title":"Requesting Additional Resources with LLsub","text":"<p>By default you will be allocated a single core for your job. This is fine for testing, but usually you'll want more than that. For example you may want:</p> <ul> <li>Additional cores on the same node (shared memory or threading)</li> <li>Multiple independent tasks (job array/throughput)</li> <li>More memory or cores per process/task/worker</li> <li>GPUs</li> </ul> <p>Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want.</p> <p>How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python's multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU.</p> <p>If you are submitting your job with LLsub, you should be aware of its behavior. If you have any Slurm options in your submission script (any lines starting with <code>#SBATCH</code>) LLsub will ignore any command line arguments you give it and only use those you specify in your script. You can still submit this script with LLsub, but it won't add any extra command line arguments you pass it.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#additional-cores-on-the-same-node-with-llsub","title":"Additional Cores on the Same Node with LLsub","text":"<p>Libraries that use shared memory or threading to handle parallelism require that all cores be on the same node. In this case you are constrained to the number of cores on a single machine. Check the Systems and Software page to see the number of cores available on the current hardware.</p> <p>To request multiple cores on the same node for your job you can use the <code>-s</code> option in <code>LLsub</code>. This stands for \"slots\". For example, if I am running a job and I'd like to allocate 4 cores to it, I would run:</p> <p><code>LLsub myScript.sh -s 4</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#job-array","title":"Job Array","text":"<p>See LLsub Job Array Triples.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#adding-more-memory-or-cores","title":"Adding More Memory or Cores","text":"<p>If you anticipate that your job will use more than ~4 GB of RAM, you may need to allocate more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. Each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the Systems and Software  page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, the Xeon-P8 nodes have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do with LLsub is the <code>-s</code> (for slots) option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I'd want to request five cores for each task:</p> <p><code>LLsub myScript.sh -s 5</code></p> <p>If you are already using the <code>-s</code> option for a shared memory or threaded job, you should increase what you put for <code>-s</code>. For example, if I know I'm going to use 4 cores in my code, but each will need 20 GB of RAM, I can request a total of 4*5 = 20 cores:</p> <p><code>LLsub myScript.sh -s 20</code></p> <p>How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first (you can request the maximum number of cores per node for this step). Then you can use the <code>sacct</code> slurm command to get the memory used:</p> <p><code>sacct -j JOBID -oJobID,JobName,State,AllocCPUS,MaxRSS --units=G</code></p> <p>where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job.</p> <p>If the MaxRSS value is larger than the per-slot/core memory limit for the compute node (again, check the Systems and Software page to get this for the resource type you are requesting), you will have to request additional memory for your job.</p> <p>This formatting for the accounting data prints out a number of memory data points for the job. They are all described in the sacct man page.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#requesting-gpus-with-llsub","title":"Requesting GPUs with LLsub","text":"<p>Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA.</p> <p>Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, use the following command:</p> <p><code>LLsub myScript.sh -g volta:1</code></p> <p>This flag will give you a single GPU. For multi-node jobs, it'll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task.</p> <p>Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our Systems and Software page. To request 20 cores and 1 GPU, run:</p> <p><code>LLsub myScript.sh -s 20 -g volta:1</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#llmapreduce","title":"LLMapReduce","text":"<p>The LLMapReduce command scans the user-specified input directory and translates each individual file as a computing task for the user-specified application. Then, the computing tasks will be submitted to scheduler for processing. If needed, the results can be post-processed by setting up a user-specified reduce task, which is dependent on the mapping task results. The reduce task will wait until all the results become available.</p> <p>You can view the most up-to-date options for the LLMapReduce command by running the command LLMapReduce -h. You can see examples of how to use LLMapReduce jobs in /usr/local/examples directory on the Supercloud system nodes. Some of these may be in the <code>examples</code> directory in your home directory. You can copy any that are missing from <code>/usr/local/examples</code> to your home directory. We also have an example in the Teaching Examples github repository, with examples in Julia and Python. These examples are also available in the bwedx shared group directory and can be copied to your home directory from there.</p> <p>LLMapReduce can work with any programs and we have a couple of examples for Java, Matlab, Julia, and Python. By default, it cleans up the temporary directory, <code>MAPRED.PID</code>. However, there is an option to keep (<code>--keep=true</code>) the temporary directory if you want it for debugging. The current version also supports a nested LLMapReduce call.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#matlaboctave-tools","title":"Matlab/Octave Tools","text":""},{"location":"using-the-system/submitting-jobs/submitting-jobs/#pmatlab","title":"pMatlab","text":"<p>pMatlab was created at MIT Lincoln Laboratory to provide easy access to parallel computing for engineers and scientists using the MATLAB(R) language. pMatlab provides the interfaces to the communication libraries necessary for distributed computation. In addition to MATLAB(R), pMatlab works seamlessly with Octave, and open source Matlab toolkit.</p> <p>MATLAB(R) is the primary development language used by Laboratory staff, and thus the place to start when developing an infrastructure aimed at removing the traditional hurdles associated with parallel computing. In an effort to develop a tool that will enable the researcher to seamlessly move from desktop (serial) to parallel computing, pMatlab has adopted the use of Global Array Semantics. Global Array Semantics is a parallel programming model in which the programmer views an array as a single global array rather than multiple subarrays located on different processors. The ability to access and manipulate related data distributed across processors as a single array more closely matches the serial programming model than the traditional parallel approach, which requires keeping track of which data resides on any given individual processor.</p> <p>Along with global array semantics, pMatlab uses the message-passing capabilities of MatlabMPI to provide a global array interface to MATLAB(R)) programmers. The ultimate goal of pMatlab is to move beyond basic messaging (and its inherent programming complexity) towards higher level parallel data structures and functions, allowing MATLAB(R)) users to parallelize their existing programs by simply changing and adding a few lines.</p> <p>Any pMatlab code can be run on the MIT Supercloud using standard pMatlab submission commands. The Practical High Performance Computing course on our online course platform provides a very good introduction for how to use pMatlab. There is also an examples directory in your home directory that provides several examples. The Param_Sweep example is a good place to start. There is an in-depth explanation of this example in the Teaching Examples github repository.</p> <p>If you anticipate that your job will use more than 4 GB of RAM, you may need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our xeon-p8 nodes have 48 cores and 192 GB of RAM, therefore each core represents about 4 GB. So if your job needs ~8 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with someone else's job.</p> <p>To do this with pMatlab, you can add the following line to your run script, before you the <code>eval(pRUN(...))</code> command:</p> <p><code>setenv('GRIDMATLAB_MT_SLOTS','2')</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#submitting-with-llsub-or-sbatch","title":"Submitting with LLsub or Sbatch","text":"<p>You can always submit a Matlab(R) script with a submission script through sbatch or LLsub. The basic submission script looks like this:</p> <pre><code>#!/bin/bash\n# Run the script\nmatlab -nodisplay -r \"myScript; exit\"\n</code></pre> <p>Where <code>myScript</code> is the name of the Matlab script that you want to run. When running a Matlab script through a submission script, you do need to specify that Matlab should exit after it runs your code. Otherwise it will continue to run, waiting for you to give it the next command.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#launchfunctionongrid-and-launchparforongrid","title":"LaunchFunctionOnGrid and LaunchParforOnGrid","text":"<p>If you want to launch your serial MATLAB scripts or functions on LLSC systems, you can use the <code>LaunchFunctionOnGrid()</code> function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. Its usage, in Matlab, is as follows:</p> <p><code>launch_status = LaunchFunctionOnGrid(m_file) launch_status = LaunchFunctionOnGrid(m_file,variables)</code></p> <p>Where <code>m_file</code> is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants.</p> <p>If you want to launch your MATLAB scripts or functions that call the <code>parfor()</code> function on LLSC systems, you can use the <code>LaunchParforOnGrid()</code> function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. While <code>LaunchParforOnGrid()</code> will work functionally, it has significant limitations in performance, both at the node level and the cluster level; it might be better to use pMatlab instead. To use the <code>LaunchParforOnGrid()</code> function in MATLAB:</p> <p><code>launch_status = LaunchParforOnGrid(m_file) launch_status = LaunchParforOnGrid(m_file,variables)</code></p> <p>Where <code>m_file</code> is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants.</p> <p>If you anticipate that your job will use more than 4 GB of RAM, you may need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our xeon-p8 nodes have 48 cores and 192 GB of RAM, therefore each core represents about 4 GB. So if your job needs ~8 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with someone else's job.</p> <p>To do this with LaunchFunctionOnGrid or LaunchParforOnGrid, you can add the following line to your run script, before you use the <code>LaunchFunctionOnGrid()</code> or <code>LaunchParforOnGrid()</code> command:</p> <p><code>setenv('GRIDMATLAB_MT_SLOTS','2')</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#triples-mode","title":"Triples Mode","text":"<p>Triples mode is a way to launch pMatlab, LLsub Job Array, and LLMapReduce jobs that gives you better performance and more flexibility to manage memory and threads. Unless you are requesting a small number of cores for your job, we highly encourage you to migrate to this model.</p> <p>With triples mode, you specify the resources for your job by providing 3 parameters:</p> <p><code>[Nodes NPPN NTPP]</code></p> <p>where</p> <ul> <li><code>Nodes</code>is number of compute nodes</li> <li><code>NPPN</code>is number of processes per node</li> <li><code>NTPP</code>is number of threads per process (default is 1)</li> </ul> <p>With triples mode your job will have exclusive use of each of the nodes that you request.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#llsub","title":"LLsub","text":"<p>A brief introduction to LLsub is provided above. To use triples mode to launch LLsub job on Supercloud, run as follows:</p> <p><code>LLsub ./submit.sh [Nodes,NPPN,NTPP]</code></p> <p>A more in-depth guide on how to convert an existing Job Array to an LLsub Triples submission is provided on the page LLsub Job Array.</p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#llmapreduce-with-triples","title":"LLMapReduce with Triples","text":"<p>A brief introduction to LLMapReduce is provided above. To use triples mode to launch your LLMapReduce job on Supercloud, use the <code>--np</code> option with the triple as its parameter, as follows:</p> <p><code>--np=[Nodes,NPPN,NTPP]</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#pmatlab-with-triples","title":"pMatlab with Triples","text":"<p>A brief introduction to pMatlab is provided above. To use triples mode to launch your pMatlab job on Supercloud, you use the pRUN() function. Its usage, in Matlab, is as follows:</p> <p><code>eval(pRUN('mfile', [Nodes NPPN NTPP], 'grid'))</code></p>"},{"location":"using-the-system/submitting-jobs/submitting-jobs/#triples-mode-tuning","title":"Triples Mode Tuning","text":"<p>Triples mode tuning provides greater efficiency by allowing you to better tune your resource requests to your application. This one-time tuning process typically takes ~1 hour:</p> <ol> <li>Instrument your code to print a rate (work/time) giving a sense of     the speed from a ~1 minute run.</li> <li>Determine best number of threads (<code>NTPPBest</code>) by examining rate     from runs with varying numbers of threads:     <code>[1,1,1], [1,1,2], [1,1,4]</code>, ... \u00a0</li> <li>Determine best number of processes per node (<code>NPPNbest</code>) by     examining rate from runs with varying numbers of processes:     <code>[1,1,NTPPBest], [1,2,NTPPBest], [1,4,NTPPBest]</code>, ... \u00a0</li> <li>Determine best number of nodes (<code>NodesBest</code>) by examining rate from     runs of with varying numbers of nodes:     <code>[1,NPPNbest,NTPPBest], [2,NPPNbest,NTPPBest], [4,NPPNbest,NTPPBest]</code>,     ... \u00a0</li> <li>Run your production jobs using <code>[NodesBest,NPPNbest,NTPPBest]</code></li> </ol> <p>You could tune <code>NPPN</code> first, then <code>NTPP</code>. This would be a better approach if you are memory bound. You can find the max <code>NPPN</code> that will fit, then keep increasing <code>NTPP</code> until you stop getting more performance.</p> <p>\"Good\" <code>NPPN</code> values for Xeon-P8: 1, 2, 4, 8, 16, 24, 32, 48</p> <p>\"Good\" <code>NPPN</code> values for Xeon-G6: 1, 2, 4, 8, 16, 20, 32, 40</p> <p>Triples mode tuning results in a ~2x increase efficiency for many users.</p> <p>Once the best settings have been found, they can be reused as long as the code remains roughly similar. Recording the rates from the above process can often result in a publishable IEEE HPEC paper. We are happy to work with you to guide you through this tuning process.</p>"}]}