{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SuperCloud Documentation","text":"<p>The purpose of these pages is to give a quick way to access information about how to use the MIT SuperCloud system.</p> <p>If you are new to SuperCloud, we recommend starting with the Getting Started Tutorial, which walks you through the content most commonly useful when you are first getting to know SuperCloud.</p> <p>To navigate the site use the left sidebar to find pages and the right sidebar to find content within a page. The site is searchable by the search bar on the top right. The Index page groups pages by tags so you can easily find pages that discuss similar topics. Getting Help describes how to best ask for help, if you cannot find the answer to your question on this page or on our Frequently Asked Questions page. Finally, we include a Glossary of words that we use in these pages and what we mean when we say them.</p> <p>Note that SuperCloud has Monthly Downtimes which are scheduled for the Second Tuesday of each month. During downtimes the system is not available. Downtimes usually last about a day and emails are sent when they are complete. We also send out a reminder email a few days before each downtime. If you are not getting these emails, send an email to supercloud@mit.edu and we will verify that you are on our mailing list.</p>"},{"location":"acknowledging-us/","title":"Acknowledging Us","text":"<p>The canonical article about the MIT SuperCloud is:</p> <p>Interactive Supercomputing on 40,000 Cores for Machine Learning and Data Analysis, Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor, Bill Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin, Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Peter Michaleas, paper presented at the 2018 IEEE High Performance Extreme Computing Conference (HPEC), July 2018.</p> <p>When referencing the MIT SuperCloud, please use this reference. The link above is to IEEE Xplore, the paper can also be found on arXiv.org.</p> <p>For convenience, here is a bibtex listing you can copy and paste into your paper:</p> <p><code>@inproceedings{reuther2018interactive,   title={Interactive supercomputing on 40,000 cores for machine learning and data analysis},   author={Reuther, Albert and Kepner, Jeremy and Byun, Chansup and Samsi, Siddharth and Arcand, William and Bestor, David and Bergeron, Bill and Gadepally, Vijay and Houle, Michael and Hubbell, Matthew and Jones, Michael and Klein, Anna and Milechin, Lauren and Mullen, Julia and Prout, Andrew and Rosa, Antonio and Yee, Charles and Michaleas, Peter},   booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)},   pages={1--6},   year={2018},   organization={IEEE}   }</code></p> <p>If you would like to acknowledge the MIT SuperCloud in your paper or report, we recommend the following (be sure to select the applicable resource(s) from among the listed resources that we provide):</p> <p>The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing (HPC, database, consultation) resources that have contributed to the research results reported within this paper/report.</p> <p>Thank you for acknowledging us -- we appreciate it.</p>"},{"location":"code-and-data-location/","title":"Code and data location","text":"<p>In order to run a job on the SuperCloud system, every processor must have access to all functions/methods in the code you are executing and any data the program accesses. Since every grid node can access your SuperCloud home directory, placing your code and data in your home directory\u00a0makes them accessible to the entire grid.</p> <p>You can also run your code from a group shared\u00a0directory\u00a0located on the SuperCloud file system.\u00a0 If you have a large amount of data, or you need to share files with other users, you should use a group shared directory.\u00a0 You can find information on how to request a group shared directory here.</p> <p>In order to transfer your code and data from your local workstation to the SuperCloud, please see the\u00a0Accessing\u00a0Files\u00a0page.</p>"},{"location":"databases/","title":"Databases","text":"<p>The MIT SuperCloud allows users to launch their own databases through the database portal. The portal is located at:</p> <p>https://txe1-portal.mit.edu/db/dbstatus.php</p> <p>This page requires you to authenticate into the portal. From here you will see all the databases you have access to. We currently support Accumulo (1.5.0, 1.6.0, 1.7.0, and 1.8.0) and PostgreSQL databases through our dynamic database capability.</p> <p>To start up a database instance, press \"Start\". You can stop it by clicking on the \"Stop\" button, and can checkpoint a stopped database as well. Clicking on \"View Info\" for Accumulo databases will take you to the Accumulo Monitoring page, where you can view ingest/query plots and current tables.</p> <p>For most of your database uses, you are unlikely to need more information than what the above web portal database interfaces provide. However, in some cases you may need to get more information. For each of your databases, much more instance information is available on the central storage at <code>/home/gridsan/groups/databases/&lt;database-name&gt;/</code>.</p> <p>If you need an instance created and cannot use one of those already available, contact us. Let us know the type of database you need, what you are using it for, what it should be called, and who should have access to the database. Let us know if there are any special configurations you need.</p> <p>There are many ways to insert and query data. While this cannot be done from the portal page, you can query and insert data from any of the nodes on the MIT SuperCloud system. We recommend using D4M, which has been installed and pre-configured to work on our system with very little effort. For more information about databases and how to use them with D4M, take a look at the Advanced Database Technologies course on our online course platform. This course contains a good introduction on how to set up a data pipeline, including parsing, ingesting, and querying data for Accumulo.</p>","tags":["Databases"]},{"location":"datasets/","title":"Datasets","text":"<p>SuperCloud hosts a variety of large datasets that can be valuable for your research. These datasets are shared by program groups and/or organizational groups in order to prevent replication of the data and to allow sharing of data and code.</p> <p>You can find and access the datasets that are available on the SuperCloud system by looking in <code>/home/gridsan/groups/datasets</code>.</p> <p>If your project needs to share data, see our page on Group Shared Directories for directions on requesting a Group Shared Directory and how to use Group Shared Directories safely.</p>","tags":["Datasets"]},{"location":"datasets/#public-datasets","title":"Public Datasets","text":"","tags":["Datasets"]},{"location":"datasets/#datacenter","title":"Datacenter","text":"","tags":["Datasets"]},{"location":"datasets/#mit-supercloud-dataset","title":"MIT SuperCloud Dataset","text":"<p>This dataset consists of the labelled parts of the data described in the paper The MIT SuperCloud Dataset. The archive contains compressed CSV files consisting of monitoring data from the MIT SuperCloud system. For details on the capabilities offered by MIT SuperCloud cluster see Reuther, et. al. IEEE HPEC 2018.</p>","tags":["Datasets"]},{"location":"datasets/#citation","title":"Citation","text":"<p>If you use this data in your work, please cite the following paper</p> <pre><code>@misc{,\n      title={The MIT SuperCloud Dataset},\n      author={Siddharth Samsi and Matthew L Weiss and David Bestor and Baolin Li and Michael Jones and Albert Reuther and\n      Daniel Edelman and William Arcand and Chansup Byun and John Holodnack and Matthew Hubbell and Jeremy Kepner and\n      Anna Klein and Joseph McDonald and Adam Michaleas and Peter Michaleas and Lauren Milechin and Julia Mullen and\n      Charles Yee and Benjamin Price and Andrew Prout and Antonio Rosa and Allan Vanterpool and Lindsey McEvoy and\n      Anson Cheng and Devesh Tiwari and Vijay Gadepally},\n      year={2021},\n      eprint={2108.02037},\n      archivePrefix={arXiv},\n      primaryClass={cs.DC}\n}\n</code></pre>","tags":["Datasets"]},{"location":"datasets/#imagenet-data","title":"ImageNet Data","text":"<p>http://www.image-net.org/challenges/LSVRC/2012/</p> <p>ImageNet Large Scale Visual Recognition Competition 2012 validation, test and training data.</p>","tags":["Datasets"]},{"location":"datasets/#ladi","title":"LADI","text":"<p>https://github.com/LADI-Dataset/</p> <p>A dataset of images collected by the Civil Air Patrol of various disasters. Two key distinctions are the low altitude, oblique perspective of the imagery and disaster-related features.The dataset currently employs a hierarchical labeling scheme of a five coarse categorical and then more specific annotations for each category. The initial dataset focuses on the Atlantic Hurricane and spring flooding seasons since 2015. We also provide annotations produced from the commercial Google Cloud Vision service and open source Places365 benchmark.</p>","tags":["Datasets"]},{"location":"datasets/#microsoft-coco","title":"Microsoft COCO","text":"<p>http://cocodataset.org</p> <p>COCO is a large-scale object detection, segmentation, and captioning dataset, with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context.</p>","tags":["Datasets"]},{"location":"disk-space/","title":"SuperCloud Disk Space Limits","text":"<p>The SuperCloud has imposed guardrails for disk space and number of files on both home and group directories for these reasons:</p> <ul> <li>To make sure no one unintentionally uses up a lot of storage</li> <li>So we can have a conversation on what your data needs are when your     requirements start to grow</li> <li>Large numbers of files have a significant impact on the overall performance of the filesystem</li> </ul> <p>The current guardrails are listed when you run the command to list your disk usage (see below). If you need to have these guardrails relaxed temporarily, email us at supercloud@mit.edu so we can discuss your needs.</p> <p>It is recommended that you not use your SuperCloud home directory as primary storage. You should transfer data from your local workstation to your SuperCloud home directory before processing, and transfer the results back for analysis.</p>","tags":["Filesystem","Tips and Best Practices"]},{"location":"disk-space/#checking-your-disk-usage","title":"Checking your Disk Usage","text":"<p>You can check your disk usage by going to the Profile page on the Web Portal. The bottom of this page lists the disk usage in your home directory and in any group directories you may be a part of.</p> <p>You can also use this command to check the disk usage in your home directory:</p> <pre><code>lfs quota -gh $USER /home\n</code></pre> <p>We recognize that during the prototyping and data analysis phases, a lot of intermediate results may be generated and which can accumulate. We ask you to be judicial in your storage use and to periodically do data \"spring cleaning\"!s</p> <p>Please also see our tips for Improving File System Performance.</p> <p>We do not backup files on SuperCloud. We strongly recommend that you regularly move files back to your local workstation for backup.</p>","tags":["Filesystem","Tips and Best Practices"]},{"location":"faqs/","title":"Frequently Asked Questions","text":"<p>This page contains the answers to a few questions that we receive often. As more questions are asked, this page may be updated.</p>"},{"location":"faqs/#how-do-i-get-an-account","title":"How do I get an account?","text":"<p>To request an account, follow the instructions and answer the questions on our Account Request page. We will reach out to you once your account is created, or if we have any questions for you.</p>"},{"location":"faqs/#i-would-like-to-log-in-from-a-new-computer-can-i-add-a-new-ssh-key","title":"I would like to log in from a new computer. Can I add a new ssh key?","text":"<p>If you have a new computer, or want to add keys for additional computers that you use, you can add your own key on our web portal. Instructions on how to generate a new ssh key and add it to your account are on our Account Request page. In summary, log in with your credentials (for MIT and other educational institutions this is the middle option when you go to https://txe1-portal.mit.edu) and then click on the \"sshkeys\" link. Scroll to the bottom and paste your key in the box.</p>"},{"location":"faqs/#how-much-storage-do-i-have-for-my-account","title":"How much storage do I have for my account?","text":"<p>We have set some guardrails on home and group directory storage. Home directories are set to 10TB and group directories are set to 50TB. It is recommended that users not use their accounts as primary storage. Further, we do not back up the storage on the system, so we strongly recommend transferring your code, data, and any other important files to another machine for backup.</p>"},{"location":"faqs/#how-can-i-share-filescodedata-with-my-colleagues","title":"How can I share files/code/data with my colleagues?","text":"<p>If you would like to share files with others you can request a shared group directory. Shared group directories are located at /home/gridsan/groups and we will put a symlink in your home directory to use as a shortcut to your shared group directory. To request one, send email to supercloud@mit.edu and let us know:</p> <ol> <li>What the group should be called. Short, descriptive names are best.</li> <li>Who should be the owner/approver for the group. We will ask this     person for approval whenever we receive a request to join a group.</li> <li>Who should be in the group. SuperCloud usernames are helpful, but     not required.</li> <li>Whether you plan to store any non-public data in the group. If you     do, let us know what requirements, restrictions, or agreements are     associated with the data. See why we ask     here.</li> </ol> <p>To learn more about Shared Groups and best practices using them, see the page on Shared Groups.</p>"},{"location":"faqs/#how-do-i-setchange-my-password","title":"How do I set/change my password?","text":"<p>We do not use passwords on SuperCloud. If you have an active MIT Kerberos or login from another university, you can log in using your institution's credentials. On the SuperCloud Web Portal Login page, select the middle option \"MIT Touchstone/InCommon Federation\". You may have to select your institution from the dropdown list, which should take you to your institution's login page. After you log in, you should see the Portal main page. If you have trouble logging in this way, please contact us and we can help.</p> <p>If you cannot log in using \"MIT Touchstone/InCommon Federation\" or one of the other options on the portal, we may set you up with a password. If you have not yet reset your password, or remember your previous password, then follow the instructions on the Web Portal page. If you have previously set your password and cannot remember it, contact us and we will help you reset your password.</p>"},{"location":"faqs/#are-there-any-resource-limits","title":"Are there any resource limits?","text":"<p>New accounts are created with a small starting resource allocation. Once you have completed the Practical HPC course you can send your certificate  to supercloud@mit.edu to request to be moved to the standard allocation. The starting and standard allocations are listed on the Systems and Software page.</p> <p>If you have a deadline and need additional resources you can request more by contacting us. If you looking to request more GPUs, please read through this page on Optimizing your GPU Jobs first. Please state the number of additional processors you need, the length of time for which you need it, and tell us about the jobs you are running and how you are submitting them. If you plan to run many independent jobs we will ask you to convert your job to use triples before giving and increased allocation. Remember this is a shared system, so during busy times we may not be able to grant your request. We will also only grant increase requests if you have completed the Practical HPC course.</p> <p>It is also important to keep in mind what your fair share of memory is for each process and request additional resources if needed. For example, if there are 40 cores and 384GB of RAM on the machine you are using, each processor's fair share would be about 9GB. Check the Systems and Software page to see how many cores and how much memory each node type has. If you think your processes will go over this, request additional slots as needed. This ensures you have sufficient memory without killing your job.</p>"},{"location":"faqs/#what-do-i-do-if-my-job-wont-be-deleted","title":"What do I do if my job won't be deleted?","text":"<p>Occasionally this will happen if the node where your job is running goes down, or your job does not exit gracefully. If this happens, contact us with the Job ID, and we'll delete the job and reboot the node if needed.</p>"},{"location":"faqs/#why-do-i-get-an-error-when-i-try-to-install-a-package","title":"Why do I get an error when I try to install a package?","text":"<p>There are two common reasons you get an error when you try to install a package. If you get a \"Permission Denied\" or similar error, it is because you are trying to install the package system-wide, rather than your own home directory. See the Software and Package Management page for more information on how to install packages.</p> <p>If you get a \"Network Error\", or similar, this is because we don't have internet/network connection on the compute nodes, this includes Jupyter and any interactive jobs. You will have to install the package on one of the login nodes.</p> <p>If you get an error like <code>Could not install packages due to an EnvironmentError: [Errno 122] Disk quota exceeded</code> when installing a package with pip or something like <code>ERROR: could not download https://pkg.julialang.org/registry/...</code> installing a package with Julia, even though you are on the login node, this is because it is filling up your quota in the <code>/tmp</code> directory. We have set quotas on this directory to prevent a single person from inadvertently filling it up, as when this happens it can cause issues for everyone using the node, including preventing anyone from installing packages. This can be fixed by setting the <code>TMPIDR</code> environment variable like so:</p> <pre><code>mkdir /state/partition1/user/$USER\nexport TMPDIR=/state/partition1/user/$USER\n</code></pre> <p>After you have installed your package you can clean up any lingering files by removing the temporary directory you have created:</p> <p><code>rm -rf /state/partition1/user/$USER</code></p>"},{"location":"faqs/#how-can-i-set-up-vscode-to-edit-files-remotely-on-supercloud","title":"How can I set up VSCode to edit files remotely on SuperCloud?","text":"<p>You can use VSCode to remotely connect to SuperCloud via the Remote-SSH extension. The default settings in the VSCode Remote - SSH extension will fail to connect. This is due to it trying to lock files in your home directory, which is disabled for performance reasons.</p> <p>The solution is to have it use the local filesystem. To get it to work, go to your VS Code settings, click \"Extensions\" and then \"Remote - SSH\". Once you're in the settings for Remote - SSH, check the box next to \"Remote.SSH: Lockfiles in Tmp\". What this will do is put any lockfiles in /tmp, rather than your home directory.</p> <p>A side note: we have seen VS Code clutter up /tmp in the past, which we keep fairly small. Disconnecting occasionally should clean these up, however we do not know for sure. If you can check it once in a while and clean up any files that are yours in /tmp, that would be really helpful.</p>"},{"location":"faqs/#how-can-i-use-tensorboard-on-supercloud","title":"How can I use Tensorboard on SuperCloud?","text":"<p>Take a look at this page on how to run Tensorboard in an interactive job.</p>"},{"location":"faqs/#i-got-an-out-of-memory-error-how-can-i-figure-out-how-much-memory-my-job-needs-and-request-more","title":"I got an Out of Memory error. How can I figure out how much memory my job needs and request more?","text":"<p>This is described on the Submitting Jobs page. If you submit your jobs with <code>sbatch</code>, check out this section, and if you use <code>LLsub</code> take a look at this section. As described in those links, you can check how much memory your job used using the <code>sacct</code> command, then request enough additional cores for the memory you need. Keep in mind that if your job was killed due to high memory use, your job may not have gotten to the point of highest memory use. To get an accurate measurement you can run your job on an exclusive node long enough to reach the part of the job that would consume the most memory, then stop the job and check the memory use with <code>sacct</code>.</p>"},{"location":"faqs/#my-pythonjulia-job-is-running-but-i-dont-see-any-output-in-the-log-files-what-is-going-on","title":"My Python/Julia job is running, but I don't see any output in the log files. What is going on?","text":"<p>Julia and Python will buffer output in batch jobs. This means they will hold on to the output and print it out all at once, sometimes this isn't until the end of a loop or the end of the program. You can force both to print the output when it is produced. In Python you can do this by using the <code>-u</code> flag when you call Python in your submission script (ex: <code>python -u myscript.py</code>). In Julia you can do this by adding <code>flush(stdout)</code> after the print statements in your Julia script that you'd like to print immediately (ex: <code>println(\"Hello World!\"); flush(stdout)</code>).</p>"},{"location":"faqs/#what-does-the-underutilizingoversubscribing-the-node-warning-message-mean","title":"What does the Underutilizing/Oversubscribing the node warning message mean?","text":"<p>When you launch a job in triples mode, the second and third numbers you provide are the number of processes per node (NPPN) and the number of threads per process (NT). You can multiply these two numbers to get the total number of threads you will have running on the node. You will usually get the best performance if you have the same total number of threads as cores on the node. See the Systems and Software page for a list of how many cores are on each node type.</p> <p>Oversubscribing: When you have more threads than the number of cores on the node. Oversubscribing can overwhelm the node, which can slow your job down or even cause it to fail. It can also be harmful for the node. Reduce the number of threads per process or number of processes per node so that the total number of threads is less than or equal to the number of cores.</p> <p>Underutilizing: When you have fewer threads than the number of cores on the node you may be undersubscribing. This means you may not be taking full advantage of the node. Many underlying packages and libraries can take advantage of multithreading. You might try increasing the number of threads per process, while avoiding oversubscribing, to see if you get improved performance.</p> <p>For more information on how to pick the best triple for your job, take a look at the tuning process and recommendations.</p>"},{"location":"faqs/#how-do-i-fix-the-glibcxx_3429-not-found-error-when-using-an-anaconda-module","title":"How do I fix the GLIBCXX_3.4.29 not found error when using an Anaconda module?","text":"<p>You may encounter the following error when using anaconda/2023a.</p> <pre><code>ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a/lib/python3.9/site-packages/pandas/_libs/window/aggregations.cpython-39-x86_64-linux-gnu.so)\n</code></pre> <p>The libstdc++ library containing the appropriate GLIBCXX version is found in the lib directory of the anaconda installation. Anaconda/2023a is installed at /state/partition1/llgrid/pkg/anaconda/anacond3-2023a. The anaconda library path has to be set in the <code>LD_LIBRARY_PATH</code> environment variable.</p> <p>To set the <code>LD_LIBRARY_PATH</code> run:</p> <pre><code>export LD_LIBRARY_PATH=/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a/lib:$LD_LIBRARY_PATH\n</code></pre> <p>If you are running a batch job, you can add this line to your job submission script. For interactive jobs, run this line before starting your application. For Jupyter, add this line to your <code>llsc_notebook_bashrc</code> file (see how here).</p> <p>The above error may occur in other anaconda modules, you can find the anaconda installation directory by running the command which python, and set <code>LD_LIBRARY_PATH</code> variable accordingly.</p>"},{"location":"faqs/#how-can-i-get-more-help","title":"How can I get more help?","text":"<p>If you have a question that is not answered here, send email to supercloud@mit.edu for more help.</p>"},{"location":"file-locking/","title":"File Locking","text":"<p>The SuperCloud Lustre network file system (home directories and shared directories reside on the network file system) do not support file locking.</p> <p>In general, there are 2 ways to fix this problem:</p> <ul> <li>Disable file locking by setting an environment variable that the package uses</li> <li>Have your code or the package use the local disk on the compute node, where file locking is permitted</li> </ul> <p>On this page, we'll provide instructions on how to fix this problem for various applications. Please email us at supercloud@mit.edu if you encounter a file locking issue with an application that isn't included here.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#hdf5","title":"HDF5","text":"<p>Here is an example error message that you might see from HDF5 when it can't lock a file:</p> <p><code>IOError: Unable to create file (file locking disabled on this file system (use HDF5_USE_FILE_LOCKING environment variable to override), errno = 38, error message = 'Function not implemented')</code></p> <p>You can disable file locking in HDF5 by setting the <code>HDF5_USE_FILE_LOCKING</code> environment variable to false. This variable can be set in various places.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#in-your-bashrc-or-bash_profile","title":"In your .bashrc or .bash_profile","text":"<p>You can disable file locking by setting the <code>HDF5_USE_FILE_LOCKING</code> environment variable. To disable file locking, add this line to your <code>~/.bashrc</code> or <code>~/.bash_profile</code> file:</p> <p><code>export HDF5_USE_FILE_LOCKING='FALSE'</code></p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#jupyter-notebook-jobs","title":"Jupyter Notebook Jobs","text":"<p>If you are running a Jupyter Notebook, you can add the line below to the file <code>~/.jupyter/llsc_notebook_bashrc</code> (you'll have to create the file if it isn't there). This file is loaded at the start of Jupyter jobs, much like a bashrc file when you log into the terminal.</p> <p><code>export HDF5_USE_FILE_LOCKING=FALSE</code></p> <p>For more information about using environment variables in Jupyter Notebooks, see the note on our Jupyter Notebooks page.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#python-jobs","title":"Python Jobs","text":"<p>If you are running python code, you can set the environment variable at the beginning of your python code:</p> <pre><code>import os\nos.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n</code></pre>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#use-the-local-disk","title":"Use the Local Disk","text":"<p>Each of the compute nodes contains a local disk where file locking is permitted. You can use the <code>$TMPDIR</code> area or <code>/state/partition1/user/&lt;username&gt;</code> area for files that need file locking capability.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#tmpdir","title":"$TMPDIR","text":"<p>The <code>$TMPDIR</code> environment variable points to a temporary directory on the local disk of the compute node. Note that <code>$TMPDIR</code> is created by the scheduler and points to a temporary directory that will not exist after the job completes.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#statepartition1","title":"/state/partition1","text":"<p>If you would like your files to persist after the job completes, you can create your own subdirectory in the <code>/state/partition1/user</code> area on the local disk. However, since the <code>/state/partition1</code> directory is on the local disk of each compute node (each compute node will have different files in its <code>/state/partition1</code> directory), as a final step of your job, you may want to copy the files from your <code>/state/partition1</code> subdirectory to a shared directory, or to your home directory.</p> <p>If you use the <code>/state/partition1</code> directory for your files, your code should create the directory <code>/state/partition1/user/$USER</code> and create any desired subdirectories within that directory. Please do not write your files in the <code>/state/partition1</code> directory itself - create a subdirectory with your username and save your files there.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"file-locking/#hugging-face","title":"Hugging Face","text":"<p>You can direct Hugging Face to use local storage for files that need file locking capability by setting the <code>HF_HOME</code> environment variable to point to the local disk. This variable can be set in various places:</p> <ul> <li> <p>In <code>~/.bashrc</code> or <code>~/.bash_profile</code> file - the variable will always be     set (this is a \"set and forget\" approach). You should also create     the directory:</p> <pre><code>export HF_HOME=/state/partition1/user/$(whoami)/hf\nmkdir -p $(HF_HOME)\n</code></pre> </li> <li> <p>In <code>~/.jupyter/llsc_notebook_bashrc</code> - the variable will always be     set when your run a Jupyter Notebook. You can add the <code>export</code> and <code>mkdir</code> statements from the previous bullet     to the file <code>~/.jupyter/llsc_notebook_bashrc</code> (you'll have to create     the file if it isn't there). This file is loaded at the start of     Jupyter jobs, much like a <code>.bashrc</code> file when you log into the     terminal. For more information about using environment variables in Jupyter     Notebooks, see the note on our Jupyter Notebooks     page.</p> </li> <li> <p>At the beginning of your python code - the variable would be set     only when your python code is running:</p> <pre><code>import os\nusername = os.getenv(\"USER\")\nos.environ[\"HF_HOME\"] = \"/state/partition1/user/\"+ username + \"/hf\"\nos.makedirs(os.getenv(\"HF_HOME\"))\n</code></pre> </li> </ul> <p>We have an example for downloading the Hugging Face data to the local disk of the login node, where file locking is enabled, then copy it back to your home directory. At the start of each job, you'd then copy it to the local disk of the compute node you are on. You can the example by clicking here.</p> <p>The <code>run.sh</code> script loads the model/dataset, and then launches the job, <code>batch_bert_v0.sh</code>. The relevant lines are 9-14, 24, 25 in <code>run.sh</code>, and 28-32 in <code>batch_bert_v0.sh</code>.</p>","tags":["Filesystem","Troubleshooting","Tips and Best Practices"]},{"location":"filesystem/","title":"Best Practices for Using the Filesystem","text":""},{"location":"filesystem/#installing-python-packages","title":"Installing Python Packages","text":"<ul> <li>Use our anaconda modules whenever possible. The newest anaconda     module should have the most up to date versions. Our anaconda     modules are on the local disk of all the nodes and so does not     affect the shared filesystem.</li> <li>If you need to install additional Python packages, install with pip     using the <code>--user</code> flag as described in our     documentation. Python     will then only go to your home directory for these installed     packages, and so should be less load on the shared filesystem. Only     use conda environments as a last resort, as this puts ALL packages     you use in your home directory, and creates many small files.</li> <li>DO NOT install anaconda or miniconda in your home directory. There     is no reason to do this and will slow your SuperCloud experience     down significantly, as these installations contain many, many small     files. If you absolutely need to use conda to install a package     create a conda environment using our anaconda modules. If you have     previously installed anaconda or miniconda in your home directory,     delete it now.</li> </ul>"},{"location":"filesystem/#submitting-jobs","title":"Submitting Jobs","text":"<ul> <li>Use Triples Mode for submitting Job Arrays and LLMapReduce jobs. These create fewer log files and group them by node, reducing the number of files per directory. It is also lighter weight on the scheduler, as it creates fewer tasks/jobs that the scheduler has to keep track of.</li> <li>DO NOT create very large Job Arrays. Each task in a Job Array     creates a log file, the more tasks in your array, the more files.     The best practice is to use Triples to submit your job arrays (see     bullet above).</li> <li>DO NOT submit many small jobs, most likely a Job Array or     LLMapReduce with Triples would be appropriate.</li> <li>Avoid doing things that actively stress the filesystem, for example     checking whether a file exists repeatedly over a long period of time     or across many tasks.</li> </ul>"},{"location":"filesystem/#file-organization","title":"File Organization","text":"<ul> <li>Aim for less than ~1000 files per directory.</li> <li>Fewer, larger files are better than many small files (file size     should be a minimum of 1MB, target ~100MB).</li> <li>Within a job, you can use $TMPDIR for temporary or intermediate     files you don't need after the job. $TMPDIR points to a temporary     directory on the local filesystem that is set up at the start of     your job and removed at the end of your job. If your job requires a     lot of I/O you may see significant performance improvement by     copying the files you need to $TMPDIR at the start of your job and     copy any new files you need to your home directory at the end of     your job.</li> <li>Use shared directories to share data among team members rather than having a     separate copy in everyone's home directory.</li> <li>Check /home/gridsan/groups/datasets before downloading large public     datasets. If there is a public dataset that you are considering     downloading that you think others may also want to use, send us a     email to supercloud@mit.edu to suggest that we add it.<ul> <li>If you are using ImageNet, we have a special setup that will stress the filesystem significantly less and should be much faster. First load its modulefile: <code>module load /home/gridsan/groups/datasets/ImageNet/modulefile</code>. This will set up the <code>$IMAGENET_PATH</code> environment variable, which you can use in your code to point to ImageNet.</li> </ul> </li> </ul>"},{"location":"finding-pmatlab-output/","title":"Finding pMatlab Output","text":"<p>When you run a pMatlab job, the standard output of each process is captured in <code>out</code> files.\u00a0The <code>.out</code> filenames include a number indicating the process id that created the file: <code>&lt;pMatlab_script_name&gt;.&lt;pid&gt;.out</code> where:</p> <ul> <li><code>&lt;Matlab_script_name&gt;</code>\u00a0is the filename of your pMatlab script</li> <li><code>&lt;pid&gt;</code> is the id of the process</li> </ul> <p>Depending on how you submit your job the location of the files will be in either of these 2 locations:</p> <ul> <li>The <code>./MatMPI</code> directory (which is created in your working directory)</li> <li>In subdirectories within the <code>MatMPI</code> directory:<ul> <li><code>./MatMPI/p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;compute node name&gt;</code> where:<ul> <li><code>p&lt;start-pid&gt;</code> is the id of the first process running on the compute node whose name is <code>&lt;compute node name&gt;</code></li> <li><code>p&lt;end-pid&gt;</code> is the id of the last process running on the     compute node whose name is <code>&lt;compute node name&gt;</code></li> </ul> </li> <li>If your job ran on multiple compute nodes, the log files will be spread across multiple subdirectories, each one containing the compute node name in the subdirectory's name.</li> </ul> </li> </ul> <p>The name of the output files will be <code>&lt;pMatlab_script_name&gt;.&lt;pid&gt;.out</code>.</p>","tags":["pMatlab"]},{"location":"finding-pmatlab-output/#where-will-the-out-files-reside","title":"Where will the .out files reside?","text":"","tags":["pMatlab"]},{"location":"finding-pmatlab-output/#default-location","title":"Default location","text":"<p>In most cases, the\u00a0<code>.out</code> files reside in the subdirectories that include the process id and compute node name within the\u00a0<code>MatMPI</code>\u00a0directory:</p> <p><code>./MatMPI/p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;compute node name&gt;/&lt;pMatlab_script_name&gt;.&lt;pid&gt;.out</code></p> <p>For example, this is the path and file name for a job named param_sweep_parallel_v2 that ran on 4 processes on compute node b-11-16-3:</p> <p><code>./MatMPI/p0-p3_b-11-16-3/param_sweep_parallel_v2.1.out</code></p>","tags":["pMatlab"]},{"location":"finding-pmatlab-output/#if-you-set-gridmatlab_manycore-to-no","title":"If you set GRIDMATLAB_MANYCORE to 'NO'","text":"<p>If you set the <code>GRIDMATLAB_MANYCORE</code> environment variable to 'no' (the default setting is 'yes'), your output files will be located in the\u00a0<code>MatMPI</code> directory:</p> <p><code>./MatMPI/&lt;pMatlab_script_name&gt;.&lt;pid&gt;.out</code></p> <p>For example:</p> <p><code>./MatMPI/param_sweep_parallel_v2.1.out</code></p>","tags":["pMatlab"]},{"location":"finding-pmatlab-output/#the-other-files-in-matmpi","title":"The other files in ./MatMPI","text":"","tags":["pMatlab"]},{"location":"finding-pmatlab-output/#prun_parallel_wrappererr","title":"pRUN_Parallel_Wrapper.err","text":"<p>The file pRUN_Parallel_Wrapper.err\u00a0will be created in the <code>MatMPI</code> directory. This file contains all standard error output from your pMatlab job. Each line in the file is prefixed with the Pid where the error occurred which you can use to identify the\u00a0<code>.out</code>\u00a0file and investigate further. You'll find the same error message in the associated\u00a0<code>.out</code>\u00a0file.</p>","tags":["pMatlab"]},{"location":"finding-pmatlab-output/#unix_commandssh","title":"Unix_Commands*.sh","text":"<p>The Unix_Commands*.sh files are the shell scripts that were used to launch your pMatlab job. This may be useful to the SuperCloud team to help diagnose any problems you may run into when you launch your job or during runtime.</p> <p>If you set the <code>GRIDMATLAB_MANYCORE</code> environment variable to 'no' (the default setting is 'yes'), gridMatlab generates one Unix_Commands..out file for each process in your job. This could result in thousands of files and cause the Lustre filesystem to perform poorly. To try to prevent this, these files are automatically deleted after job completion unless you set the\u00a0<code>GRIDMATLAB_KEEP_TEMP_FILES</code> environment variable as shown below: <pre><code>setenv('GRIDMATLAB_KEEP_TEMP_FILES','ON');\n</code></pre>","tags":["pMatlab"]},{"location":"fixing-windows-scripts/","title":"Fixing Scripts Written in Windows","text":"<p>Do you get obscure errors like \"\\r: command not found\" or \"/bin/bash^M: bad interpreter\" when trying to run your scripts on an SuperCloud system? If so, the most likely explanation is that the script file was created in an editor on Windows and then copied to the SuperCloud.</p> <p>In this example, the file <code>test.sh</code> was created on a Windows system and copied to SuperCloud.</p> test.sh<pre><code>#!/bin/bash\necho \"test.sh was created on Windows\"\n</code></pre> <p>When we try to run this script, we get an error:</p> <pre><code>$ ./test.sh\n-bash: ./test.sh: /bin/bash^M: bad interpreter: No such file or directory\n</code></pre> <p>Text files created on DOS/Windows machines have different line endings than files created on Linux (which is the OS that the SuperCloud system runs). DOS uses carriage return and line feed (<code>\\r\\n</code>) as a line ending, while Linux uses just a line feed (<code>\\n</code>). You need to be careful when transferring files between Windows machines and the SuperCloud system to make sure the line endings are translated properly.</p> <p>From the the login node of a SuperCloud system, you can see what line endings a text file contains by running the <code>cat -vet</code> command on the file:</p> <pre><code>$ cat -vet test.sh\n#!/bin/bash^M$\necho \"test.sh was created on Windows\"^M$\n</code></pre> <p>If you see <code>^M$</code> at the end of each line, then this file will not run on a Linux system. If you see just <code>$</code> at the end of each line, then it is properly formatted to run on a Linux system.</p> <p>There's a simple way to convert your DOS formatted file to Linux format - just run the command <code>dos2unix</code> on the file:</p> <pre><code>$ dos2unix test.sh\ndos2unix: converting file test.sh to Unix format...\n</code></pre> <p>Verify that the line endings are Linux line endings:</p> <pre><code>$ cat -vet test.sh\n#!/bin/bash$\necho \"test.sh was created on Windows\"$\n</code></pre> <p>Try running the script again:</p> <pre><code>$ ./test.sh\ntest.sh was created on Windows\n</code></pre>","tags":["Windows","Linux","Troubleshooting","Getting Started","Tips and Best Practices"]},{"location":"getting-help/","title":"Getting Help","text":""},{"location":"getting-help/#additional-documentation","title":"Additional Documentation","text":"<p>If you haven't found your answer elsewhere in this wiki, you may find it in one of these places:</p> <ul> <li>Frequently Asked Questions page answers a number of frequently asked questions</li> <li>The Getting Started page contains articles to help get you started with using SuperCloud</li> <li>You can use the Search box at the top of any page to search our entire website</li> <li>If none of these resources are answering your question, please send us email</li> </ul>"},{"location":"getting-help/#quick-reference-guide","title":"Quick Reference Guide","text":"<p>You can find downloadable two-page quick reference guides for SuperCloud here.</p>"},{"location":"getting-help/#llx-online-courses","title":"LLx Online Courses","text":"<p>You can find helpful online courses about using the LLSC on the LLx Online Course platform.</p>"},{"location":"getting-help/#practical-hpc-course","title":"Practical HPC Course","text":"<p>The Practical HPC course is an introductory course that all SuperCloud users should take. The course:</p> <ul> <li>Includes an introduction to HPC, canonical HPC Workflows, and the SuperCloud system.</li> <li>Walks you through setting up your account, installing software, running your first test job, submitting your first batch job.</li> <li>Describes how to scale up efficiently and measure your performance.</li> </ul>"},{"location":"getting-help/#email","title":"Email","text":"<p>If none of these resources are answering your question, please contact us at supercloud@mit.edu. This mailing list includes the entire team, so the best available person to answer your question will respond. Sending email to the entire team will also likely get you the fastest response.</p> <p>In this email, please provide, where applicable:</p> <ul> <li>The Operating System you are using (for ssh key troubleshooting)</li> <li>Description of your issue or request</li> <li>The command that you used to launch your job</li> <li>Job ID(s)</li> <li>What you tried</li> <li>The full error message you are receiving</li> <li>Any supporting files (code, submission scripts, screenshots, etc)</li> </ul> <p>Please also copy supercloud@mit.edu in all of your correspondence with us. Including supercloud@mit.edu in all of your emails to us keeps the entire team in the loop with regard to your questions and issues.</p>"},{"location":"getting-help/#office-hours","title":"Office Hours","text":"<p>We also host weekly office hours. A reminder email is sent out weekly with the exact location and time. If you would like to attend office hours and don't think you are getting these reminder emails, please email\u00a0supercloud@mit.edu.</p> <p>SuperCloud Office hours are also listed on the ORCD webpage: https://orcd.mit.edu/news-and-events/office-hours. The office hours are listed under:</p> <ul> <li>HPC Help Office Hours</li> <li>HPC Help Virtual Office Hours</li> </ul>"},{"location":"getting-started/","title":"Getting Started Tutorial","text":"<p>This page contains the most common steps for setting up and getting started with your SuperCloud account. We provide this page as a convenient reference to get started. To learn how to use your SuperCloud account, complete the Practical HPC course, which contains the material below and more. The course will walk you through these steps in more detail and often with videos to see how it is done. The course is self paced, can be accessed anytime, and is kept up to date. More reference material is available throughout site (some of this material links to those pages).</p> <p>When your account is first created you will have a small startup allocation. Upon completing and earning a certificate for the Practical HPC course (requires a grade of 70% or better on the graded assignments), you can update your resource allocation to the standard allocation via the User Profile page on the Web Portal. In order to update your allocation, you will need the certificate ID number located at the bottom of the page of your certificate of course completion. Copy and paste the certificate ID number to the text box in the \u201cUser Resource Limits\u201d section. Complete the update by clicking \u201cSubmit\u201d. A successful update will display the message \u201cCertificate verified. Please allow 5 to 10 minutes for your SLURM limits to revert to the standard default limits.\u201d Please wait 5-10 minutes before refreshing the page. If you run into trouble with this process send an email to supercloud@mit.edu. Resource allocations are listed on the User Profile page and on the Systems and Software page.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#logging-in-via-ssh","title":"Logging in Via ssh","text":"<p>The first thing you should do when you get a new account is verify that you can log in. The primary way to access the MIT SuperCloud system is through ssh. Instructions for different operating systems are below. Keep in mind that you will only be able to access the system from ssh from the machine where you generated your ssh key. You will not be able to log in until we have sent you an email stating that we have created your account, which will contain your username.</p> <p>First, add your ssh key to the web portal. Go to\u00a0https://txe1-portal.mit.edu/. If you affiliated with MIT or another institution/university select the middle option \"MIT Touchstone/InCommon Federation\" to log in. Select your institution from the dropdown (be aware they are spelled out, MIT is listed as Massachusetts Institute of Technology, for example), click \"Remember my choice\" box and then the \"Select\" button. Then log in with your institution credentials. Once you are logged in, click on the \"sshkeys\" link and paste your ssh key in the box at the bottom of the page and click \"Update Keys\".</p> <p>For instructions on how to generate ssh keys, retrieve your public key, and additional troubleshooting tips, watch the videos in the \"Account Setup and SSH Keys\" section in the \"Getting Started\" module of the Practical HPC course, or see this page.</p> <p>First open a command line terminal window where you generated your ssh keys. Enter the following command, where <code>USERNAME</code> is your username on the MIT SuperCloud system:</p> <p><code>ssh USERNAME@txe1-login.mit.edu</code></p> <p>If you generated your keys using PuTTY, open a PuTTY window. In the box labeled \"Host Name\" enter<code>USERNAME@txe1-login.mit.edu</code>, where<code>USERNAME</code>is your username on the MIT SuperCloud system. Keep the ssh box checked (this should be default) and Port should be set to 22. Click \"Open\" to start the session. You may also need to indicate your private key on the Connection -&gt; SSH -&gt; Auth page.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#shared-hpc-clusters","title":"Shared HPC Clusters","text":"<p>The MIT SuperCloud is an HPC-style shared cluster. You are sharing this resources with a number of other researchers, staff, and students so it is important that you read this page and use the system as intended.</p> <p>Being a cluster, there are several machines connected together with a network. We refer to these as nodes. Most nodes in the cluster are referred to as compute nodes, this is where the computation is done on the system (where you will run your code). When you ssh into the system you are on a special purpose node called the login node. The login node, as its name suggests, is where you log in and is for editing code and files, installing packages and software, downloading data, and starting jobs to run your code on one of the compute nodes.</p> <p>Each job is started using a piece of software called the scheduler, which you can think of as a resource manager. You let it know what resources you need and what you want to run, and the scheduler will find those resources and start your job on them. When your job completes those resources are relinquished. The scheduler is what ensures that no two jobs are using the same resources, so it is very important not to run anything unless it is submitted properly through the scheduler.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#software-and-packages","title":"Software and Packages","text":"<p>The first thing you may want to do is make sure the system has the software and packages you need. We have installed a lot of software and packages on the system already, even though it may not be immediately obvious that it is there. Review our page on Software and Package Management, paying particular attention to the section on modules and installing packages for the language that you use. If you are ever unsure if we have a particular software, and you cannot find it, please send us an email and ask before you spend a lot of time trying to install it. If we have it, we can point you to it, provide advice on how to use it, and if we don't have it we can often give pointers on how to install it. Further, if a lot of people request the same software, we may consider adding it to the system image.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#linux-command-line","title":"Linux Command Line","text":"<p>The MIT SuperCloud runs Linux, so much of what you do on the cluster involves the Linux command line. That doesn't mean you have to be a Linux expert to use the system! However the more you can get comfortable with the Linux command line and a handful of basic commands, the easier using the system will be. If you are already familiar with Linux, feel free to skip this section, or skim as a refresher.</p> <p>Most Linux commands deal with directories and files. A directory, synonymous to a folder, contains files and other directories. The list of directories that lead to a particular directory or file is called its path. In Linux, directories on a path are separated by forward slashes <code>/</code>. It is also important to note that everything in Linux is case sensitive, so a file <code>myScript.sh</code> is not the same as the file <code>myscript.sh</code>. When you first log in you are in you home directory. Your home directory is where you can put all the code and data you need to run your job. Your home directory is not accessible to other users, if you need a space to share files with other users, let us know and we can make a shared group directory for you. The path to your home directory on SuperCloud is <code>/home/gridsan/[USERNAME]</code>, where <code>[USERNAME]</code> is your username. The character <code>~</code> is also shorthand for your home directory in any Linux commands.</p> <p>Anytime after you start typing a Linux command you can press the \"Tab\" button your your keyboard. This called tab-complete, and will try to autocomplete what you are typing. This is particularly helpful when typing out long directory paths and file names. Pressing \"Tab\" once will complete if there is a single completion, pressing it twice will list all potential completions. It is a bit difficult to explain in text, but you can try it out yourself and watch the short demonstration here.</p> <p>Finally, below is a list of Linux Commands. Try them out for yourself at the command line.</p> <ul> <li>Creating, navigating and viewing directories:<ul> <li><code>pwd</code>: tells you the full path of the directory you are     currently in</li> <li><code>mkdir dirname</code>: creates a directory with the name \"dirname\"</li> <li><code>cd dirname</code>: change directory to directory \"dirname\"<ul> <li><code>cd ../</code>: takes you up one level</li> </ul> </li> <li><code>ls</code>: lists the files in the directory<ul> <li><code>ls -a</code>: lists all files including hidden files</li> <li><code>ls -l</code>: lists files in \"long format\" including ownership     and date of last update</li> <li><code>ls -t</code>: lists files by date stamp, most recently updated     file first</li> <li><code>ls -tr</code>: lists files by dates stamp in reverse order, most     recently updated file is listed last (this is useful if you     have a lot of files, you want to know which file you changed     last and the list of files results in a scrolling window)</li> <li><code>ls dirname</code>: lists the files in the directory \"dirname\"</li> </ul> </li> </ul> </li> <li>Viewing files<ul> <li><code>more filename</code>: shows the first part of a file, hitting the     space bar allows you to scroll through the rest of the file, q     will cause you to exit out of the file.</li> <li><code>less filename</code>: allows you to scroll through the file, forward     and backward, using the arrow keys.</li> <li><code>tail filename</code>: shows the last 10 lines of a file (useful when     you are monitoring a logfile or output file to see that the     values are correct)<ul> <li>t<code>ail &lt;number&gt; filename</code>: show you the last &lt;number&gt;     lines of a file.</li> <li><code>tail -f filename</code>: shows you new lines as they are written     to the end of the file. Press CMD+C or Control+C to exit.     This is helpful to monitor the log file of a batch job.</li> </ul> </li> </ul> </li> <li>Copying, moving, renaming, and deleting files<ul> <li><code>mv filename dirname</code>: moves filename to directory dirname.<ul> <li><code>mv filename1 filename2</code>: moves filename1 to filename2, in     essence renames the file. The date and time are not changed     by the mv command.</li> </ul> </li> <li><code>cp filename dirname</code>: copies to directory dirname.<ul> <li><code>cp filename1 filename2</code>: copies filename1 to filename2. The     date stamp on filename2 will be the date/time that the file     was moved</li> <li><code>cp -r dirname1 dirname2</code>: copies directory dirname1 and its     contents to dirname2.</li> </ul> </li> <li><code>rm filename</code>: removes (deletes) the file</li> </ul> </li> </ul>","tags":["Getting Started","Linux"]},{"location":"getting-started/#transferring-files-to-mit-supercloud","title":"Transferring Files to MIT SuperCloud","text":"<p>One of the first tasks is to get your code, data, and any other files you need into your home directory on the system. If your code is in github you can use git commands on the system to clone your repository to your home directory. You can also transfer your files to your home directory from your computer by using the commands scp or rsync. Read the page on Transferring Files to learn how to use these commands and transfer what you need to your home directory.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#testing-your-code","title":"Testing your Code","text":"<p>At this point you may want to do a test-run of your code. You always want to start small in your test runs, so you should choose a small example that tests the functionality of what you would ultimately like to run on the system. If your test code is serial and runs okay on a moderate personal laptop or desktop you can request an interactive session to run your code in by executing the command:</p> <p><code>LLsub -i</code></p> <p>After you run this command you will be on a compute node and you can do a test-run of your code. This command will allocate one core to your job. If your test code is multithreaded or parallel, or uses a lot of memory, you should request a full node to be sure you don't impact other jobs on the system:</p> <p><code>LLsub -i full</code></p> <p>These commands by no means represent the full use of the system, and most likely won't be the primary way you run your code. In our tutorial we go over much more on how to submit jobs and will make sure you have the tools you need to get the most out of the MIT SuperCloud.</p>","tags":["Getting Started","Linux"]},{"location":"getting-started/#supercloud-downtimes","title":"SuperCloud Downtimes","text":"<p>Note that SuperCloud has Monthly Downtimes which are scheduled for the Second Tuesday of each month. During downtimes the system is not available. Downtimes usually last about a day and emails are sent when they are complete. We also send out a reminder email a few days before each downtime.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/","title":"Glossary of Unix Terms","text":"<p>The following is a list of commonly used Unix terms and symbols and their definitions when used in the context of SuperCloud.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#_1","title":"<code>/</code>","text":"<p>The root directory.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#_2","title":"<code>~</code>","text":"<p>Pronounced \"tilde\". Shorthand expression representing a user's home directory. On the SuperCloud, <code>~</code> refers to <code>/home/gridsan/&lt;username&gt;</code>. When using tilde in a pathname, it must be the first character. For example, <code>~/test/test.txt</code>.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#_3","title":"<code>.</code>","text":"<p>Pronounced \"dot\". Shorthand expression representing the current working directory</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#_4","title":"<code>..</code>","text":"<p>Pronounced \"dot-dot\". Shorthand expression representing the parent directory of the current working directory</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#absolute-pathname-or-full-pathname","title":"absolute pathname or full pathname","text":"<p>A pathname that begins at the root directory. For example, <code>/home/gridsan/groups</code>.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#background-process","title":"background process","text":"<p>A process that runs without interfering with normal command line entry. A process runs in the background when the command to start the process is issued with an ampersand <code>&amp;</code> character at the end of the command.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#binary","title":"binary","text":"<p>A file whose contents are in binary (non-text) format, and contains compiled source code. Binary files can be, but are not necessarily executable (for example, library files).</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#child-process","title":"child process","text":"<p>A process that was created by another process, the parent process.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#current-directory-current-working-directory-or-working-directory","title":"current directory, current working directory, or working directory","text":"<p>The user's current directory; the directory where a file is read or written when a directory path is not included in the name of the file. When specifying a pathname, a shorthand for the current directory is the dot <code>.</code> character.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#directory","title":"directory","text":"<p>A type of file that contains names and information about other files or other directories. This is the Linux equivalent of Windows folders.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#environment","title":"environment","text":"<p>The set of conditions under which a user is working on the computer.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#environment-variable","title":"environment variable","text":"<p>Environment variables allow you to customize the environment in which programs run. They become part of the environment in which the programs run and can be queried by running programs and any of its child processes.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#executable","title":"executable","text":"<p>A file that can be executed from the command line by entering the name of the file as the command. The file's \"executable\" permissions flag must also be set. An executable file can be, but is not necessarily a binary file (for example, a script).</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#file-system","title":"file system","text":"<p>The collection of files and file management structures on a mass storage device.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#foreground-process","title":"foreground process","text":"<p>A process that must complete or be interrupted before control is returned to the command line.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#grep","title":"grep","text":"<p>The command used for searching files for lines containing characters that match specified strings or patterns (regular expressions) and writes the matching lines to standard output.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#group","title":"group","text":"<p>For SuperCloud purposes, a group is a set of users that share access to protected resources.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#hidden-file","title":"hidden file","text":"<p>A file whose name begins with a period (for example, <code>.bash_profile</code>). By default, the <code>ls</code> command does not include these files in its listings. Use <code>ls -a</code> to see hidden files in the file listing.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#home","title":"$HOME","text":"<p>An environment variable containing the full pathname to your home directory.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#home-directory","title":"home directory","text":"<p>A directory that is where a particular user's personal files reside. A home directory is also called a login directory because when you log into the system, your current work directory is your home directory. When specifying a pathname, a shorthand for a user's home directory is the tilde <code>~</code> character.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#kill","title":"kill","text":"<p>To stop the operation of a process. In most cases, a user can kill a foreground process by pressing ctrl-c.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#make","title":"make","text":"<p>A tool that builds applications/executables from source files.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#man","title":"man","text":"<p>An interface to the online reference manuals. The <code>man</code><code>arg</code> command will display the manual page for the given argument.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#parent-directory","title":"parent directory","text":"<p>The directory in which another directory resides. When specifying a pathname, a shorthand for the parent directory is dot-dot <code>..</code>. A directory that is contained in the parent is called a subdirectory.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#parent-process","title":"parent process","text":"<p>A process that has created other processes (its children processes). In Unix, every command that isn't a built-in command creates a child process.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#path","title":"$PATH","text":"<p>An environment variable containing the user's search path for commands.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#path_1","title":"path","text":"<p>An ordered list of directories that the shell searches for executables that are not built-in commands and are not entered with a pathname.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#pathname","title":"pathname","text":"<p>The name of a file, concatenated onto a list of the directories that must be accessed to get to the file. Absolute pathnames begin at the root directory and are written with an initial slash (for example, <code>/home/gridsan/myusername/test/myfile.txt</code>). Relative pathnames begin at the user's working directory and are written without the initial slash (for example, <code>test/myfile.txt</code>).</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#permissions","title":"permissions","text":"<p>The constraints placed on a file to control what users or groups may read, write, or execute the file. There are three sets of permissions: those applied to the owner of the file, those applied to the owner's group, and those applied to everyone else, called \"other.\"</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#pipe","title":"pipe","text":"<p>A command that lets you use two or more commands, sending the output of one command to serve as the input to the next command. A pipe is represented as the symbol \"|\".</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#process","title":"process","text":"<p>An independent computation running on a computer. Processes have their own address (memory) space and may create threads that will share their address space. Processes must use interprocess communication to communicate with other processes.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#process-id-or-pid","title":"process id or pid","text":"<p>A unique number assigned to a process that is running.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#redirection","title":"redirection","text":"<p>A feature that allows you to change where standard input, standard output and/or standard error are sent during execution of a command.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#regular-expression-regex-or-regexp","title":"regular expression, regex, or regexp","text":"<p>A pattern of one or more characters used to search data and matching complex patterns. Regular expressions are similar to wildcards, but more powerful. You can find an introduction to regular expressions here.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#relative-pathname","title":"relative pathname","text":"<p>A pathname that begins at the user's working directory; it is written without the initial <code>/</code>. For example, <code>docs/myfile.txt</code></p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#root","title":"root","text":"<p>The login name for the superuser (system administrator). Also the name of the topmost directory in the file system, represented by the slash <code>/</code> character.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#script","title":"script","text":"<p>A text file that is executed by the specified shell, as indicated by the first line of the script (for example, <code>#!/bin/bash</code>).</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#search-path","title":"search path","text":"<p>A list of pathnames (usually separated by colons) of directories to be searched for executable files and other kinds of files. You can create search paths by defining environment variables such as <code>$PATH</code>, <code>$PYTHONPATH</code>, <code>$LD_LIBRARY_PATH</code>.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#shell","title":"shell","text":"<p>A command-line Interface between a user and the kernel. The shell accepts and interprets your commands to run programs, work with file systems, manipulate text files, etc.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#standard-error-or-stderr","title":"standard error or stderr","text":"<p>The file where programs write error messages. The standard error file is a virtual file that is by default assigned to the user's screen but can be redirected to any file available to the user.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#standard-input-or-stdin","title":"standard input or stdin","text":"<p>The file where programs receive input data or commands. The standard input file is a virtual file that is by default assigned to the user's keyboard but can be redirected to any file available to the user.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#standard-output-or-stdout","title":"standard output or stdout","text":"<p>The file where programs write output data. The standard output file is a virtual file that is by default assigned to the user's screen but can be redirected to any file available to the user.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#subdirectory","title":"subdirectory","text":"<p>A directory that is contained within another directory. The containing directory is called the parent directory.</p>","tags":["Getting Started","Linux"]},{"location":"glossary-of-unix-terms/#symbolic-link-symlink-or-soft-link","title":"symbolic link, symlink, or soft link","text":"<p>A type of file that points to another file or directory on the system. Symlinks are similar to shortcuts in Windows.</p>","tags":["Getting Started","Linux"]},{"location":"glossary/","title":"Glossary","text":"<p>The following is a list of commonly used terms and acronyms, and their definitions when used in the context of the MIT SuperCloud.</p> <p>First is a visual labeling the portions of the system with the terminology we tend to use for each piece.</p> <p></p> <p>Accelerators A piece of hardware used to speed up computation, usually for a   specific operation. GPUs are used as an accelerator for certain matrix   operations.</p> <p>Bandwidth A theoretical measure of how much data could be transferred from   source to destination in a given amount of time.</p> <p>Bash shell A specific shell and language used at the command line. This is the   shell used on SuperCloud.</p> <p>Bash script/Shell script A script using bash command syntax.</p> <p>Batch job A job for running a pre-written script or executable. Resources are   requested through the scheduler, the schedule allocates the resources   when they are available, runs the script, and then exits.</p> <p>Cluster Many nodes connected via a fast network interconnect.</p> <p>Command Line A text-based user interface that allows a user to type commands that   the computer then executes.</p> <p>Compute Nodes Nodes where the computation is done on the system (where you will run   your code). Compute nodes are managed by the scheduler.</p> <p>Core A core is the smallest computation unit that can run a program.</p> <p>CPU The Central Processing Unit (CPU) is the part of a computer which   executes software programs. CPU refers to an individual silicon chip,   such as Intel's Xeon-E5 or AMD's Opteron. \u00a0A CPU contains one or   more cores. \u00a0Also known as a processor or socket.</p> <p>Data Server Also called an Object Storage Server. A component of a parallel file   system which stores all of the data of the files on the file system.</p> <p>(Job) Dependency Defer the start of a job until the specified dependencies have been   satisfied completed. This is usually the completion of another job.</p> <p>Distributed Memory (see \"Memory Models\") In\u00a0a distributed memory system, each CPU has its own private memory.   Processes can only operate on local data. If remote data is required,   the process must communicate with the remote process over an   interconnect.</p> <p>Downtime A regular maintenance day during which the system is unavailable.</p> <p>Environment Variable Environment variables allow you to customize the environment in which   programs run. They become part of the environment in which the   programs run and can be queried by running programs. For example, you   can set an environment variable to contain the path to your data   files. Your running process can query this environment variable to get   the location of the files.</p> <p>File Permissions Properties of a file that determine who can read, write, or execute   (run) a file.</p> <p>Filesystem The system that controls how and where data is stored on storage disk.   See Shared/Central Filesystem and Local Filesystem.</p> <p>GPU A Graphics Processing Unit (GPU) is a specialized device originally   used to generate computer output. \u00a0Each compute node can host one or   more GPUs. \u00a0Modern GPUs have many simple compute cores and have been   used for parallel processing.</p> <p>Group Shared Directory A directory, created upon user request, where members of the group   shared directory can share files with other members of the group.   \u00a0Since a user's home directory is accessible only to the user, a group   shared directory is the only mechanism for users to share files.</p> <p>GUI Graphical User Interface- these are interfaces that allow the user to   interact with a program with a mouse through visual icons, as opposed   to a command line interface.</p> <p>Home Directory Where the user keeps their files. Each user has their own home   directory.</p> <p>HPC High Performance Computing (HPC) refers to the practice of aggregating   computing power to achieve higher performance that would not possible   by using a typical computer.\u00a0 The community often used\u00a0concurrent   computing\u00a0to mean programs running at the same time v in serial one   after another.</p> <p>Hub A networking component that takes an incoming message and broadcasts   it across all of the other ports of the hub.</p> <p>Independent (Tasks/Processes) Tasks/processes that can operate by themselves without needing data   from another.</p> <p>Interactive Job An interactive job allows you to actually log in to a compute node.   This is useful for when you need to compile software, test jobs and   scripts, or run software that requires keyboard inputs and user   interaction, such as a graphical interface\u00a0.</p> <p>Interconnects The connections between components of the computer (this interconnect   is called the System Network), and the computer to the Internet   network (this interconnect is called the Network Connection).</p> <p>I/O (Input/Output) Refers operations that involve a transfer of data, particularly   reading from and writing to the filesystem.</p> <p>Job A job is a separately executable unit of work whose resources are   allocated and shared. \u00a0Users create job submission scripts to ask the   scheduler for resources (cores, a specific processor type, etc). \u00a0The   scheduler places the requests in a queue and allocates the requested   resources.</p> <p>Job Array According to the Slurm documentation: \u00a0\"Job arrays offer a mechanism   for submitting and managing collections of similar jobs quickly and   easily\". \u00a0 Job arrays are useful for applying the same processing   routine to a collection of multiple inputs, data, or files. \u00a0Job   arrays offer a very simple way to submit a large number of independent   or\u00a0High Throughput\u00a0processing jobs.</p> <p>Job Slot A computational resource unit that is roughly equivalent to a   processor core. One or more job slots can be used to execute a   process.</p> <p>Jupyter Notebook An interactive browser-based programming environment.</p> <p>Latency The delay before a transfer of data begins following an instruction   for its transfer.</p> <p>Lgpn Average observed Load per GPU on the node.</p> <p>LLGrid Beta LLGrid Beta is a collection of software packages that are released as   a beta test on the SuperCloud. \u00a0The beta software packages are ones   that SuperCloud users have requested but are not included in the   SuperCloud system image.</p> <p>LLMapReduce A language-agnostic command for running loosely coupled or MapReduce   applications.</p> <p>LLx A course platform containing online courses that use the SuperCloud   system for exercises.</p> <p>Lnode Average observed Load on the node.</p> <p>Local Filesystem Each node in the cluster has its own local filesystem that is only   accessible from that node. The system image and software stack is on   the local filesystem. It also contains space that can be used during   jobs for fast file access.</p> <p>Login Node The login node controls user access to a parallel computer. \u00a0Users   usually connect to login nodes via SSH to compile and debug their   code, review their results, do some simple tests, and submit their   interactive and batch jobs to the scheduler.</p> <p>Loosely Coupled Applications that involve an independent (map) step where the same   operation can be performed by many processes on different inputs,   followed by a serial step that uses the output of the first step as   its input. Also called MapReduce.</p> <p>Lppn Average observed Load per process on the node.</p> <p>Man page Short for \"manual page\". Documentation for a command or program.</p> <p>MapReduce See \"Loosely Coupled\".</p> <p>Mbpc Memory Bytes Per Core.</p> <p>Mbpn Memory Bytes Per Node.</p> <p>Mbpp Memory Bytes Per Process.</p> <p>Memory See \"Volatile Memory\".</p> <p>Memory Models (see \"Distributed Memory\" and \"Shared Memory\")</p> <p></p> <p>Metadata Server A component of a parallel file system which maintains the state of all   files and folders within the file system, and the list of data servers   where it can find the data for the files.</p> <p>MIMO Mode Multiple input, multiple output is an application mode for use with   LLMapReduce. In MIMO mode your application iterates through multiple   inputs. LLMapReduce calls and loads your application once in order to   process multiple assigned inputs.</p> <p>Modules Here we are referring to \"environment modules\", but we often refer   to them just as \"modules\". An open source software management tool   used in most HPC facilities. \u00a0Using modules enable users to   selectively pick the software that they want and add them to their   environment. \u00a0Using the module command, you can manipulate your   environment to gain access to new software or different versions of a   package.</p> <p>MPI The Message Passing Interface (MPI) is a library for passing messages   between processes and between compute nodes within a parallel job   running on a cluster. There are a variety of open source and   commercial versions of MPI that have\u00a0been developed over the past   several decades including mpich,\u00a0 OpenMPI, and Intel MPI.</p> <p>Multi-Threaded Describes an application that uses multiple threads. See \"Shared   Memory\".\u00a0</p> <p>Ncpn Number of hardware Cores Per Node.</p> <p>Ngpn Number of hardware GPUs Per Node.</p> <p>Nnode Number of Nodes.</p> <p>Node A stand-alone computer where jobs are run. \u00a0Each node is connected to   other compute nodes via a fast network interconnect. \u00a0While accessible   via interactive jobs, compute nodes are not meant to be accessed   directly by users.</p> <p>Non-Volatile Memory Storage device where the information stored on it remains intact even   when the computer is shut down or restarted, e.g., disk drives.</p> <p>Np Number of Processes = Nnode * Nppn.</p> <p>Nppn Number of Processes Per Node.</p> <p>Ntpn Number of Threads Per Node = Nppn * Ntpp.</p> <p>Ntpp Number of Threads Per Process.</p> <p>Operating System (OS) The software that manages how each of the applications running on the   computer interact with the hardware of the computer to accomplish   tasks.</p> <p>Path A list of directories separated by \"/\" characters that shows the   location of a file or directory in the directory structure.</p> <p>Absolute Path The full path from the root of the filesystem, /. For example, the   absolute path to the home directory for studentx would be:   /home/gridsan/studentx.</p> <p>Relative\u00a0Path The path to a file or directory from the current location.</p> <p>Partition A group of nodes with a set of constraints or rules for the jobs that run   on them. Partitions may group nodes by node type (a partition of GPU nodes) or by job type (a set of nodes put aside for for jupyter, interactive, or download jobs). \"Partition\" is Slurm terminology, other schedulers refer to this as a \"queue\".</p> <p>Process An independent computation running on a computer. \u00a0Processes have   their own address space and may create threads that will share their   address space. \u00a0Processes must use interprocess communication to   communicate with other processes.</p> <p>Router A networking component that acts as a special switch that moves   messages across defined network boundaries.</p> <p>Rsync A command for transferring and syncing files between systems.</p> <p>Scheduler The scheduler receives job and task execution requests from users and   manages how and where they are executed across the many compute nodes   in the HPC system. Before starting a job, it ensures that the needed   resources are available for the job. The scheduler monitors running   jobs, can stop jobs, and can provide information about completed jobs   and the status of the system (e.g. what resources are currently   available).</p> <p>Shared/Central Filesystem The shared filesystem is the filesystem that is available to all nodes   in the cluster. Home and group directories are on the shared   filesystem.</p> <p>Shared Memory\u00a0(see \"Memory Models\") In\u00a0a shared memory system, there is shared memory that can be   simultaneously accessed by multiple CPUs in a multiprocessor CPU.   Communication or data passing among threads or processes in a shared   memory system is via memory.</p> <p>Shell Another term for the Linux command line interface.</p> <p>SISO Mode Single input, single output is an application mode for use   with\u00a0LLMapReduce. In SISO mode, your application runs on a single   input. LLMapReduce calls and loads your application once in order to   process one assigned input.</p> <p>Slurm Simple Linux Utility for Resource Management (SLURM) is a job   scheduler which coordinates the running of many programs on a shared   facility. \u00a0Slurm is used on the MIT SuperCloud system. \u00a0It replaced   the SGE scheduler.</p> <p>Socket A computational unit packaged as one, and usually made of a single   chip often called processor. \u00a0Modern sockets carry many cores.</p> <p>SPMD Single Program Multiple Data</p> <p>SSH Secure Shell (SSH) is a protocol to securely access remote computers.   \u00a0Based on the client-server model, users with an SSH client can access   a remote computer. \u00a0Some operating systems such as Linux and Mac OS   have a built-in SSH client and others can use one of many publicly   available clients. \u00a0For Windows, we recommend PuTTY or Cygwin for ssh.</p> <p>SSH Keys Credentials used as an authentication method for ssh. These come in   pairs: a public and private key. Public keys are placed on the system   you need to access, private keys are placed on your computer. When you   ssh in the ssh program checks to see whether the public key fits your   private key.</p> <p>Submission/batch script A script for submitting a batch job to the scheduler. It is a bash   script that tells the scheduler how to run your job, and may include   the resources you are requesting for you job.</p> <p>Switch A networking component that is more efficient than a hub. It takes an   incoming network message and sends it out only onto the switch port on   which its destination will be reached. Switches only transmit messages   within a defined network.</p> <p>Symlink Short for symbolic link. A file that acts as a shortcut by pointing to   another file or directory on the filesystem. If you are in a group you   may see a symbolic link to the shared group directory in your home   directory.</p> <p>Terminal (Window) A window containing a command line prompt.</p> <p>Third-party software According to Wikipedia: a third-party software component is a reusable   software component developed to be either freely distributed or sold   by an entity other than the original vendor of the development   platform.\u00a0</p> <p>Examples of third-party software on the SuperCloud system include MATLAB and TensorFlow.\u00a0</p> <p>Thread Threads are lightweight processes which exist within a single   operating system process. \u00a0Threads share the address space of the   process that created them and can communicate directly with other   threads in the same process.</p> <p>Throughput An actual measure of how much data is successfully transferred from   source to destination in a given amount of time.</p> <p>Throughput (Workflow) A throughput application is one that is fully independent. Often this   means it is running the same operation on a number of different inputs   or parameters, and the result of an operation on one input does not   depend on the result of another.</p> <p>Triples (Mode) A job submission mode that allows you to request resources in a   triple: Number of Nodes, Number of Processes per Node, and Number of   Threads per Process. Available for LLsub job arrays, LLMapReduce, and   pMatlab jobs.</p> <p>Ubpn Average observed Used bytes per node.</p> <p>Ubpp Average observed Used bytes per process.</p> <p>Unix, Linux Unix is a family of portable, multi-tasking, multi-user operating   systems. Linux is an open source, Unix-like operating system that is   derived from Unix. The SuperCloud system runs the Ubuntu version of   the Linux operating system.</p> <p>User space User space is a set of locations where normal user processes (i.e.   everything other than the kernel, the lowest part of the operating   system) run.</p> <p>Volatile Memory Storage device where applications and data are loaded so that the   processors can actively work with them, e.g. RAM and cache. The   information stored on it does not remain intact when the computer is   shut down or restarted.</p> <p>Web Portal A web page for SuperCloud where you can access your SuperCloud   account. On the Web Portal you can add ssh keys, access the files in   your home directory, and start Jupyter Notebooks.</p>"},{"location":"gpu-jobs/","title":"Optimizing your GPU Jobs","text":"<p>The GPUs on SuperCloud often are in very high demand. Before asking for more GPUs you'll want to do what you can to optimize your code to get the most out of the resources you do have.</p> <p>The first thing you need to do is profile your code.\u00a0Time your code both with and without the GPU. When you are running with a GPU, monitor the GPU Utilization and GPU Memory use. You can do this by going to the compute node where it is running (get the node name from LLstat, then ssh to the node with <code>ssh nodename</code>) and run the <code>nvidia-smi -l</code> command (press <code>Ctrl+C</code> to exit <code>nvidia-smi -l</code> when you are done).</p> <p>If your GPU utilization and GPU memory use is low, that means you aren't getting the full advantage out of the GPU. If this is the case, try out some of the suggestions below.</p> <p>If you are only getting a small speedup, say 2x or 4x, especially after trying the suggestions below, it may not be worthwhile using GPUs at all. In general, the CPU nodes on SuperCloud are more available so we will likely be willing to increase your allocation of CPU nodes to make up for the small loss of speedup switching to CPUs.</p>"},{"location":"gpu-jobs/#getting-more-out-of-the-gpus","title":"Getting More out of the GPUs","text":"<p>If your GPU utilization and memory use are low, usually this means you might be able to get some more performance out of the GPUs. There are a few things you can try if you are training machine learning models:</p> <ul> <li>Increase the batch size.</li> <li>Enable CUDA kernel tuning.<ul> <li>For Pytorch, add this before the training starts:     <code>torch.backends.cudnn.benchmark = True</code></li> <li>For Tensorflow, set this environment variable in your submission     script: <code>export TF_CUDNN_USE_AUTOTUNE=1</code></li> </ul> </li> <li>If you are using Tensorflow you can also try mixed-precision     training (we haven't played with this in Pytorch, but it could be     possible).<ul> <li>Tensorflow 2.4.1 and newer (anaconda/2021a+)<ul> <li>Add the following to the beginning of your Python code:<ul> <li><code>from tensorflow.keras import mixed_precision</code></li> <li><code>policy = mixed_precision.Policy('mixed_float16')</code></li> <li><code>mixed_precision.set_global_policy(policy)</code></li> </ul> </li> </ul> </li> <li>Pre-Tensorflow 2.4.1 (anaconda/2020b and older)<ul> <li>To do so set the following environment variables in your     submission script:<ul> <li><code>export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1</code></li> <li><code>export TF_ENABLE_AUTO_MIXED_PRECISION=1</code></li> </ul> </li> <li>And add this to your Python code, here <code>opt</code>\u00a0is the     optimizer object:<ul> <li><code>opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)</code></li> </ul> </li> </ul> </li> </ul> </li> <li>If the number of filters in the model layers is a multiple of 64,     you will see an additional speedup because the V100 tensor cores are     optimized for matmul/conv ops of these sizes.</li> <li>If both GPU utilization and memory are below 50% you could try to     train two models per GPU. This isn't too hard to do with Triples     Mode, simply set     the number of processes per node to 4.</li> </ul>"},{"location":"gpu-jobs/#requesting-more-gpus","title":"Requesting More GPUs","text":"<p>If you have tried the above suggestions and you still find you need more GPUs you can put in a request. Send an email to supercloud@mit.edu and tell us what you need, what you need it for, and how long you need the allocation increase. Are you trying to run a large distributed training run, or are you doing hyperparameter searches? Justify your request with numbers: show us that you have compared CPU and GPU run times, that you have good GPU utilization, and that you have tried the above suggestions. Explain how you got to the number of GPUs you are requesting. If the system is not too busy we may grant your request. Keep in mind that others are likely to have the same paper deadlines as you and we may not be able to grant requests during these busy periods, so plan ahead.</p>"},{"location":"gridmatlab-env-vars/","title":"GridMatlab Environment Variables","text":"<p>On the SuperCloud system, pMatlab jobs are launched through gridMatlab, which integrates the scheduler and pMatlab in order to provide/manage the resources requested by pMatlab jobs dynamically. In order to control the job launch and execution behavior through gridMatlab, a number of gridMatlab environment variables are provisioned as shown below.</p> <p>Those environment variables designated as \"internal use only\" are used only by gridMatlab. However, the other environment variables are meant to be modified by the user in order to change the default behavior of gridMatlab.</p> <p>Use the following command to set an environment variable, replacing <code>&lt;var-name&gt;</code> and\u00a0<code>&lt;value&gt;</code> with the desired values. <code>&lt;value&gt;</code> is typically <code>yes</code>, <code>no</code>, <code>on</code> or <code>off</code>\u00a0(case insensitive).</p> <pre><code>setenv('&lt;var-name&gt;','&lt;value&gt;')\n</code></pre>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_cpu_type","title":"<code>GRIDMATLAB_CPU_TYPE</code>","text":"<p>Select compute nodes with a specific CPU type. Valid values are <code>xeon-g6</code> and <code>xeon-p8</code>.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_debug","title":"<code>GRIDMATLAB_DEBUG</code>","text":"<p>Enable display of scheduler job submission command by setting <code>setenv('GRIDMATLAB_DEBUG','yes')</code>.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_desktop_filesys_stub","title":"<code>GRIDMATLAB_DESKTOP_FILESYS_STUB</code>","text":"<p>For internal use only.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_gpu_req","title":"<code>GRIDMATLAB_GPU_REQ</code>","text":"<p>Select the type and number of GPUs. Valid values are <code>volta:1</code> or <code>volta:2</code>.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_grid_sshkey","title":"<code>GRIDMATLAB_GRID_SSHKEY</code>","text":"<p>For internal use only.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_keep_temp_files","title":"<code>GRIDMATLAB_KEEP_TEMP_FILES</code>","text":"<p>By default, all temporary files will be deleted at the end of the run, but if this variable is set to <code>YES</code>, those files will not be deleted after the run.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_lite","title":"<code>GRIDMATLAB_LITE</code>","text":"<p>Internal use only. Dynamically select a light MATLAB\u00ae version only if the light version is used to launch a job from a login node. The light version does not install any MATLAB\u00ae toolboxes, allowing for a much faster launch of MATLAB\u00ae. This variable is available starting with the MATLAB\u00ae 2017B version.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_local_fs","title":"<code>GRIDMATLAB_LOCAL_FS</code>","text":"<p>By setting <code>setenv('GRIDMATLAB_LOCAL_FS','no')</code>, use of the local filesystem will be disabled for pMatlab operations such as aggregation (<code>agg</code>) and message communication (<code>SendMsg</code>/<code>RedvMsg</code>/<code>BcastMsg</code>) functions. Instead, the central Lustre parallel filesystem will be used, which could decrease your performance due to in increased filesystem latency. It may be helpful to set this variable to <code>no</code> while you are debugging your pMatlab code.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_manycore","title":"<code>GRIDMATLAB_MANYCORE</code>","text":"<p>By default, this variable is set to <code>YES</code>, enabling triples mode pMatlab job launches by default. To request non-triples mode pMatlab job launches, set this variable to <code>NO</code>.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_memory_req","title":"<code>GRIDMATLAB_MEMORY_REQ</code>","text":"<p>This variable specifies the amount of RAM (in megabytes) to reserve for each gridMatlab process.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_mt_slots","title":"<code>GRIDMATLAB_MT_SLOTS</code>","text":"<p>By default each compute task consumes one slot (equivalent to one core in the SuperCloud environment). This variable allows you to specify the number of slots to be used by each compute task.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_srun","title":"<code>GRIDMATLAB_SRUN</code>","text":"<p>By default, pMatlab jobs require that the requested resources be available when you launch your job. If the resources are not available the job will not launch. If instead you would like your job to wait in the queue until the resources are available, you can set the <code>GRIDMATLAB_SRUN</code> environment variable to <code>YES</code> prior to launch.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_use_nojvm","title":"<code>GRIDMATLAB_USE_NOJVM</code>","text":"<p>This variable controls the <code>-nojvm</code> option in the MATLAB\u00ae command line option. By setting <code>setenv('GRIDMATLAB_USE_NOJVM','yes')</code> before executing the <code>pRUN()</code> command, it will add the <code>-nojvm</code> option to the MATLAB\u00ae command line. This will help speed up MATLAB\u00ae launch time.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"gridmatlab-env-vars/#gridmatlab_wait_time","title":"<code>GRIDMATLAB_WAIT_TIME</code>","text":"<p>This variable is used to specify the wait time control in the <code>execwait_LaunchFunctionOnGrid()</code> function.</p>","tags":["pMatlab","Troubleshooting"]},{"location":"installing-wsl/","title":"Installing Windows Subsystem for Linux","text":"<p>We highly recommend that Windows 10 users install the Windows Subsystem for Linux (WSL) on their desktop system. With WSL, you get a full Ubuntu shell capability that works directly on the Windows 10 OS kernel.</p>","tags":["How To","Linux","Windows"]},{"location":"installing-wsl/#installation-instructions","title":"Installation instructions","text":"<p>In order to install WSL, you will need Administrator privileges on your desktop system.</p> <p>You can find instructions on how to install WSL on the Microsoft webpage  Install Linux on Windows with WSL (select the default distribution, Ubuntu).</p>","tags":["How To","Linux","Windows"]},{"location":"job-array-triples/","title":"Job Arrays with LLsub Triples in 3 Steps","text":"<p>If you are currently running a Job Array, you can take advantage of SuperCloud's triples mode submission by submitting your job array with LLsub. In most cases, this can be done in a few easy steps.</p> <p>What is triples mode? It's a different way to submit your job by specifying three numbers:</p> <ul> <li>Nodes: The number of nodes you want to use up</li> <li>NPPN: The number of processes that should run per node</li> <li>NTPP: The number of threads per process</li> </ul> <p>Why would you want to use triples mode to submit your job? Triples mode provides a few advantages. Since it does whole-node scheduling you don't have to worry about other jobs impacting yours (or your job impacting someone else's). We also do task-pinning when you request resources with a triple, so your processes are arranged in the best way possible for the layout of the hardware architecture. Finally, submitting a Job Array with LLsub triples mode the way we describe here will greatly reduce the startup time for your jobs, over a job array with many pending tasks.</p>"},{"location":"job-array-triples/#step-1-batch-up-your-array","title":"Step 1: Batch Up your Array","text":"<p>The first thing you want to do is set up your job so that your script splits up your inputs among each process/task and each process/task iterates a subset of your inputs. This way you can change your input set without changing the number of tasks or processes that you schedule. This step alone will save you on startup time. Anytime you submit more jobs than your allocation allows, those additional jobs will remaining pending until some of your first running jobs complete. Then, when one of your first jobs complete, the scheduler now has to find the pending job some resources and start it running. If you batch up your job array, you only have to go through the scheduling process once. If you've set up a job array following our instructions in the past you may already be doing this, and you can skip to the next section.</p> <p>First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you'll first want to enumerate all the combinations of what you are iterating over so you have one big loop.</p> <p>If your code is written so it uses the <code>$SLURM_ARRAY_TASK_ID</code> and uses that to run a single thing, first add a for loop that iterates over the full set of things you want to run your code on. If you can't rewrite your code in such a way that it iterates over multiple inputs, you can use LLMapReduce to submit your job with triples mode, see this example.</p> <p>Then you add a few lines to your code to take in two arguments, a process/task ID and the number of processes/tasks, and use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, <code>fnames</code>. In python I would add:</p> PythonJulia <pre><code># Grab the arguments that are passed in\nmy_task_id = int(sys.argv[1])\nnum_tasks = int(sys.argv[2])\n\n# Assign indices to this process/task\nmy_fnames = fnames[my_task_id:len(fnames):num_tasks]\n\nfor f in my_fnames: ...\n</code></pre> <pre><code># Grab the arguments that are passed in\ntask_id = parse(Int,ARGS[1])\nnum_tasks = parse(Int,ARGS[2])\n\n# Assign indices to this process/task\nmy_fnames = fnames[task_id+1:num_tasks:length(fnames)]\n\nfor f in my_fnames\n    ...\n</code></pre> <p>Notice that I am iterating over <code>my_fnames</code>, which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this).</p>"},{"location":"job-array-triples/#step-2-changing-your-submission-script","title":"Step 2: Changing Your Submission Script","text":"<p>If you have been running your Job Arrays with sbatch, you most likely have a few environment variables in your submission script. To submit with LLsub triples, you can just replace these:</p> <ul> <li><code>$SLURM_ARRAY_TASK_ID</code> -&gt; <code>$LLSUB_RANK</code></li> <li><code>$SLURM_ARRAY_TASK_COUNT</code> -&gt; <code>$LLSUB_SIZE</code></li> </ul> <p>If you have any SBATCH flags in your submission script, remove those as well (LLsub will see these and submit with sbatch, ignoring any command line arguments you give it). For example, if you are running a python script, your final submission script will look something like this:</p> <pre><code>#!/bin/bash\n\n# Load Module(s)\nmodule load anaconda/2023a\n\necho \"My task ID: \" $LLSUB_RANK\necho \"Number of Tasks: \" $LLSUB_SIZE\n\npython top5each.py $LLSUB_RANK $LLSUB_SIZE\n</code></pre> <p>You will also need to make your script executable. You can do that with this simple command line command:</p> <p><code>chmod u+x submit_LLsub.sh</code></p>"},{"location":"job-array-triples/#step-3-submit-your-job-with-llsub-triples","title":"Step 3: Submit your Job with LLsub Triples","text":"<p>Now when you submit your job, you can run:</p> <p><code>LLsub ./submit.sh [NODES,NPPN,NTPP]</code></p> <p>where</p> <ul> <li><code>NODES</code> is the number of nodes you want to use up</li> <li><code>NPPN</code> is the number of processes that should run per node</li> <li><code>NTPP</code> is the number of threads per process</li> </ul> <p>The total number of threads per node, or <code>NTPP*NPPN</code>, should not be more than the number of cores on the node, otherwise you may overwhelm the node with too many running processes and/or threads. For example, if you are running on the 48-core Xeon-P8 nodes, if you are running with <code>NTPP</code>=1, you should not set <code>NPPN</code> more than 48. If <code>NTPP</code>=2, <code>NPPN</code> should be at most 24, etc. The numbers you choose depend on your application, if it is multithreaded it may be worth increasing <code>NTPP</code> and decreasing <code>NPPN</code>. If your application consumes a lot of memory, you may need to decrease <code>NPPN</code> so each process has the memory it needs to proceed. The best way to determine what numbers to choose is to tune your triples, which is a relatively quick exercise and can improve your speedup in the long run by helping you select the ideal numbers for your triple.</p> <p>So if you want to run on 2 nodes, 10 processes per node, and 4 threads per process, you would run:</p> <p><code>LLsub ./submit.sh [2,10,4]</code></p> <p>If you were to run LLstat after running this command, you would see what looks like a 2 task job array, for example:</p> <pre><code>$ LLstat\nLLGrid: txe1 (running slurm-wlm 20.11.3)\nJOBID ARRAY_JOB_ NAME USER START_TIME PARTITION CPUS FEATURES MIN_MEMORY ST NODELIST(REASON)\n9651412_1 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-2\n9651412_2 9651412 submit_LLsub.sh studentx 2021-03-19T10:32:26 normal 40 xeon-g6 8500M R d-13-8-1\n</code></pre> <p>However, you are still running 2*10 = 20 total processes. Triples mode uses slurm to request full nodes, then takes care of launching the processes on each node. This is why you'll always see one task for each node in the <code>LLstat</code> output, rather than each process. One advantage of this is it makes for a more compact <code>LLstat</code> output that is easier to read, instead of having to scroll through tons of tasks.</p> <p>You can check on your individual processes by looking at the log files. LLsub with Triples mode will create a directory with the prefix \"LLSUB\" followed by a unique number to hold all the log files. Within this directory will be one launch log file, which will capture any errors that occur during launch ,and one directory per node and put the log files for each process in its node's directory. These subdirectories are labeled with the process ID range and the node name.</p> <p>For example, here are the log files from a triples run using [2,4,10]:</p> <pre><code>$ ls LLSUB.23004\nREADME.md helpers.py submit_LLsub.sh submit_sbatch.sh top5each.py\n\n$ ls LLSUB.23004/\nllsub-triple-mode-launch.log-10006591 p0-p3_d-19-4-1 p4-p7_d-19-3-4\n\n$ ls LLSUB.23004/p0-p3_d-19-4-1/\nsubmit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7\n\n$ ls LLSUB.23004/p4-p7_d-19-3-4/\nsubmit_LLsub.sh.log.4 submit_LLsub.sh.log.5 submit_LLsub.sh.log.6 submit_LLsub.sh.log.7\n</code></pre> <p>In this example, <code>LLSUB.23004</code> is the directory containing the log files, <code>llsub-triple-mode-launch.log-10006591</code> is the log file for the job launch. <code>p0-p3_d-19-4-1</code> and <code>p4-p7_d-19-3-4</code> are the directories for each node, and <code>submit_LLsub.sh.log.0</code>, <code>submit_LLsub.sh.log.1</code>, ..., <code>submit_LLsub.sh.log.7</code> are the log files for each process.</p>"},{"location":"jupyter-notebook-conversion/","title":"Converting Jupyter Notebooks to Scripts","text":"<p>The Jupyter nbconvert tool allows you to convert an .ipynb notebook file into an executable python, Julia, Matlab, or R script.</p> <p>In a terminal window go to the directory with your notebook. Then run:</p> <pre><code>jupyter nbconvert --to script notebook.ipynb\n</code></pre> <p>where <code>notebook.ipynb</code> is the name of the notebook you'd like to convert.</p> <p>This will create a python script called <code>notebook.py</code> which you can run from the command line or launch to run as a job on the SuperCloud.</p>","tags":["Jupyter","Tips and Best Practices","How To"]},{"location":"jupyter-notebook-conversion/#running-your-python-script-as-a-supercloud-job","title":"Running your python script as a SuperCloud job","text":"<p>In order to run your python script as a batch job on SuperCloud, you will need to create a launch script. In this example, the script is called <code>train.sh</code>\u00a0and we will load the Anaconda 2022b module:</p> <pre><code>#!/bin/bash\n\nmodule load anaconda/2022b\npython notebook.py\n</code></pre> <p>To launch your script, use the <code>LLsub</code> command:</p> <p><code>$ LLsub train.sh</code></p> <p>Please note that this is the basic instruction on how to convert and submit a Jupyter Notebook as a batch job.\u00a0This job as is will only request a single core.  If you need to use additional resources, you will need additional scheduler options. Please refer to the Submitting Jobs page for how to request additional resources.</p>","tags":["Jupyter","Tips and Best Practices","How To"]},{"location":"jupyter-notebooks-best-practices/","title":"Best Practices for Jupyter Notebooks","text":"<p>Here are some best practices that will help you be successful using Jupyter.</p>","tags":["Jupyter","Tips and Best Practices"]},{"location":"jupyter-notebooks-best-practices/#use-the-chrome-browser","title":"Use the Chrome browser","text":"<ul> <li>We've found that the Chrome browser is the best browser for Jupyter     Notebooks.</li> <li>Our Jupyter Notebooks may not\u00a0work with the Safari     browser.</li> </ul>","tags":["Jupyter","Tips and Best Practices"]},{"location":"jupyter-notebooks-best-practices/#shut-down-your-notebooks-when-you-are-done","title":"Shut down your notebooks when you are done","text":"<ul> <li>Notebooks on the SuperCloud take up computing resources. When you leave     Jupyter running, you are leaving resources occupied that could be     used by someone else.</li> <li>To shut down your notebook, click on the \"Shutdown\" button in the     top right of every Jupyter page, or click the \"Shutdown\" button on     the Jupyter launching     page.</li> <li>Make sure you save your notebooks and any important data before     shutting down!</li> </ul>","tags":["Jupyter","Tips and Best Practices"]},{"location":"jupyter-notebooks-best-practices/#save-frequently","title":"Save frequently","text":"<ul> <li>Save your notebook or text file by clicking File-&gt;Save or     CMD+S/Control+S.</li> <li>Save important data or results as files on the filesystem. Work     <code>save</code> or <code>write</code> commands into your code wherever you produce an     important result. In Python you can save in a text file, or use     <code>np.save()</code> or <code>np.savez()</code>.</li> <li>When choosing the appropriate method for your application, note that     both Numpy and HDF5 save the data in binary files which makes them     computationally efficient. However, if you need to interface with     scripts or code in a language other than Python, HDF5 is a standard     across C/C++, FORTRAN, MATLAB and Python, providing flexibility and     portability (please see information on HDF5 file locking     here). If you need human readable data     then csv format is your preferred option.</li> </ul>","tags":["Jupyter","Tips and Best Practices"]},{"location":"jupyter-notebooks-best-practices/#export-your-notebook-as-a-script","title":"Export your notebook as a script","text":"<ul> <li>When you\u00a0are done developing your code and need to run it     without making too many edits, export your notebook as a script.</li> <li>Save your notebook before exporting to get any recent changes.</li> <li>In python, you can type     <code>!jupyter nbconvert --to script &lt;notebook-name&gt;.ipynb</code> in a     code block to export from within the notebook. Be sure to remove     this line from your Python script before running it.</li> <li>See the page on converting your     notebook\u00a0for more information.</li> </ul>","tags":["Jupyter","Tips and Best Practices"]},{"location":"jupyter-notebooks-best-practices/#know-when-to-use-a-batch-job","title":"Know when to use a batch job","text":"<ul> <li>Jupyter notebooks are great for development and running interactive     analysis, but if you have long running code you may want to submit     that code as a batch job instead.</li> <li>If you find you are going through the notebook running all cells     without editing, or your notebook takes so long to run that you are     taking frequent coffee breaks, it may make more sense to convert     your notebook into a runable script that you can run from the     command line. Once you have a runable script, you can submit that     script as a job on the system, and it can run without your input. As     long as you are saving your results, you can collect those results     in the same directory that you run the notebook in.</li> <li>Batch jobs will run the code you submit and exit independent of user     input.</li> <li>Export your notebook as a script to use as a starting point for a     batch job.</li> <li>See this page for more information on     running batch jobs.</li> <li>You can use the Jupyter Terminal to submit batch jobs, or submit     from the login node through a terminal on your desktop.</li> </ul>","tags":["Jupyter","Tips and Best Practices"]},{"location":"jupyter-notebooks/","title":"Jupyter Notebooks","text":"<p>Jupyter notebooks are an interactive IDE environment. The environment allows you to start up notebooks in a variety of languages (Python, Matlab, Octave, Julia), terminal windows, and a simple text editor. You can also download and upload files to your home directory through this interface. This page will describe how to start up and use your own Jupyter instance and how to set environment variables for Jupyter.</p>"},{"location":"jupyter-notebooks/#starting-up-jupyter","title":"Starting Up Jupyter","text":"<p>When you start up Jupyter, the Jupyter instance gets submitted through the scheduler as a job. To do this, navigate to the following page:</p> <p>https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php</p> <p>To launch a notebook with default options, you can simply click on the \"Launch Notebook\" button.</p> <p>To see what the default options are or to change these options, click \"Show Advanced Launch Options\". A form will appear where you can select alternative options.</p> <ul> <li>Partitions: The partition the job is submitted to. You do not have     to change this option, it will adjust based on the resources you     select.</li> <li>CPU Type: The type of CPU you would like to launch to. You do not have     to change this option, it will adjust based on the resources you     select.</li> <li>GPU Resource Flag: If you would like to allocate GPUs to your     Jupyter instance you can select the GPU type here. Note your code or     packages must take advantage of GPUs, otherwise they will sit idle     if you request them.</li> <li>GPU Resource Count: If you select a GPU the number of GPUs to     allocate to your Jupyter instance.</li> <li>ncpu: The number of CPUs or cores allocated to your notebook.</li> <li>exclusive: Check this box if you would like exclusive use of a     compute node. You will be allocated all the CPUs on the node.</li> <li>Anaconda/Python Version: Select the Anaconda and/or Python version     that you want. Note some languages are not available on all Anaconda     versions. Refer to our How To pages to see which versions you should     select for the language you want to use.</li> <li>Application: Choose between Jupyter Notebook and Jupyter Lab. They     both have the same features but with different layouts. Jupyter     Notebook is the stable production application, Jupyter Lab is a beta     application. The application you choose is personal preference. In     this course we will be showing examples using Jupyter Notebook.</li> </ul> <p>Once you click on the \"Launch\" button, the scheduler launches your job, if the resources are available. When your Jupyter instance is ready, a link will appear on the page. This can take a minute or so.</p> <p>One very important thing to note is that once your Jupyter instance has launched, it will continue to run until you stop the job. This is particularly important if you are using a lot of resources when launching your job. Stopping the job can be done by clicking on the \"Shutdown\" button in the top right corner of the Jupyter interface page, by navigating back to the initial launch page and clicking the \"Shutdown\" button, or by using one of the scheduler commands (<code>LLkill</code>).</p>"},{"location":"jupyter-notebooks/#the-jupyter-environment","title":"The Jupyter Environment","text":"<p>When you first enter the Jupyter environment, you will see the files and directories in your home directory. You can navigate through these by clicking. By clicking the \"New\" button in the top right, you can:</p> <ul> <li>Start a new notebook (Julia, Python, Matlab, Octave, R)</li> <li>Open a terminal</li> <li>Create a new text file with a simple text editor</li> </ul> <p>In addition to these, you can upload and download data, and edit text files from Jupyter. Click on the checkbox next to the file you want to edit, and go to the top of the page and click \"Edit\". This will bring you to a simple text editor. For a bit of an intro to the Jupyter interface, see this page on Notebook Basics.</p>"},{"location":"jupyter-notebooks/#a-note-on-environment-variables-and-modules","title":"A Note on Environment Variables and Modules","text":"<p>Environment variables you have defined may not be set in the Jupyter environment, and modules you need may not be loaded. You can add these to the file <code>~/.jupyter/llsc_notebook_bashrc</code>, then shutdown and restart your Jupyter instance on the Jupyter portal (https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php). You can check that your environment is set the way you need it by opening a terminal and running the <code>env</code> command or by running the <code>module list</code> command to check which modules are loaded. Note that the Anaconda module is automatically loaded when starting up Jupyter, so you do not need to load it, and loading multiple Ananconda modules may cause unexpected behavior.</p>"},{"location":"launching-pmatlab-jobs/","title":"Launching pMatlab Jobs","text":"<p>On this page, we'll describe how to launch pMatlab jobs on the SuperCloud system. For information on submitting MATLAB\u00ae jobs and other types of jobs, see the Submitting Jobs page.</p> <p>To launch your pMatlab job use one of these commands:</p> <pre><code>eval(pRUN('myScriptName', Ncpus, 'grid&amp;'))\neval(pRUN('myScriptName', [Nnode Nppn Ntpp], 'grid&amp;'))\n</code></pre> <p><code>myScriptName</code> should not contain the \".m\" extension. See the section Triples Mode Launch below for details on requesting resources using <code>[Nnode Nppn Ntpp]</code>. Jobs submitted with only <code>Ncpus</code> are converted to Triples Mode during launch and you can have more control over how your processes are scheduled by using a triple instead of <code>Ncpus</code>.</p>","tags":["pMatlab","Submitting Jobs"]},{"location":"launching-pmatlab-jobs/#triples-mode-launch","title":"Triples Mode Launch","text":"<p>Triples Mode gives you better performance and more flexibility to manage memory and threads. We highly encourage users to migrate to this model.</p> <pre><code>eval(pRUN('myScriptName', [Nnode Nppn], 'grid&amp;'))\neval(pRUN('myScriptName', [Nnode Nppn Ntpp], 'grid&amp;'))\n</code></pre> <p>where:</p> <ul> <li><code>Nnode</code>: number of compute nodes</li> <li><code>Nppn</code>: number of MATLAB\u00ae/Octave processes per node</li> <li><code>Ntpp</code> (<code>OMP_NUM_THREADS</code>): number of OpenMP threads</li> </ul> <p><code>OMP_NUM_THREADS</code> is set to 1 by default.</p> <p>Note <code>myScriptName</code> should not contain the \".m\" extension.</p> <p>See the Triples Mode section on the Submitting Jobs page for additional details on Triples Mode, including how to tune your launch for the best performance.</p>","tags":["pMatlab","Submitting Jobs"]},{"location":"launching-pmatlab-jobs/#available-resources-for-pmatlab","title":"Available Resources for pMatlab","text":"<p>By default, pMatlab jobs require that the requested resources be available when you launch your job. You can check if the required resources are available with the <code>LLfree</code> command. If the resources are not available the job will not launch.</p> <p>If instead you would like your job to wait in the queue until the resources are available, you can set the <code>GRIDMATLAB_SRUN</code> environment variable prior to launch. For example:</p> <pre><code>setenv('GRIDMATLAB_SRUN','YES');\n\neval(pRUN('myScriptName', Ncpus, 'grid&amp;'))\neval(pRUN('myScriptName', [Nnode Nppn], 'grid&amp;'))\neval(pRUN('myScriptName', [Nnode Nppn Ntpp], 'grid&amp;'))\n</code></pre> <p>Note that <code>GRIDMATLAB_SRUN</code> is only valid for jobs that can be submitted as a batch job and does not support interactive jobs (using <code>grid</code> instead of <code>grid&amp;</code>).</p>","tags":["pMatlab","Submitting Jobs"]},{"location":"llcopy2tmp/","title":"LLcopy2tmp","text":"<p>The <code>LLcopy2tmp</code> command can be used to copy files to a temporary directory located on a compute node's local disk.\u00a0 Accessing files located on the local disk of a compute node is much faster than accessing files located on the shared Lustre file system or the file system where users' home directories are located.</p> <p>When your job is launched by the scheduler, the environment variable <code>$TMPDIR</code> is automatically set and exported.\u00a0 The value of <code>$TMPDIR</code> for each process is unique and references a temporary directory located on the compute node's local disk. While your job is running, your code and scripts can reference the environment variable <code>$TMPDIR</code> when it needs to access files in the temporary directory. See the page How to Use Environment Variables for instructions on how to use it in your code.</p> <p>When the job is complete, the temporary directories are automatically deleted.</p> <p>Usage: <code>LLcopy2tmp [-gCopyGroup] &lt;source1&gt; [source2 ...]</code></p> <p>This command will copy <code>&lt;source&gt;</code> and its contents to the local directory of the compute node pointed to by the TMPDIR environment variable.</p> <p><code>&lt;source&gt;</code> can be either a directory or a file. If it is a directory, then it will copy the directory and all of the files in the directory, but it will not copy subsequent directories within the <code>&lt;source&gt;</code> directory.</p> <p>This command will intelligently only copy once per node for the same CopyGroup, which will default to the environment variable <code>$SLURM_JOB_ID</code>.</p> <p>This command will dereference symlinks.</p> <p>The optional <code>-g</code> flag designates the group ownership that the directory and files will have once they are copied to <code>$TMPDIR</code>.</p>","tags":["Filesystem","Tips and Best Practices"]},{"location":"llcopy2tmp/#examples","title":"Examples","text":"<p>For each of the examples below, let's say we have directories and files in the following structure:</p> <ul> <li>~/mydata_dir<ul> <li>datafile1.dat</li> <li>datafile2.dat<ul> <li>mydata_subdir1</li> <li>datafile3.dat</li> <li>datafile4.dat</li> </ul> </li> <li>mydata_subdir2<ul> <li>datafile5.dat</li> <li>datafile6.dat</li> </ul> </li> </ul> </li> </ul> <p>and <code>TMPDIR=/state/partition1/user/userid1/</code></p>","tags":["Filesystem","Tips and Best Practices"]},{"location":"llcopy2tmp/#example-1","title":"Example 1","text":"<pre><code>LLcopy2tmp ~/mydata_dir/datafile1.dat\n</code></pre> <p>This will copy <code>datafile1.dat</code> to the local temporary directory at location <code>/state/partition1/user/userid1/datafile1.dat</code>/</p>","tags":["Filesystem","Tips and Best Practices"]},{"location":"llcopy2tmp/#example-2","title":"Example 2","text":"<pre><code>LLcopy2tmp ~/mydata_dir/\n</code></pre> <p>This will copy the <code>mydata_dir</code> directory, <code>datafile1.dat</code> and <code>datafile2.dat</code> files into <code>/state/partition1/user/userid1/</code> so that it will contain:</p> <ul> <li>/state/partition1/user/userid1/<ul> <li>mydata_dir<ul> <li>datafile1.dat</li> <li>datafile2.dat</li> </ul> </li> </ul> </li> </ul>","tags":["Filesystem","Tips and Best Practices"]},{"location":"llcopy2tmp/#example-3","title":"Example 3","text":"<pre><code>LLcopy2tmp ~/mydata_dir/mydata_subdir1 ~/mydata_dir/mydata_subdir2\n</code></pre> <p>This will copy the <code>mydata_subdir1</code> and <code>mydata_subdir2</code> directories and their file contents, into <code>/state/partition1/user/userid1/</code> so that it will contain:</p> <ul> <li>/state/partition1/user/userid1<ul> <li>mydata_subdir1<ul> <li>datafile3.dat</li> <li>datafile4.dat</li> </ul> </li> <li>mydata_subdir2<ul> <li>datafile5.dat</li> <li>datafile6.dat</li> </ul> </li> </ul> </li> </ul>","tags":["Filesystem","Tips and Best Practices"]},{"location":"llmapreduce/","title":"LLMapReduce","text":"","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#what-is-mapreduce","title":"What is MapReduce","text":"<p>MapReduce is a programming model for processing large data sets with a parallel, distributed algorithm on a set of distributed networked computers. It became popular with the Java community when Hadoop implemented the map-reduce parallel program model for Java.</p> <p>There are many workflows where multiple independent files are processed in one stage, followed by gathering the results and post-processing them in a second stage. This workflow is representative of \"loosely coupled\" processing, where the coupling comes from gathering the result files and is often handled using the MapReduce paradigm.</p> <p>To ease the burden on researchers and to provide good performance for the MapReduce Use Case on a community shared production machine, the SuperCloud Team created LLMapReduce.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#the-llmapreduce-command","title":"The LLMapReduce Command","text":"<p>The options included in the LLMapReduce API provide extensive flexibility. At its most basic, LLMapReduce executes a user script on all the files in a user supplied input directory. The results are saved to unique files in the output directory. If the user requires a 2nd phase to post-process the data, the user provided reducer script performs the gather task followed by the post-processing.</p> <p>LLMapReduce is language agnostic, and we provide examples for MATLAB\u00ae, Java and Python. LLMapReduce supports nested LLMapReduce calls and we provide an example of this along with the other examples.</p> <p>The LLMapReduce command scans the user-specified input directory and translates each individual file into a computing task for the user-specified application (noted as Mapper in the image below). Then, all of the computing tasks will be submitted to the SuperCloud systems for processing. If needed, the results can be post-processed by setting up a user-specified reduce task, which is dependent on the mapping task results. The reduce task will wait until all the results become available. </p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#benefits-of-llmapreduce","title":"Benefits of LLMapReduce","text":"<p>LLMapReduce can reduce startup cost (loading packages, setting up data if it's the same dataset, etc.) in two ways. First, say you need to train 160 models. You would normally have to wait for the first 16 GPUs to be available, and then as those complete, you'd have to wait for the next 16 to be available, etc., 10 times until you've trained all your models. LLMapReduce will allocate the 16 GPUs to you, train your first 16, and then train your next 16 within that same allocation. You've reduced the time your jobs are waiting to run by quite a bit, especially when the system is quite busy. This you get for free with LLMapReduce, you usually don't have to make any changes to your code at all, or some very minor changes.</p> <p>Another way that LLMapReduce can reduce startup cost is by using mimo (multiple input multiple output) mode, which we highly recommend. You make a few changes to your code so that instead of training a single model during each run, it will iterate through and train several (it would train 10 models in sequence from the example above) models during each run. In this case you only have to pay the startup cost of your code once. Depending on how much time this takes, you can really save a lot of time here. You can find an example of code modified to use mimo mode in the description of the <code>--apptype</code> option below.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#llmapreduce-examples","title":"LLMapReduce Examples","text":"<p>We offer a few basic examples in the Examples section on this page. You can also find more examples on the Where to Find Examples page.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#usage","title":"Usage","text":"<p>The most basic LLMapReduce requires 3 inputs:</p> <ul> <li>The name of the script to be run, specified using the <code>--mapper</code>     option</li> <li>The input file directory, specified using the <code>--input</code> option</li> <li>The output file directory, specified using the <code>--output</code> option</li> </ul> <p>You can display the full set of options by running <code>LLMapReduce -h</code>.</p> <pre><code>$ LLMapReduce\nUsage: LLMapReduce [options]\n\nOptions:\n\u00a0 --version\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 show program's version number and exit\n\u00a0 -h, --help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 show this help message and exit\n\u00a0 --np=NPROCS\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Number of processes to run concurrently. Either N or\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [Nnode,Nppn,Ntpp], where N (total number of\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 processes), Nnode (number of nodes), Nppn (number of\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 processes per node), Ntpp (number of threads per process,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1 is default). Without --ndata, all data will be evenly\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 distributed to the given number of processes.\n\u00a0 --ndata=NDATAPERTASK\u00a0 Number of input data to be processed per task for fine\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 control, The default value is one. You may want to use\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 the --np option instead if you want to distribute the\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 entire work load to the given nProcs processes by the\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --np option.\n\u00a0 --distribution=DATADIST\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Distribution rule for the data, block or cyclic.\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Default is block.\n\u00a0 --mapper=MYMAPPER\u00a0\u00a0\u00a0\u00a0 Specify the mapper program to execute.\n\u00a0 --input=INPATH\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify a path where the input files are or a file\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 containing the list of the input files.\n\u00a0 --output=OUTDIRPATH\u00a0\u00a0 Specify a directory path where the output files to be\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 saved.\n\u00a0 --prefix=PREFIXFILENAME\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify a string to be prefixed to output file name.\n\u00a0 --subdir=USESUBDIR\u00a0\u00a0\u00a0 Specify true if data is located at sub-directories.\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 All the data under the input directory will be scanned\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 recursively. The same sub-directory structure will be\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 maintained under the output directory. Default is\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 false.\n\u00a0 --ext=OUTEXTENSION\u00a0\u00a0\u00a0 Specify a file extension for the output. Default is\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 out. Use noext if no extension is preferred.\n\u00a0 --extSearch=SEARCHEXTENSION\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Specify a file extension when searching input files\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 with the --subdir option.\n\u00a0 --delimeter=OUTDELIMETER\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify a file extension delimeter for the output.\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Default is the dot(.)\n\u00a0 --exclusive=EXCLUSIVEMODE\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Turn on the exclusive mode (true/false). The default\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 is false.\n\u00a0 --reducer=MYREDUCER\u00a0\u00a0 Specify the reducer program to execute.\n\u00a0 --redargs=REDARGUMENTS\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 List of arguments to be passed to the reducer\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [optional].\n\u00a0 --redout=REDOUTFILENAME\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Output filename for the reducer [optional].\n\u00a0 --changeDepMode=DEPENDENCYMODE\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Change the dependency mode. By default, the reduce job\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 starts only when all mapper tasks are completed\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 successfully. The alternative behavior\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (--changeDepMode=true) lets the reduce job start when\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 the mapper job terminates regardless of its exit\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 status.\n\u00a0 --keep=KEEPTEMPDIR\u00a0\u00a0\u00a0 Decide whether or not to keep the temporary\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .MAPRED.PID dirctory (true/false). The default is\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 false.\n\u00a0 --apptype=APPLICATIONTYPE\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 If your application can take multiple lines of input\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 and output format, set apptype=mimo. By default, your\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 application takes one line of input and output (siso).\n\u00a0 --cpuType=CPUTYPE\u00a0\u00a0\u00a0\u00a0 Request compute nodes with a specific CPU type\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [optional].\n\u00a0 --gpuNameCount=GPUNAMECOUNT\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify the GPU name and number of counts to be used\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 for each task as GPU_NAME:COUNT. Currently each node\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 has 2 Volta (V100) units.\n\u00a0 --slotsPerTask=SLOTSPERTASK\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify the number of slots(cores) per task. Default\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 value is 1 [optional].\n\u00a0 --slotsPerTaskType=SLOTSPERTASKTYPE\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify how the number of slots(cores) per task be\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 applied. Default value is 1 [Map only], Other options\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 are 2 [Both Map and Reduce] and 3 [Reduce only].\n\u00a0 --reservation=ADV_RES_NAME\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Specify an advanced reservation name to which a job is\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 submitted (requires LLSC coordination).\n\u00a0 --tempdir=TEMPDIR\u00a0\u00a0\u00a0\u00a0 Specify a temporary directory which replaces the\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 default MAPRED.PID directory.\n\u00a0 --partition=PARTNAME\u00a0 Specify a partition name where the job is submitted\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to.\n\u00a0 --options=SCHEDOPTIONS\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 If you want to add additional scheduler options,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 define them with --options as a single string.`\n\n\u00a0 If you need any other options:\n\u00a0 \u00a0 please send your requirements to supercloud@mit.edu\n</code></pre>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#output-logs","title":"Output logs","text":"<p>The location of the output logs from your LLMapReduce job depends on how the job was submitted:</p> <ul> <li>Triples mode: if you launched your LLMapReduce job using triples     mode (<code>--np=[Nnode,Nppn,Ntpp</code>), the output logs will reside in the     directory <code>MAPRED.&lt;pid&gt;/logs</code> where <code>pid</code> is the process id number of     the <code>LLMapReduce</code> process. The log files from the individual     processes will reside in subdirectories in the <code>MAPRED.&lt;pid&gt;/logs</code>     directory. These subdirectories will be called    <code>p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;nodename&gt;</code> where <code>p&lt;start-pid&gt;-p&lt;end-pid&gt;</code> is the     range of process ids whose log files are contained in the     subdirectory, and <code>&lt;nodename&gt;</code> is the name of the compute node that     those processes ran on.</li> <li>Regular (non-triples) mode: if you launched your LLMapReduce job     without using triples mode (<code>--np=N</code>), the output logs will reside     in the directory <code>MAPRED.&lt;pid&gt;/logs</code> where <code>&lt;pid&gt;</code> is the process id     number of the <code>LLMapReduce</code> process.</li> <li>With <code>--tempdir</code> option: if you launched your LLMapReduce job with     <code>--tempdir=&lt;your-dir&gt;</code> where <code>&lt;your-dir&gt;</code> is a directory that you     choose, the output logs will reside in the directory     <code>&lt;your-dir&gt;/logs</code>.<ul> <li>If you launched without triples mode (<code>--np=N</code>), the output logs     will reside in <code>&lt;your-dir&gt;/logs</code>.</li> <li>If you launched with triples mode (<code>--np=[Nnode,Nppn,Ntpp</code>),     the output logs will reside in subdirectories in     <code>&lt;your-dir&gt;/logs</code> These subdirectories will be called     <code>p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;nodename&gt;</code> (see the Triples mode bullet     above for an explanation of the name of the subdirectories)</li> </ul> </li> </ul> <p>To summarize:</p> <ol> <li>If you launched with triples mode, your log files will be in     <code>MAPRED.&lt;pid&gt;/logs/p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;nodename&gt;</code>.</li> <li>If you launched in regular (non-triples) mode, your log files will     be in <code>MAPRED.&lt;pid&gt;/logs</code>.</li> <li>If you specified the <code>-tempdir</code> option, replace <code>MAPRED</code> with the name of the directory you specified.</li> </ol>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#environment-variables","title":"Environment Variables","text":"<p>If you use Triples Mode with the <code>--np</code> option (<code>--np=[Nnode,Nppn,Ntpp]</code>) to specify the number of processes to run, the following environment variables are available for your use.</p> <ul> <li><code>LLMR_NODE_ID</code> - the node number the process is running on: 0 &lt;=     <code>LLMR_NODE_ID</code> &lt; <code>Nnode</code></li> <li><code>LLMR_TASKS_PER_NODE</code> - the number of tasks/processes per node,     <code>Nppn</code></li> <li><code>LLMR_RANK</code> - the process number: 0 &lt;= <code>LLMR_RANK</code> &lt; <code>Nnode</code>\u00a0*     <code>Nppn</code></li> <li><code>LLMR_RANK_MIN</code> - the job's lowest process number running on a     particular node; <code>LLMR_NODE_ID</code> * <code>Nppn</code></li> <li><code>LLMR_RANK_MAX</code> - the job's highest process number running on a     particular node; ((<code>LLMR_NODE_ID</code> + 1) * <code>Nppn</code>) - 1</li> <li><code>LLMRID</code> - a unique identifier for each process running on a     particular node; 0 &lt;= <code>LLMRID</code> &lt;\u00a0<code>Nppn</code> - 1</li> <li><code>LLMR_LOG_SUBDIR</code> - the name of the subdirectory containing the     output log files; this directory name includes the range of process     ids and the name of the compute node on which the processes ran</li> <li><code>LLMR_LOG_DIR</code> - pathname to the subdirectory containing the output     log files</li> </ul> <p>In the example below, the triple [2,48,1] was used:</p> <ul> <li><code>LLMR_NODE_ID</code>: 0 or 1</li> <li><code>LLMR_TASKS_PER_NODE</code>: 48</li> <li><code>LLMR_RANK</code><ul> <li>0 - 47 for <code>LLMR_NODE_ID</code>=0</li> <li>48 - 95 for <code>LLMR_NODE_ID</code>=1</li> </ul> </li> <li><code>LLMR_RANK_MIN</code><ul> <li>0 for <code>LLMR_NODE_ID</code>=0</li> <li>48 for <code>LLMR_NODE_ID</code>=1</li> </ul> </li> <li><code>LLMR_RANK_MAX</code><ul> <li>47 for <code>LLMR_NODE_ID</code>=0</li> <li>95 for <code>LLMR_NODE_ID</code>=1</li> </ul> </li> <li><code>LLMRID</code>: 0 - 47</li> <li><code>LLMR_LOG_SUBDIR</code><ul> <li><code>p0-p47_c-15-4-1</code> for <code>LLMR_NODE_ID</code>=0</li> <li><code>p48-p95_c-15-3-4</code> for <code>LLMR_NODE_ID</code>=1</li> </ul> </li> <li><code>LLMR_LOG_DIR</code><ul> <li><code>./MAPRED.20418/logs/p0-p47_c-15-4-1</code> for <code>LLMR_NODE_ID</code>=0</li> <li><code>./MAPRED.20418/logs/p48-p95_c-15-3-4</code> for <code>LLMR_NODE_ID</code>=1</li> </ul> </li> </ul> <p>See our page, How to Use Environment Variables, for information on using environment variables.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#options","title":"Options","text":"","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-apptypeapplication-type","title":"--apptype=[APPLICATION-TYPE]","text":"<p>Specify the application type.</p> <p>APPLICATION-TYPE is one of the following:</p> APPLICATION-TYPE Description siso single input, single output (default) mimo multiple input, multiple output <p>By default, the <code>LLMapReduce</code> command invokes your application with just one input filename and one output filename at a time. A single process calls your application multiple times to process all of the input files that are assigned to it. This is referred to as the single input, single output (siso) model.</p> <p>In the multiple input, multiple output (mimo) model, multiple temporary files named <code>input_&lt;#&gt;</code> are automatically generated and written into your temporary MAPRED directory. Each of these files contains a list of filename pairs: an input data filename and its corresponding output filename. Each of the launched processes will be given, as an argument, the name of one of the <code>input_&lt;#&gt;</code> files. The number of these <code>input_&lt;#&gt;</code> is determined by the value of the <code>--np</code> or <code>--ndata</code> parameter that is used. See the Resource Limit Enforcement page for the default values for <code>--np</code> based on the cpu type.</p> <p>The advantage of using the mimo model is that your application is called and loaded just once to process all of its assigned input files. For example, if you are running MATLAB\u00ae code, the overhead of starting and stopping MATLAB\u00ae for each input file can become significant if you are processing a lot of data files.</p> <p>The <code>--apptype=mimo</code> option allows you to run your application in mimo mode, to eliminate the unnecessary overhead of repeated starts and stops of the application. The only caveat is that your application has to be modified slightly to be able to read a file and process the multiple lines of input output filename pairs in that file.</p> <p>This change is easy to implement in high level programming languages such as MATLAB\u00ae and Python.</p> <p>In your <code>~/examples/LLGrid_MapReduce/Java</code> directory, there is an example of an <code>LLMapReduce</code> job which uses the <code>--apptype=mimo</code> option. The Java code for this example is in the src subdirectory, in WordFrequenceCmdMulti.java. The code below shows the basic structure of an application that uses the <code>--apptype=mimo</code> option. The details of what the application does to count the words in the input file are left out here, but it would be the same code that is used in the single input, single output example.</p> <p>Instead of receiving a single input filename and a single output filename as arguments and processing just the single input file, <code>WordFrequencyCmdMulti.java</code> reads input and output filename pairs from a file and processes each input/output filename pair before exiting. To do this, we need a while loop to read the file containing the input output filename pairs, one line at a time, and then we process the input file the same way as we did in the single input, single output version.</p> <p></p> <ol> <li>The input parameter to the main function of the Word Frequency     Command Multi class is a string containing the name of a file which     contains a list of input and output filename pairs, and the name of     the reference words file. Without the <code>--apptype=mimo</code> option, the     input string would contain an input filename, an output filename,     and the reference word filename.</li> <li>We open the file that contains the input output filename     pairs.</li> <li>The while loop will read one line at a time from the     file.</li> <li>We parse the line that was just read from the file to get the input     and output filenames.</li> <li>We open the input, output, and reference word     files.</li> <li>Processing is done on the input file the same way it is done in the     single input, single output model.</li> <li>When the word counting on the input file is done, we need to reset     any counters or other temporary variables that were used in counting     the words for a single input file.</li> <li>This is the end of the processing loop. Go back and get another     input output filename pair from the     file.</li> <li>The application can exit when all of the input output filename     pairs have been processed.</li> </ol>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-changedepmodechange-dependency-mode","title":"--changeDepMode=[CHANGE-DEPENDENCY-MODE]","text":"<p>This option enables/disables the dependency mode, which controls whether the reduce job will start upon completion of the map job. By default, the launch of the reduce job is dependent upon successful exit status from the map job. Changing the dependency mode allows the reduce job to start regardless of the mapper exit status.</p> <p>CHANGE-DEPENDENCY-MODE is one of the following:</p> CHANGE-DEPENDENCY-MODE Description false don't change the dependency mode for starting the reduce job (default) true change the dependency mode to allow the reduce job to start regardless of mapper exit status","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-cputypecpu-type","title":"--cpuType=[CPU-TYPE]","text":"<p>Select the cpu type of the nodes that your application will run on. By default, the <code>LLMapReduce</code> command launches your application to run on the Xeon-P8 nodes.</p> <p>CPU-TYPE is one of the following:</p> CPU-TYPE Description xeon-p8 Intel Xeon Platinum\u00a0(default) xeon-g6 Intel Xeon Gold 6248","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-delimeteroutput-file-extension-delimiter","title":"--delimeter=[OUTPUT-FILE-EXTENSION-DELIMITER]","text":"<p>Specify the file extension delimiter for the output files.</p> <p>The <code>--delimeter</code> option allows you to specify the output file extension delimiter. By default, the delimiter is the dot character <code>.</code>.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-distributiondistribution-type","title":"--distribution=[DISTRIBUTION-TYPE]","text":"<p>Specify how data is distributed to the processes.</p> <p>DISTRIBUTION-TYPE is one of the following:</p> DISTRIBUTION-TYPE Description block block distribution (default) cyclic cyclic distribution <p>This example illustrates how 16 input files are allocated to processes when the distribution rule is block, and when the distribution rule is cyclic.</p> <p></p> <p>Block distribution divides the input files into contiguous blocks of files. If an <code>ndata</code> value is specified, each block will contain <code>ndata</code> number of files. If an <code>np</code> value is specified, the size of the block will be the total number of input files divided by the number processes. In this example, with 16 input files and <code>--np=4</code>, each process gets 4 input files. Process P1 gets files f1, f2, f3, and f4. Process P2 gets files f5, f6, f7, and f8, and so on.</p> <p>Cyclic distribution distributes the input files in a round robin fashion, similar to dealing a hand of cards. In this example with 16 input files and <code>--np=4</code>, each process gets 4 input files. Process P1 gets files f1, f5, f9 and f13. Process P2 gets files f2, f6, f10 and f14, and so on.</p> <p>When using the <code>--distribution</code> option, you must also provide either the <code>--np</code> option or the <code>--ndata</code> option. Without the <code>--np</code> or <code>--ndata</code> option, each process receives just 1 input file and the distribution option would be meaningless.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-exclusiveexclusive-mode","title":"--exclusive=[EXCLUSIVE-MODE]","text":"<p>Enable or disable exclusive use of a node.</p> <p>This option is used to enable exclusive use of a node to avoid interference with other jobs. By default, exclusive mode is disabled. When you enable exclusive mode, your job might not be dispatched right away if there aren't enough empty nodes.</p> <p>EXCLUSIVE-MODE is one of the following:</p> EXCLUSIVE-MODE Description false disable exclusive use of the node (default) true enable exclusive use of the node","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-extfile_extension","title":"--ext=[FILE_EXTENSION]","text":"<p>Specify a file extension for the output files, or request no file extension on the output files</p> <p>This option allows you to specify a file extension for the job's output files. The default is <code>out</code>.</p> <p>To specify that no file extension be used on the output files, use <code>--ext=noext</code>.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-extsearchsearch_extension","title":"--extSearch=SEARCH_EXTENSION","text":"<p>Specify a file extension for input file search by extension.</p> <p>This option allows you to specify a file extension when searching through subdirectories for input files. In order to use this option, you must also set <code>--subdir=true</code>.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-gpunamecountgpu-namecount","title":"--gpuNameCount=[GPU-NAME:COUNT]","text":"<p>Specify the GPU name and number of GPUs units to be used for each task.</p> <p>The COUNT value allows you to specify how many GPU units to use.</p> <p>GPU-NAME is the following:</p> GPU-NAME Description Valid COUNT values volta NVIDIA Volta V100 1 or 2","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-h-help","title":"-h, --help","text":"<p>Display a list of the <code>LLMapReduce</code> command options and a brief description of each, and then exit.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-inputinput-path","title":"--input=[INPUT-PATH]","text":"<p>Specify the path to the input files.</p> <p>The <code>--input</code> option is a required option and specifies the path where the input files are located. The specified path may point to a directory that contains all of the input files, or the path may point to the name of a file that contains a list of input files.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-keepkeep-temp-files","title":"--keep=[KEEP-TEMP-FILES]","text":"<p>Keep or remove temporary files upon job completion.</p> <p>KEEP-TEMP-FILES is one of the following:</p> KEEP-TEMP-FILES Description false delete temporary files upon job completion (default) true keep temporary files upon job completion <p>When you call the <code>LLMapReduce</code> command, many temporary files are created in a temporary subdirectory, located in the directory where you launched the <code>LLMapReduce</code> command. The name of this temporary subdirectory is <code>MAPRED.&lt;number&gt;</code>, where <code>&lt;number&gt;</code> is the id of the <code>LLMapReduce</code> process (which should not be confused with the job id that is assigned by the scheduler).</p> <p>The MAPRED directory contains scripts used to launch each process and output log files generated by your application. Whatever log output you would have seen when running your application in serial can be found in log files in the MAPRED directory. There will be one log file for each process.</p> <p>When your job completes, the default action is for <code>LLMapReduce</code> to delete the MAPRED directory and its contents. If you'd like to override this action for debugging purposes, you can use the <code>--keep=true</code> option. Once your code has been debugged and is in production mode, you should avoid keeping these temporary files around because they use a lot of disk space.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-mappermapper-program","title":"--mapper=[MAPPER-PROGRAM]","text":"<p>Specify the name of the mapper program.</p> <p>This required option specifies the name of the mapper program that will be launched.The mapper program file must have execute permission. Use the following command to add execute permission to a file in your home directory:</p> <p><code>$ chmod u+x &lt;file&gt;</code></p> <p>If the mapper file resides in a group shared directory and other members of the group will launch LLMapReduce jobs using the mapper program, the file will also need group execute permission. Use the following command to add execute permission to a file in a group shared directory:</p> <p><code>$ chmod ug+x &lt;file&gt;</code></p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-ndatanum-input-files-per-process","title":"--ndata=[NUM-INPUT-FILES-PER-PROCESS]","text":"<p>Specify the number of input files handled per process.</p> <p>The <code>--ndata</code> option allows you to specify the maximum number of input files to be handled by each process. The default value is 1, resulting in 1 input file per process. However, if the <code>--np</code> option is used without the <code>\u2011\u2011ndata</code> option, the input files will be evenly distributed to the processes.</p> <p>The <code>--ndata</code> option is useful in cases where an application might crash due to bugs or bad data. In this case, you might want to have fewer data files assigned to each process, in case the process terminates early and can't process all of the input files that were assigned to it. This would minimize the number of unprocessed files in your job submission.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-npnum-processes","title":"--np=[NUM-PROCESSES]","text":"<p>Specify the number of processes to launch.</p> <p>The <code>\u2011\u2011np</code> option allows you to specify the number of processes that will be launched to run your application. You can specify just the total number of processes you want to use and allow the scheduler to determine how to distribute the processes among the nodes, or you can use Triples Mode\u00a0to specify how you would like the processes to be distributed across the nodes.</p> <p>To specify the number of processes to launch, use one of the following <code>\u2011\u2011np</code> option formats:</p> <ul> <li>Let the scheduler distribute the processes: <code>\u2011\u2011np=N</code>, where <code>N</code> = total number of processes.</li> <li> <p>Use Triples Mode to specify how the processes     will be distributed: <code>\u2011\u2011np=[Nnode,Nppn,Ntpp]</code>, where:</p> <ul> <li><code>Nnode</code>\u00a0= number of nodes</li> <li><code>Nppn</code>\u00a0= number of processes per node</li> <li><code>Ntpp</code> = number of threads per process (default is 1)</li> </ul> <p>Entries are comma-separated, should be enclosed in brackets <code>[]</code>, and there should not be any spaces separating the entries.</p> <p>You can find additional information on triples mode job launches (including tips on how to tune your triples for best performance) on the Triples Mode page.</p> </li> </ul> <p>Without also using the <code>--ndata</code> option, all data will be evenly distributed to the given number of processes.</p> <p>The input files will be evenly distributed to the processes. By default, the number of processes to launch is set to your default allotment (see the Resource Enforcement page for the default allotments based on cpu type).</p> <p><code>--np</code> is not a required option. If you don't specify the number of processes, the number of processes that will be launched to run your application is equal to the default number of processes that you are allocated for the requested CPU type (see the Resource Limits page for the default allocations by CPU type).</p> <p>If you specify a number of processes that is greater than the number that you're allotted by default, the extra processes will be queued, waiting for your other processes to finish. After a process terminates, one of the queued processes will be launched.</p> <p>We strongly recommend using the default setting or a value that is less than the default setting. Setting the number of processes to a value higher than your default allotment results in a great deal of overhead for the scheduler.</p> <p>The <code>\u2011\u2011np</code> and <code>\u2011\u2011ndata</code> options are mutually exclusive. If both options are used together, the <code>np</code> value will be ignored and the number of processes launched will be the number of input files divided by the <code>--ndata</code> value.</p> <p>This example illustrates how 16 input files are allocated to processes in 3 different scenarios: when not using the <code>--np</code> and <code>--ndata</code> options; when using <code>--np=3</code>; and when using <code>--ndata=3</code>.</p> <p></p> <p>When neither option is specified, the number of processes launched is the same as the number of input files, resulting in one input file per process.</p> <p>With <code>--np=3</code>, the scheduler launches 3 processes and distributes the 16 input files evenly among the 3 processes. When the number of input files is not evenly divisible by the number of processes, the lower numbered processes handle the leftover input files. In this example, the single leftover input file is handled by process 1.</p> <p>With <code>--ndata=3</code>, 6 processes will be launched. The number of processes is the smallest integer value greater than, or equal to the number of input files divided by the <code>ndata</code> value. If the number of input files is not evenly divisible by the <code>ndata</code> value, the highest numbered process handles the leftover input files.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-optionsscheduler-options","title":"--options=[SCHEDULER-OPTIONS]","text":"<p>Specify additional scheduler options.</p> <p>This option is used to pass additional option(s) to the Slurm scheduler. The additional scheduler option(s) string should be enclosed within single or double quotes. Please refer to the Scheduler Commands page and the specific Slurm Documentation pages for information on additional scheduler options.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-outputoutput-path","title":"--output=[OUTPUT-PATH]","text":"<p>Specify the path to the output files.</p> <p>The <code>--output</code> option is a required option and specifies the path where the output files should be written.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-partitionpartition-name","title":"--partition=[PARTITION-NAME]","text":"<p>Specify the name of the partition to submit the job to. The default partition name is determined by the cpu type that is selected.</p> CPU-TYPE PARTITION-NAME xeon-p8 xeon-p8 (default) xeon-g6 xeon-g6-volta","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-prefixprefix-text","title":"--prefix=[PREFIX-TEXT]","text":"<p>Specify a text string to pre-pend to the output file names.</p> <p>This option specifies the name of the reducer program that will be launched when all of the mapper jobs have completed.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-redargsreducer-args","title":"--redargs=[REDUCER-ARGS]","text":"<p>Specify a list of arguments for the reducer.</p> <p>This option allows you to pass arguments to your reducer script. The parameters list is a single string and should be enclosed within a set of double quotes.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-redoutreducer-output-file-name","title":"--redout=[REDUCER-OUTPUT-FILE-NAME]","text":"<p>Specify the name of the reducer output file.</p> <p>This option specifies the reducer output filename. The default output filename is <code>llmapreduce.out</code>. If a path is not specified in <code>REDUCER-OUTPUT-FILE-NAME</code>, the reducer output file will be written to the current working directory.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-reducerreducer-program","title":"--reducer=[REDUCER-PROGRAM]","text":"<p>Specify the name of the reducer program.</p> <p>This option specifies the name of the reducer program that will be launched when all of the mapper jobs have completed. The reducer program file must have execute permission. Use the following command to add execute permission to a file in your home directory:</p> <p><code>$ chmod u+x &lt;file&gt;</code></p> <p>If the reducer file resides in a group shared directory and other members of the group will launch LLMapReduce jobs using the reducer program, the file will also need group execute permission. Use the following command to add execute permission to a file in a group shared directory:</p> <p><code>$ chmod ug+x &lt;file&gt;</code></p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-reservationadvanced_reservation_name","title":"--reservation=[ADVANCED_RESERVATION_NAME]","text":"<p>Specify the name of the advanced reservation for the job.</p> <p>An advanced reservation is useful if you have a demo scheduled and would like to ensure that there will be resources available for your job at the time of your demo.</p> <p>This option requires that an advanced reservation be created by the SuperCloud team before the job is submitted.</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-slotspertasknum-slots","title":"--slotsPerTask=[NUM-SLOTS]","text":"<p>Specify the number of slots used by each task.</p> <p>This option specifies the number of slots (cores) used by each task. The default value is 1. If your jobs require more than the per-slot memory limit (see table below) you may need to request additional slots for your jobs so that they can run to completion. Please note that requesting additional slots for your job will reduce the number of tasks that you can run simultaneously. For example, if you require 2 slots per task and your allotment of cores is 256, you will be able to run only 128 tasks simultaneously.</p> NodeNode Per-Slot memory Limit (GB) Intel Xeon P8 4 Intel Xeon G6 9","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-slotspertasktypeslots-per-task-type","title":"--slotsPerTaskType=[SLOTS-PER-TASK-TYPE]","text":"<p>Specify which stage (map or reduce) of the LLMapReduce job the number of slots assigned per task should be applied.</p> SLOTS-PER-TASK-TYPE Value When slotsPerTask value is applied 1 During Mapper stage only 2 During Mapper and Reducer stages 3 During Reducer stage only","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-subdirsearch-subdirs","title":"--subdir=[SEARCH-SUBDIRS]","text":"<p>Enable or disable recursive search of the input directory.</p> <p>If your input files reside in a multi-level directory structure, the <code>--subdir=true</code> option will enable recursive searching of the specified input directory. All files in the subdirectories will be treated as input files. By default, this capability is disabled.</p> <p><code>SEARCH-SUBDIRS</code> is one of the following:</p> SEARCH-SUBDIRS Description false disable recursive searching of the input directory (default) true enable recursive searching of the input directory","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#-tempdirtempdir","title":"--tempdir=[TEMPDIR]","text":"<p>Specify the name of the temporary directory to generate (instead of <code>MAPRED.PID</code>).</p>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"llmapreduce/#examples","title":"Examples","text":"<p>To use LLMapReduce to launch a parameter sweep, follow these steps:</p> <ol> <li>Modify your code to accept input parameters</li> <li>Create a mapper script to launch your code</li> <li>Create a text file with the desired sets of parameters</li> <li>Submit to the scheduler using the LLMapReduce command</li> </ol> <p>In this example, we will run python code in the file <code>mnist_cnn.py</code>, which expects 3 input parameters: batch, epochs and rate.</p> <p>The following is an example of how we would run the python code with 1 set of input parameters:</p> <p><code>$ python mnist_cnn.py --batch=64 --epochs=1 --rate=0.001</code></p> <p>Here is the mapper script, <code>drv_keras.sh</code>, that will be used to launch the python code. Note that <code>$1 $2 $3</code> is used to represent the 3 input parameters being passed to the python code.</p> drv_keras.sh<pre><code>#!/bin/bash\nsource /etc/profile\nmodule load anaconda/2020b\npython mnist_cnn.py $1 $2 $3\n</code></pre> <p>Here are the contents of an example parameters file, <code>param_sweep.txt</code>, which is passed in as the LLMapReduce input file:</p> param_sweep.txt<pre><code>--batch=64\u00a0 --epochs=1000 --rate=0.001\n--batch=128 --epochs=500\u00a0 --rate=0.002\n--batch=256 --epochs=200\u00a0 --rate=0.003\n--batch=512 --epochs=1100 --rate=0.004\n</code></pre> <p>Here is the LLMapReduce command to submit the job to the scheduler, for running on a single Volta GPU:</p> <pre><code>LLMapReduce --mapper=./drv_keras.sh --gpuNameCount=volta:1 --input=param_sweep.txt --output=results --keep=true\n</code></pre> <p>Additional LLMapReduce examples can be found in these locations:</p> <ul> <li>In the user's SuperCloud home directory: <code>~/examples/LLGrid_MapReduce</code></li> <li>In the <code>/usr/local/examples</code> directory on SuperCloud system nodes. This     directory contains the latest version of the examples.</li> <li>The <code>/home/gridsan/groups/bwedx/teaching-examples</code> directory contains a number of examples, including a few for LLMapReduce.</li> <li>The <code>/home/gridsan/groups/bwedx/Practical_HPC_2022/LLMapReduce</code> directory contains a number of examples you try out for yourself, solutions are in the <code>solutions</code> subdirectory.<ul> <li>There is a walkthrough of some of these examples in the Practical HPC Course.</li> </ul> </li> </ul>","tags":["Submitting Jobs","LLMapReduce"]},{"location":"monitoring-system-and-jobs/","title":"Monitoring System and Job Status","text":"<p>The four actions you may take the most are checking system status and starting, monitoring, and stopping jobs. Since scheduling jobs is a longer topic, see this page for an in-depth description of how to start your job. Here we describe how to check the status of the system for available resources, monitor a currently running job), and stop a running job.</p> <p>Each of these tasks is done through the scheduler, which is Slurm on the MIT SuperCloud system. On this page and the job submission page we describe some of the basic options for submitting, monitoring, and stopping jobs. More advanced options are described in Slurm's documentation, and this handy two-page guide\u00a0gives a brief description of the commands and their options.</p>"},{"location":"monitoring-system-and-jobs/#checking-system-status","title":"Checking System Status","text":"<p>Our wrapper command, <code>LLGrid_status</code>, has a nicely formatted and easy to read output for checking system status:</p> <pre><code>[StudentX@login-0 ~]$ LLGrid_status\nLLGrid: txe1 (running slurm 16.05.8)\n============================================ \nOnline Intel xeon-e5 nodes: 36\nUnclaimed nodes: 24\nClaimed slots: 172\nClaimed slots for exclusive jobs: 80\n-------------------------------------------- \nAvailable slots: 404\n</code></pre> <p>In the output, you can see the name of the system you are on (e1 here), the scheduler that's being used (Slurm), the number of unclaimed nodes, and the number of available slots.</p>"},{"location":"monitoring-system-and-jobs/#monitoring-jobs","title":"Monitoring Jobs","text":"<p>To list all of your running jobs you can use the <code>LLstat</code> command. For more information about how your jobs are utilizing the resources, you can use the <code>LLload</code> command.</p>"},{"location":"monitoring-system-and-jobs/#llstat","title":"LLstat","text":"<p>You can monitor your jobs using the <code>LLstat</code> command:</p> <pre><code>[StudentX@login-0 ~]$ LLstat   LLGrid: txe1 (running slurm 16.05.8)\nJOBID\u00a0\u00a0\u00a0\u00a0 ARRAY_J\u00a0\u00a0\u00a0 NAME\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 USER\u00a0\u00a0\u00a0\u00a0START_TIME\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 PARTITION\u00a0 CPUS\u00a0 FEATURES\u00a0 MIN_MEMORY\u00a0 ST\u00a0 NODELIST(REASON)   \n40986\u00a0\u00a0\u00a0\u00a0 40986\u00a0\u00a0\u00a0\u00a0\u00a0 myJob\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Student\u00a0 2017-10-19T15:35:46 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2  \n40980_100 40980\u00a0\u00a0\u00a0\u00a0\u00a0 myArrayJob\u00a0Student\u00a0 2017-10-19T15:35:37 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2  \n40980_101 40980\u00a0\u00a0\u00a0\u00a0\u00a0 myArrayJob Student\u00a0 2017-10-19T15:35:37 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2  \n40980_102 40980\u00a0\u00a0\u00a0\u00a0\u00a0 myArrayJob Student\u00a0 2017-10-19T15:35:37 normal\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0 xeon-e5\u00a0\u00a0 5G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 R\u00a0\u00a0 gpu-2\n</code></pre> <p>The output of the LLstat command lists the job IDs of the jobs running, their names, the start time, the number of cpus per task, its status, and the node that it is running on. If it is in error state, it lists that as well.</p>"},{"location":"monitoring-system-and-jobs/#llload","title":"LLload","text":"<p>For more information about how your jobs are utilizing the resources, you can use the <code>LLload</code> command.</p> <p>The nodes in the main partitions on SuperCloud are exclusive by user, meaning nodes will be shared between different users, but multiple jobs from the same user can run together on the same node. This makes it very easy to examine how well your jobs are using the resources on the node, without jobs from others influencing those numbers.</p> <p>To more view these statistics, we've introduced the LLload command that you can use to evaluate the efficiency of your jobs. If you run:</p> CPU Only JobsGPU Jobs <p><pre><code>LLload\n\nLLGrid: SuperCloud(TXE1)\nUsername: studentx, Nodes used: 4\nHOSTNAME CORES -  USED =  FREE    LOAD MEMORY -   USED =   FREE\nd-4-13-1    48 -    48 =     0   27.09  192GB -   82GB =  110GB\nd-6-3-1     48 -    48 =     0    7.43  192GB -   20GB =  172GB\nc-16-13-4   48 -    48 =     0    9.23  192GB -   17GB =  175GB\nc-17-13-3   48 -    48 =     0    7.68  192GB -   67GB =  125GB\n</code></pre> this command lists all of the nodes that you have jobs running on, how many of the cores on those nodes you have allocated, and some statistics about how the resources on those nodes are being used:</p> <ul> <li>CPU Load: how much of the CPUs are used (5 minute average)<ul> <li>Target: 50-150% of the number of CPUs (24.0-72.0 for the Xeon-P8 CPU nodes and 20.0-60.0 for the Xeon-G6 GPU nodes)</li> </ul> </li> <li>Memory Utilization: how much memory you are using, plus memory used for caching</li> </ul> <p>A good target for CPU Load is 50-150% of the number of CPUs. For example, that is 24-48 for the Xeon-P8 CPU nodes and 20-40 for the Xeon-G6 GPU nodes, see the Systems and Software page for current core counts. If this number is lower, it's likely you could take advantage of more resources on the node. If you find the load numbers are very high, you risk the chance of speeddown or even overwhelming the node. You have a few knobs to turn to adjust your cpu utilization, often this is by either by changing the number of threads used by your application or by running more jobs or processes per node. Adjusting these is very easy if you are submitting your job with Triples Mode, which we highly encourage for those jobs that support it.</p> <p>Since the memory utilization includes some additional values you will need to ssh to the node and run <code>htop</code> to see your true memory utilization, or use the <code>sacct</code> command after the job has completed to get the peak memory utilization (see this page).</p> <p><pre><code>LLload -g\n\nLLGrid: SuperCloud(TXE1)\nUsername: studentx, Nodes used: 2\nHOSTNAME CORES -  USED =  FREE    LOAD MEMORY -   USED =   FREE  GPUS -  USED = FREE LOAD GPUMEM -  USED =  FREE\nd-10-10-1    40 -    40 =     0    2.71  384GB -   48GB =  336GB     2 -     2 =    0 0.40   64GB -   4GB =  60GB\nd-13-12-1    40 -    40 =     0    0.22  384GB -   49GB =  335GB     2 -     2 =    0 0.40   64GB -   4GB =  60GB\n</code></pre> this command lists all of the nodes that you have jobs running on, how many of the cores on those nodes you have allocated, and some statistics about how the resources on those nodes are being used:</p> <ul> <li>CPU Load: how much of the CPUs are used (5 minute average)<ul> <li>Target: 50-150% of the number of CPUs (24.0-72.0 for the Xeon-P8 CPU nodes and 20.0-60.0 for the Xeon-G6 GPU nodes)</li> </ul> </li> <li>Memory Utilization: how much memory you are using, plus memory used for caching</li> <li>GPU Utilization: how much of both GPUs are being used, 2.0 is 100% of both GPUs (snapshot)<ul> <li>Target: 50-100% of the GPUs allocated (0.5 or higher for 1 GPU, 1.0 or higher for 2 GPUs)</li> </ul> </li> <li>GPU Memory: how much of the GPU memory is being used</li> </ul> <p>A good target for CPU Load is 50-150% of the number of CPUs. For example, that is 24-48 for the Xeon-P8 CPU nodes and 20-40 for the Xeon-G6 GPU nodes, see the Systems and Software page for current core counts. If this number is lower, it's likely you could take advantage of more resources on the node. If you find the load numbers are very high, you risk the chance of speeddown or even overwhelming the node. You have a few knobs to turn to adjust your cpu utilization, often this is by either by changing the number of threads used by your application or by running more jobs or processes per node. Adjusting these is very easy if you are submitting your job with Triples Mode, which we highly encourage for those jobs that support it.</p> <p>Since the memory utilization includes some additional values you will need to ssh to the node and run <code>htop</code> to see your true memory utilization, or use the <code>sacct</code> command after the job has completed to get the peak memory utilization (see this page).</p> <p>The GPU load is normalized such that 100% utilization on both GPUs will give a value of 2, so if both GPUs are well utilized you'll see a value close to 2. Note that this is derived from an instantaneous value rather than averaged over a period of time, so you may have to run it a few times to get a good idea of your GPU utilization. If you find your GPU utilization is low, check out our page on Optimizing your GPU Usage. You can see these numbers broken down by GPU and more information by adding the <code>--detail</code> flag:</p> <pre><code>LLload -g --detail\n</code></pre>"},{"location":"monitoring-system-and-jobs/#stopping-jobs","title":"Stopping Jobs","text":"<p>Jobs can be stopped using the <code>LLkill</code> command. You specify the list of job IDs, separated by commas that you would like to stop, for example:</p> <p><code>LLkill 40986,40980</code></p> <p>Stops the jobs with job IDs 40986 and 40980. You can also use the <code>LLkill</code> command to stop all of your currently running jobs:</p> <p><code>LLkill -u USERNAME</code></p>"},{"location":"online-courses/","title":"Online Courses","text":""},{"location":"online-courses/#some-available-online-courses","title":"Some Available Online Courses","text":"<p>Practical HPC: An introductory course that:</p> <ul> <li>Includes an introduction to HPC, canonical HPC Workflows, and the     SuperCloud system.</li> <li>Walks you through setting up your account, installing software,     running your first test job, submitting your first batch job.</li> <li>Describes how to scale up efficiently and measure your performance.</li> </ul> <p>Mathematics of Big Data and Machine Learning: Available through OCW.</p>"},{"location":"online-courses/#accessing-the-llx-online-course-site","title":"Accessing the LLx Online Course Site","text":"<p>Navigate to LLx and follow the instructions below to create an account, or click \"Sign In\" if you have an account. Click the \"Courses\" tab to register for courses. Once you have logged in hover over your username in the top right corner and click on the \"Dashboard\" link to access the courses you are enrolled in.</p>"},{"location":"online-courses/#creating-an-online-course-account","title":"Creating an Online Course Account","text":"<p>Below, you will find instructions on registering for an online course account:</p> <ol> <li>To sign up for an LLx account, go to the the LLx     Platform</li> <li>In the upper right corner, click on \"Register\".</li> <li>Complete the Registration Form to create your LLx account. Note, the     following items are required:<ul> <li>A valid and accessible email address</li> <li>Your name</li> <li>A public username of your choosing, it cannot include spaces</li> <li>A password, you can change this later</li> </ul> </li> <li>Once you have completed the registration form, click on the button     to create your account.</li> <li>Once you click on the button to create your account, you should see     a message stating that an activation email has been sent to your     email address.</li> <li>Check your email. When you receive the activation email, click on     the link to activate your account. Note you will not be able to log     back into the course site if you have not activated your account.</li> <li>Once you have activated your account you can register for courses.</li> </ol> <p>NOTE: If you forget your login password you may request a new password by clicking on \"Need help logging in?\" to the right above the password box. A password will be sent to your specified email. Follow the instructions for changing your password.</p>"},{"location":"online-courses/#questions","title":"Questions?","text":"<p>If you have any questions about our online courses or are having trouble with the platform, please contact us at\u00a0llx-help@mit.edu. If you have any questions about the MIT SuperCloud System, contact us at supercloud@mit.edu.</p>"},{"location":"pmatlab-getting-started/","title":"Getting Started with pMatlab","text":"<p>pMatlab was created at MIT Lincoln Laboratory to provide easy access to parallel computing for engineers and scientists using the MATLAB\u00ae language. pMatlab provides the interfaces to the communication libraries necessary for distributed computation. In addition to MATLAB\u00ae, pMatlab works seamlessly with Octave, an open source MATLAB\u00ae toolkit. This page provides an overview on how to create pMatlab code.</p> <p>This page focuses on converting your serial code to pMatlab code. Once you have pMatlab code you should review the following guides to launching pMatlab jobs on the SuperCloud systems:</p> <ul> <li>Launching Jobs<ul> <li>Launching Production Runs</li> </ul> </li> <li>Troubleshooting Checklist: pMatlab</li> </ul>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#creating-pmatlab-codes-the-basics","title":"Creating pMatlab Codes: The Basics","text":"<p>To get started converting serial MATLAB\u00ae or Octave codes into parallel MATLAB\u00ae code using pMatlab, we recommend our online course, Practical HPC. The PGAS Example: pMatlab Implementation section, within the Distributed Applications module provides a detailed introduction, working step-by-step through the Parameter Sweep application that is in the examples directory in your home directory. It should take approximately 30-45 minutes to work through the section.</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#creating-pmatlab-codes-single-program-multiple-data-considerations","title":"Creating pMatlab Codes: Single Program Multiple Data Considerations","text":"<p>The programming model used by pMatlab is \"Single Program, Multiple Data\" (SPMD) which means that every process of your application is executing the same commands (program) but on different data. While the use of multiple processes speeds up the computation, it has significant impact on any I/O that your program executes. Some common concerns and the appropriate parallel programming techniques include:</p> <ul> <li>I/O Considerations</li> <li>Creating Unique Random Numbers</li> <li>Using Global Variables</li> </ul>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#io-considerations-saving-a-matlab-workspace-or-variable","title":"I/O Considerations: Saving a MATLAB\u00ae workspace or variable","text":"<p>When saving your workspace, you need to remember that each processor has a copy of each variable and a distinct workspace. To ensure that you properly save all the data and don't overwrite the data on one processor with that from a second processor you need to:</p> <ul> <li>Save the workspace of each processor into a distinct file</li> <li>Differentiate between the values of variables on separate     processors</li> </ul> <p>The easiest way to do this is to tag variables with the label \"local\", e.g. myVar_local, and tag the files with the processor ID (Pid) which is unique. If the data is uniquely tagged it will be possible to reconstruct the complete workspace and data structures in a post-processing step.</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#saving-distributed-workspaces","title":"Saving Distributed Workspaces","text":"<p>The example code snippet below saves the entire workspace of the processor into a file called <code>output.&lt;pid&gt;.mat</code>.</p> <pre><code>filename = ['output.' num2str(Pid) '.mat'];\nsave(filename);\n</code></pre>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#saving-a-distributed-variable","title":"Saving a Distributed Variable","text":"<p>To save individual variables within the workspace follow the template code block shown below.</p> <pre><code>filename = ['output.' num2str(Pid) '.mat'];\nsave(filename, 'variable1', 'variable2', ..., 'variableN');\n</code></pre> <p>If you do not specify a path for your output data, it will be saved in the directory where you ran the code, generally the current working directory.</p> <p>Note: the same rules apply when writing out data files from all processes, each file must have a distinct file name. The easiest way to accomplish this is to use the process id, Pid, as a unique identifier.</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#creating-unique-random-numbers","title":"Creating Unique Random Numbers","text":"<p>The Random Number Generator in MATLAB\u00ae has been designed to start from a set value every time MATLAB\u00ae is restarted. Since a pMatlab job starts a new MATLAB\u00ae process on a remote processor, each MATLAB\u00ae that is part of your job will start with the same value. In general, in order to get good statistics you want to start from different values on each processor. To accomplish this, you want to seed the random number generator so that each process has a different set of random numbers. MATLAB\u00ae has changed the recommended methods for achieving the generation of different random numbers and we recommend checking the MATLAB\u00ae documentation when modifying your random number generation routines. (See Mathworks document: \"Generate Random Numbers That Are Different\")</p> <p>For example, the following code segment can be used with pMatlab in order to generate different seed on each processor.</p> <pre><code>state = sum( (Pid + 1)*clock*100. );\nrng(state);\n</code></pre> <p>More details are available at the following web page:</p> <p>http://www.mathworks.com/help/matlab/examples/controlling-random-number-generation.html</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#using-global-variables","title":"Using Global Variables","text":"<p>Global variables are permitted in pMatlab code, but remember that in this distributed memory model a variable that is global to the code is local to the processor and each instance of the global variable can have a different value on each processor which is running your code. Great care should be taken when using global variables in parallel, you may want to explicitly set the value somewhere or read it into from a file to ensure consistency.</p> <p>Also, note that when you clear global variables in your code, through the use of \"clear all\" or \"clear global\" commands, all of the pMatlab library commands will also be cleared causing unpredictable behavior in your parallel environment.</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#how-to-run-a-pmatlab-job","title":"How to Run a pMatlab Job","text":"<p>For information on how to launch your pMatlab job to run on the SuperCloud system, see the Launching pMatlab Jobs page.</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-getting-started/#troubleshooting-pmatlab-problems","title":"Troubleshooting pMatlab Problems","text":"<p>If you run into problems launching your pMatlab jobs, or you are getting errors during job execution, see this page on Troubleshooting pMatlab Jobs.</p>","tags":["pMatlab","Getting Started"]},{"location":"pmatlab-job-errors/","title":"pMatlab Job Errors","text":"<p>All standard error output from pMatlab jobs will go into the file <code>MatMPI/pRUN_Parallel_Wrapper.err</code>.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-errors/#finding-the-output-file-associated-with-the-error","title":"Finding the output file associated with the error","text":"<p>Each error line in the file is prefixed with the Pid where the error occurred (see example below):</p> <pre><code>$ cat MatMPI/pRUN_Parallel_Wrapper.err\nPid=1: Undefined function or variable 'dis'.\nPid=1: Error in pSUCCESS (line 21)\nPid=1: \u00a0\u00a0\u00a0dis(['Pid = ' num2str(Pid)]);\nPid=1: Error in pRUN_Parallel_Wrapper (line 25)\nPid=1: eval(m_file);\n. . . \n</code></pre> <p>You can use the Pid to identify which .out file to look in to investigate the reported error.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-errors/#where-to-find-the-out-files","title":"Where to find the .out files","text":"<p>For jobs submitted using triples mode [Nnode Nppn Ntpp]:\u00a0\u00a0MATLAB\u00ae and Octave <code>*.out</code> files are written to one or more subdirectories:</p> <pre><code>./MatMPI/p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;compute-node-name&gt;/&lt;pMatlab-script-name&gt;.&lt;pid&gt;.out\n</code></pre> <p>where:</p> <ul> <li><code>p&lt;start-pid&gt;</code>\u00a0is the id of the first process running on the compute node whose name is\u00a0<code>&lt;compute-node-name&gt;</code></li> <li><code>p&lt;end-pid&gt;</code>\u00a0is the id of the last process running on the compute node whose name is\u00a0<code>&lt;compute-node-name&gt;</code></li> </ul> <p>If your job was run on multiple compute nodes, the log files will be spread across multiple subdirectories</p> <p>In the example above, the reported Pid is 1, so you would look in the file <code>&lt;pMatlab_script_name&gt;.1.out</code> to investigate the source of the errors.</p> <p>More information on where to find pMatlab output can be found on the Finding pMatlab Output page.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-errors/#finding-the-compute-node-associated-with-a-process","title":"Finding the compute node associated with a process","text":"<p>Sometimes a compute node may get into a failure state, so it may be\u00a0useful to know which compute node a failed process ran on.\u00a0 You can find the name of the compute node where the process was executed by looking in the file <code>&lt;pMatlab_script_name&gt;.&lt;Pid&gt;.out</code>. Look for the line of output that begins with <code>MANYCORE JOB BEGIN</code>.</p> <p>For example:</p> <pre><code>&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;\nMANYCORE JOB BEGIN: on b-6-18-4\n</code></pre> <p>If you suspect that your process was running on a node that was in a failed state, contact supercloud@mit.edu and provide the job id and the name of the compute node.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/","title":"pMatlab Job Problems","text":"<p>This level of troubleshooting assumes that you are able to connect to the SuperCloud system and submit a job. If you are unable to connect  please email supercloud@mit.edu with the error that you are experiencing.</p> <p>Below, we list some steps to help you sanity check\u00a0your\u00a0SuperCloud configuration and debug the problem. We also list some common errors and the steps to resolve the problem, and where to find your output and results when running pMatlab jobs. If you still run into problems, email supercloud@mit.edu with the error that you are experiencing.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#steps-to-sanity-check-and-debug","title":"Steps to Sanity Check and Debug","text":"<p>If you are able to submit a job but receive errors that do not seem related to your application, we recommend the following strategy:</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#1st-step-is-my-configuration-correct","title":"1st Step - Is my Configuration Correct?","text":"<p>An easy way to check on potential configuration issues is to run the Param_Sweep example. There are instructions for running <code>Param_Sweep</code> on the Verifying your pMatlab setup page.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#2nd-step-you-can-run-param_sweep-but-your-code-fails","title":"2nd Step - You can Run Param_Sweep, but your Code fails","text":"<p>Your configuration is correct, so the next step is to look at the output from your job. In this case, we are not looking at files that are application specific but rather at the output that is normally sent to the command window. The compute nodes produce output which is then directed to a <code>.out</code> file in the MatMPI directory (or a subdirectory within MatMPI) of your working MATLAB\u00ae directory.</p> <p>The first task is to see that all of the remote processors started a valid MATLAB\u00ae session and created the <code>.out</code> file. To check this you can  read the page on where to find pMatlab output files, or briefly:</p> <ul> <li>Go to the MatMPI directory in your working directory</li> <li>To find your output log files, look for subdirectories within the     MatMPI directory with names like this: <code>p&lt;start-pid&gt;-p&lt;end-pid&gt;_&lt;compute-node-name&gt;</code> .      where:<ul> <li><code>p&lt;start-pid&gt;</code> is the id of the first process running on the compute node whose name is\u00a0<code>&lt;compute-node-name&gt;</code></li> <li><code>p&lt;end-pid&gt;</code> is the id of the last process running on the compute node whose name is\u00a0<code>&lt;compute-node-name&gt;</code> If your job ran on multiple compute nodes, there will be several of these subdirectories.</li> </ul> </li> <li>Look at the list of files in those subdirectories - there should be     a total of <code>n</code> files with the <code>.out</code> extension</li> <li>Each of the <code>.out</code> files should include your filename, the processor     id and the <code>.out</code> extension - e.g. for Param_Sweep on 4 processors you     would see: <code>param_sweep_parallel_v2.0.out</code>, <code>param_sweep_parallel_v2.1.out</code>,     <code>param_sweep_parallel_v2.2.out</code> and <code>param_sweep_parallel_v2.3.out</code>.</li> </ul> <p>If any <code>.out</code> file is missing, this is a problem and should be reported to the SuperCloud Team by sending email to supercloud@mit.edu. Often what the user sees is that their application hangs on an <code>agg</code> command and with a bit of inspection they discover that one remote processor didn't properly start its MATLAB\u00ae session.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#3rd-step-errors-warnings-and-messages-from-remote-matlab-sessions","title":"3rd Step - Errors, Warnings, and Messages from Remote MATLAB\u00ae Sessions","text":"<p>If all of the remote MATLAB\u00ae sessions started properly, the next step is to check each of the <code>.out</code> files. Each <code>.out</code> file contains all of the diary or screen output from a remote node. In most cases these files should have the same information, with the exception of numerical data, as that in the command window on your local machine and should provide some indication of a warning or error related to the failure of the compute job. If the error message is not clear to you, or you are unsure how to correct the error send email, with the error information, to supercloud@mit.edu.</p> <p>If error messages were sent to STDERR by any of the remote MATLAB\u00ae sessions, these error messages will appear in the <code>.out</code> files. These error messages will also be written to the <code>.err</code> file (located in the MatMPI directory). The process id (pid) of the process which generated the error will be pre-pended to the error message so you can look in that process's <code>.out</code> file for additional output that may help you debug the cause of the error.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#problems-starting-matlab","title":"Problems Starting MATLAB\u00ae","text":"<p>If run into problems starting MATLAB\u00ae (e.g., you never get the MATLAB\u00ae prompt <code>&gt;&gt;</code>), try deleting the <code>.matlab</code> directory (note the leading <code>.</code> before \"matlab\") in your SuperCloud home directory, then restart MATLAB\u00ae.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#common-problems-launching-or-running-your-pmatlab-job","title":"Common Problems Launching or Running Your pMatlab Job","text":"<p>In this section we provide solutions to common problems and errors you might encounter when running your pMatlab jobs. If you don't see a solution for your problem below, the Getting Help page may point you in the right direction.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#flfilesystemnotdirectoryerror","title":"<code>fl:filesystem:NotDirectoryError</code>","text":"<p>MATLAB\u00ae often runs into errors with its cache directory when it is created (by default) in your home directory, as <code>~/.matlab</code>. You may see an error from MATLAB\u00ae that includes the text <code>fl:filesystem:notdirectoryerror</code>.</p> <p>In order to avoid these potential issues, you can point MATLAB\u00ae to a different location for creating its cache directory by setting the <code>MATLAB_PREFDIR</code> environment variable.</p> <p>To change the MATLAB\u00ae cache directory, create a directory with your username on the local file system that MATLAB\u00ae can use as its cache directory:</p> <pre><code>mkdir /state/partition1/user/$USER/\n</code></pre> <p>Set the <code>MATLAB_PREFDIR</code> environment variable to the new directory. It's best to add this line to your <code>~/.bashrc</code> file.</p> <pre><code>export MATLAB_PREFDIR=/state/partition1/user/$USER/\n</code></pre>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#undefined-function-or-variable-matmpidefs1","title":"<code>Undefined function or variable 'MatMPIdefs1'</code>","text":"<p>The mostly likely reason for this error is that your path is not set properly and you are missing the <code>./MatMPI</code> path. Send email to supercloud@mit.edu and a member of the SuperCloud team will help you resolve the path issue.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#out-of-memory-errors","title":"<code>OUT OF MEMORY</code> Errors","text":"<p>There are two primary options for enabling large compute jobs: requesting more memory per process by requesting more slots and dividing and distributing the workload. Each core comes with an amount of memory (see the Systems and Software page for up to date numbers), and so asking for more slots will get you more memory for each pMatlab process. If you have given each process an entire node and you are still running out of memory, you will to either try to reduce the amount of memory used by your application, or divide and distribute your workload.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#dividing-and-distributing-the-workload","title":"Dividing and distributing the workload","text":"<p>If you don't need to aggregate a large data structure, you can create a distributed matrix using more processors so that each processor is working on a smaller portion of the data. The Param_Sweep example in your SuperCloud home directory <code>~/examples/Param_Sweep</code> provides an example of how to do this.</p> <p>We recommend our online course,\u00a0Practical HPC. The PGAS Example: pMatlab Implementation section, within the Distributed Applications module provides a detailed introduction, working step-by-step through the Parameter Sweep application that is in the examples directory in your home directory. It should take approximately 30-45 minutes to work through the section.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#your-pmatlab-program-hangs","title":"Your pMatlab program hangs","text":"<p>If your pMatlab program seems to be hung (the output logs are empty or have not had additional data written to them, or the <code>LLstat</code>command shows the job is not in the RUNNING state), you should delete the job.</p> <p>The preferred method for deleting a job is to use <code>LLkill &lt;jobID&gt;</code>. You obtain the <code>&lt;jobID&gt;</code> by using the command <code>LLstat</code> These commands are described on the General Job Management Commands page.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-job-problems/#finding-your-output-and-results","title":"Finding Your Output and Results","text":"<p>Please see the page Finding pMatlab Output for details on how to find your output and results.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/","title":"pMatlab Troubleshooting Checklist","text":"<p>On this page, you'll find a series of questions and answers that will help you resolve the more common problems with running pMatlab jobs on the SuperCloud system.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#job-launch-problems","title":"Job Launch Problems","text":"<p>In this section, you'll find a series of questions and answers that will help you resolve some of the more common pMatlab job launch problems.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#is-your-path-set-up-to-include-the-gridmatlab-directories","title":"Is your path set up to include the gridMatlab directories?","text":"<p>Launch MATLAB\u00ae and enter the <code>path</code> command in the MATLAB\u00ae command window. In the first few lines of the output, does the path include these directories from your SuperCloud home directory?</p> <ul> <li><code>tools/gridMatlab/src</code></li> <li><code>tools/pMatlab/src</code></li> <li><code>tools/pMatlab/MatlabMPI/src</code></li> <li><code>matlab</code></li> </ul> <p>If not, check your startup code (<code>startup.m</code>, <code>startup_local.m</code> and any other startup code that is called from those files) for calls to <code>clear all</code> and remove those calls. Make sure your pMatlab script and code does not use <code>clear all</code> either.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#did-you-check-whether-there-are-resources-available-for-running-your-job","title":"Did you check whether there are resources available for running your job?","text":"<p>From the MATLAB\u00ae command window, enter the LLfree command and verify there are enough nodes and cores available for the CPU type that you requested. You can also enter LLfree at the Linux prompt if you are connected to the login node. Remember that if you are using triples mode to launch your job, you will be using whole nodes, so make sure there are enough nodes available.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#does-the-code-that-youre-trying-to-run-reside-on-the-supercloud","title":"Does the code that you're trying to run reside on the SuperCloud?","text":"<p>In order to run a job on the SuperCloud system, every processor must have access to all functions/methods in the code you are executing and any data the program accesses. Since every node can access your SuperCloud home directory or group shared directories, placing your code and data in your home directory\u00a0or any group shared directory makes them accessible to the entire system.</p> <p>If your code is not on SuperCloud, copy your code and any files that are needed in order to run your job to somewhere in your SuperCloud home directory. See Accessing and Transferring Data and Files for instructions on how to copy your files to  SuperCloud.</p> <p>If your code is on SuperCloud, confirm that your MATLAB\u00ae current working directory is somewhere in your SuperCloud home directory by running the <code>pwd</code> command.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#-did-you-try-running-the-param_sweep-example-in-your-supercloud-home-directory","title":"- Did you try running the Param_Sweep example in your SuperCloud home directory?","text":"<p>The <code>Param_Sweep</code> example is located in <code>$HOME/examples/Param_Sweep</code>.</p> <p>No: For instructions on how to run the <code>Param_Sweep</code> example, see the Verifying Your pMatlab Setup page.</p> <p>Yes, it worked: the problem is probably in your code somewhere. Look in your job's log files for errors or other clues. See the page on Finding Your pMatlab Output.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#youve-tried-and-confirmed-everything-here-and-you-still-cant-launch-a-pmatlab-job","title":"You've tried and confirmed everything here and you still can't launch a pMatlab job","text":"<p>If you've tried and confirmed all of the above items and still can't launch a pMatlab job, please let us know what you've tried, and also copy, paste and send any errors that you see to supercloud@mit.edu. Please attach any files that might be helpful in diagnosing the problem.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"pmatlab-troubleshooting-checklist/#references","title":"References","text":"<p>Here is a list of webpages that were mentioned here, plus some others that might be helpful:</p> <ul> <li>Verifying Your pMatlab Setup</li> <li>Troubleshooting pMatlab Job Problems</li> <li>Steps to sanity check and debug</li> <li>Handling common job errors</li> <li>Finding Your Output</li> <li>Triples Mode</li> <li>Accessing and Transferring Data and Files</li> <li>LLx Online Courses: Practical HPC course, \"Distributed Applications\" module</li> <li>pMatlab: Getting Started</li> <li>Launching pMatlab Jobs on the SuperCloud Systems</li> <li>Troubleshooting SuperCloud pMatlab Job Errors</li> </ul>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"practices/","title":"SuperCloud Practices","text":"","tags":["Getting Started"]},{"location":"practices/#standard","title":"Standard","text":"<p>Standard MIT Policy on the use of information technology resources apply to all accounts on the SuperCloud system. You can find MIT's policy on the use of information technology resources here.</p>","tags":["Getting Started"]},{"location":"practices/#access","title":"Access","text":"<p>It is our practice to provide access from within the United States.</p>","tags":["Getting Started"]},{"location":"practices/#backing-up-data-and-files","title":"Backing up Data and Files","text":"<p>We do not back up files on SuperCloud. We strongly recommend that you regularly move files back to your local workstation. See the page Accessing and Transferring Data and Files for instructions on how to copy your files.</p>","tags":["Getting Started"]},{"location":"practices/#non-public-data","title":"Non-Public Data","text":"<p>Not all data is appropriate for SuperCloud. If your data is not publicly available, we ask for any agreements or requirements you have for your data to make sure SuperCloud is the right place to be putting the data. Please be as detailed as you can. To get a general idea of the sorts of data that may or may not be appropriate for SuperCloud, take a look at MIT IS&amp;T's guidance for storing data in Dropbox, OneDrive, and Google Drive here.</p>","tags":["Getting Started"]},{"location":"practices/#regular-maintenance","title":"Regular Maintenance","text":"<p>SuperCloud has Monthly Downtimes which are scheduled for the Second Tuesday of each month.</p> <p>We usually start carefully draining the scheduler of jobs on Monday evening. Our automated re-imaging scripts kick off at 12:01 am on Tuesday. The re-imaging usually takes much of our sleeping hours, and in the morning the SuperCloud system engineers begin the planned hardware replacements and software validation.</p> <p>During the maintenance period, the Jupyter Portal, the Database Portal, and all of the compute nodes are unavailable.</p> <p>While we occasionally are able to get the system back to users on Tuesday afternoon, we usually get it back to users by Wednesday morning.</p> <p>We send an email with a reminder of the scheduled downtime to all users. When the maintenance is complete, an email will be sent informing users that the system has been returned to service. If you do not receive these emails, please contact supercloud@mit.edu.</p> <p>If the scheduled maintenance day falls on the day of a campus closure (for example, a holiday) the downtime will occur on the third Tuesday of the month. Off-schedule downtimes do happen and are announced in advance.</p>","tags":["Getting Started"]},{"location":"preferred-shells/","title":"Preferred Shells","text":"","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#why-do-i-need-a-shell","title":"Why do I need a shell?","text":"<p>If you use a Jupyter Notebook or the MATLAB\u00ae GUI, for the majority of your work you probably won't need to log into the login node of the SuperCloud system. However, at some point, there may be tasks that you will need to perform that will require you to log into a login node. Here are some examples of things that you may have to do from a login node:</p> <ul> <li>Downloading data or packages, or anything else that requires an internet connection</li> <li>Check what resources are currently available</li> <li>Launch your job using a batch script</li> <li>Check the status of your job</li> <li>Kill a job</li> <li>Set up your environment (e.g., use module files, create your own conda environment, download and build a 3rd party software package)</li> <li>Compile your code</li> <li>Copy or search through files</li> </ul> <p>In order to log into SuperCloud, you will need to run a shell on your desktop. Linux and Mac OS desktops come with a Linux shell already installed, the Terminal.</p> <p>For users with a Windows 10 desktop, there are many options for shells that you can use to access SuperCloud. Here we will provide a list of shells that we recommend for accessing SuperCloud, and some that we don't recommend.</p> <p>Once you are logged into a login node, you will be at the Linux shell command line, where you'll have to issue Linux commands. If you haven't used Linux before or need a brief refresher, see our Basic Unix Commands page for a list of commands that should cover most, if not all, of what you'll need to do, along with links to additional resources if you'd like to learn more.</p> <p>In addition to the standard Linux commands, we provide some job management commands and job submission commands (<code>LLsub</code>, <code>LLMapReduce</code>) that can also be issued from the Linux shell command line or a shell script.</p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#recommended-shells-for-windows-users","title":"Recommended shells for Windows users","text":"<p>In the sections that follow, we provide instructions for installing the shells that we recommend, and how to copy and paste in these shells.</p> <p>Please note that most of these shells have their own location where they store ssh keys, so if you generate keys in one of these you may have to do some additional configuration to use another shell.</p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#windows-subsystem-for-linux","title":"Windows Subsystem for Linux","text":"","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#launching-wsl","title":"Launching WSL","text":"<p>If you haven't already set up the Windows Subsystem for Linux and installed the Ubuntu terminal, you can find directions on how to do that here. Once they\u00a0are set up, launch Ubuntu from your start menu</p> <p></p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#enabling-copy-and-paste-tofrom-the-wsl-shell-ubuntu","title":"Enabling copy and paste to/from the WSL shell (Ubuntu)","text":"<ol> <li>Right click on the icon in the top left corner of the window and select Properties </li> <li>In the Properties window, select the Options tab. In the Edit Options section, enable QuickEdit Mode, then OK.</li> </ol>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#copying-and-pasting-text-tofrom-the-wsl-shell-ubuntu","title":"Copying and Pasting text to/from the WSL shell (Ubuntu)","text":"<p>In the following example we will copy a job number from the output of <code>LLstat</code>, then paste the number to the end of the <code>LLkill</code> command, to kill the job.</p> <ol> <li>Highlight the text that you want to copy </li> <li>Press the enter key to copy the highlighted text. The text will no longer be highlighted, but it has been copied to your clipboard.</li> <li>Position your cursor to the place where you want to paste the text. This can be in the same Ubuntu shell window, in an email message, in a new window, etc. To continue with this example, we type <code>LLkill</code> at the Linux prompt of our Ubuntu window, then we are ready to paste the copied job number to the end of the command. Notice the cursor is located after <code>LLkill</code>. </li> <li>Press the right mouse button to paste the text to your current cursor position </li> </ol>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#windows-command-prompt","title":"Windows Command Prompt","text":"","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#launching-the-windows-command-prompt","title":"Launching the Windows Command Prompt","text":"<p>To launch the Windows Command Prompt on your Windows 10 desktop, type \"command\" into the search box on the taskbar and select Command Prompt from the Best match list.</p> <p></p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#copying-and-pasting-tofrom-the-windows-command-prompt","title":"Copying and pasting to/from the Windows Command Prompt","text":"<p>Follow the instructions in the sections \"Enabling copy and paste to/from the WSL shell (Ubuntu)\" and \"Copying and Pasting Text to/from the WSL shell (Ubuntu)\". Once Quick Edit Mode is enabled, you can also use the standard Windows copy and paste shortcut keys Ctrl-C and Ctrl-V.</p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#windows-powershell","title":"Windows PowerShell","text":"","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#launching-the-windows-powershell","title":"Launching the Windows PowerShell","text":"<p>To launch the PowerShell on your Windows 10 desktop, type \"powershell\" into the search box on the taskbar and select Windows PowerShell from the Best match list. Make sure you do not select the ISE or x86 versions.</p> <p></p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#copying-and-pasting-tofrom-the-windows-powershell","title":"Copying and pasting to/from the Windows Powershell","text":"<p>Follow the instructions in the sections \"Enabling copy and paste to/from the WSL shell (Ubuntu)\" and \"Copying and Pasting Text to/from the WSL shell (Ubuntu)\". Once Quick Edit mode is enabled, you can also use the standard Windows copy and paste shortcut keys <code>Ctrl-C</code> and <code>Ctrl-V</code>.</p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#jupyter-notebook-terminal","title":"Jupyter Notebook Terminal","text":"","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#launching-the-jupyter-notebook-terminal","title":"Launching the Jupyter Notebook Terminal","text":"<p>If you have a Jupyter Notebook session running in your browser, you can open a terminal window from your notebook dashboard by clicking on the New button, then select Terminal from the pull down menu to launch a terminal session on the SuperCloud system.</p> <p></p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#how-to-copypaste-from-the-jupyter-notebook-terminal","title":"How to copy/paste from the Jupyter Notebook Terminal","text":"<p>In the following example we will copy a job number from the output of <code>LLstat</code>, then paste the number to the end of the <code>LLkill</code> command, to kill the job.</p> <ol> <li> <p>Highlight the text that you want to copy </p> </li> <li> <p>Right click and select Copy. You cannot use Ctrl-C because it will be interpreted by the Linux shell. </p> </li> <li> <p>Position your cursor to the place where you want to paste the text. This can be in the same Jupyter Notebook terminal window, in an email message, in a new window, etc. To continue with this example, we type <code>LLkill</code> at the Linux prompt of our Jupyter Notebook terminal window, then we are ready to paste the copied job number to the end of the command. Notice the cursor is located after <code>LLkill</code>. </p> </li> <li> <p>Press the right mouse button and click on Paste to paste the text to your current cursor position. You cannot use <code>Ctrl-V</code> because it will be interpreted by the Linux shell.</p> </li> </ol> <p></p> <p></p>","tags":["Getting Started","Windows"]},{"location":"preferred-shells/#shells-that-we-do-not-recommend","title":"Shells that we do not recommend","text":"<p>We do not recommend the following shells on Windows because they do not use OpenSSH. Access to the SuperCloud works best with ssh keys that are generated by OpenSSH.</p> <ul> <li>cygwin</li> <li>puTTY</li> <li>SecureCRT</li> <li>X-Win32</li> </ul>","tags":["Getting Started","Windows"]},{"location":"python-troubleshooting-checklist/","title":"Python Troubleshooting Checklist","text":"<p>On this page, you'll find a series of questions and answers that will help you resolve some of the more common Python and conda environment problems.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#installing-python-packages","title":"Installing Python Packages","text":"","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#are-you-seeing-a-permissions-error-during-the-pip-install","title":"Are you seeing a permissions error during the pip install?","text":"<p>Yes: Install the package in your home directory:</p> <pre><code>pip install --user &lt;package_name&gt;\n</code></pre> <p>By default Python might try to install your package system-wide or in our anaconda module. Take a look at our page on installing Python packages for further instructions.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#are-you-seeing-a-network-is-unreachable-error-during-the-pip-install","title":"Are you seeing a \"Network is unreachable\" error during the pip install?","text":"<p>Yes: Install the package from the SuperCloud login node.</p> <p>Compute nodes on SuperCloud aren't connected to the internet. You will need to be on a login node to install new packages.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#are-you-seeing-a-disk-quota-error-during-the-pip-install","title":"Are you seeing a \"Disk Quota\" error during the pip install?","text":"<p>Yes: Use <code>$TMPDIR</code> to tell pip to put its temporary files elsewhere:</p> <pre><code>export TMPDIR=/state/partition1/user/$USER\nmkdir $TMPDIR\n</code></pre> <p>If it still fails, try including the <code>--no-cache-dir</code> flag:</p> <pre><code>pip install --user --no-cache-dir &lt;package_name&gt;\n</code></pre> <p>After the install is complete, you can delete the temporary directory:</p> <pre><code>rm -rf $TMPDIR\n</code></pre> <p>Pip by default uses the <code>/tmp</code> directory for temporary files. This space is very small on SuperCloud so we have a pretty restrictive quota on <code>/tmp</code>. Telling pip to use a different location as described above will therefore solve this problem.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#conda-environments","title":"Conda Environments","text":"","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#do-you-need-to-have-your-own-conda-environment","title":"Do you need to have your own conda environment?","text":"<p>Have you tried using our conda environment?</p> <p>To see if the package you need is already installed in one of our anaconda modules:</p> <pre><code>module load anaconda/version\nconda list packageName\n</code></pre> <p>Yes: See the section Installing Your Package in Your Own Conda     Environment.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#you-cant-see-your-environment-in-a-jupyter-notebook","title":"You can't see your environment in a Jupyter Notebook?","text":"","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#did-you-add-the-jupyter-package-to-your-environment","title":"Did you add the jupyter package to your environment?","text":"<p>No: Add the jupyter package to your environment. See the section Installing Your Package in Your Own Conda Environment.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#did-you-install-ipykernel-in-your-conda-environment","title":"Did you install ipykernel in your conda environment?","text":"<p>No: Install the ipykernel package to your conda environment, then run <code>ipykernel</code> to \"register\" your conda environment.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#did-you-call-conda-init-bash","title":"Did you call <code>conda init bash</code>?","text":"<p>Check your <code>$HOME/.bashrc</code> file. If the following lines or similar appear somewhere in the file, then at some point you ran <code>conda init bash</code>:</p> <pre><code>#\u00a0&gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/state/partition1/llgrid/pkg/anaconda/anaconda3-2019b/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n\u00a0 \u00a0 eval \"$__conda_setup\"\nelse\n\u00a0 \u00a0 if [ -f \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/etc/profile.d/conda.sh\" ]; then\n\u00a0 \u00a0 \u00a0 \u00a0 . \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/etc/profile.d/conda.sh\"\n\u00a0 \u00a0 else\n\u00a0 \u00a0 \u00a0 \u00a0 export PATH=\"/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/bin:$PATH\"\n\u00a0 \u00a0 fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt; \n</code></pre> <p>Yes: Run the <code>conda config</code> command below at the terminal, and then log out and log back in. You only need to do this once.</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>The <code>conda init bash</code> command makes a change to your <code>.bashrc</code> file, which gets run at startup. These lines configure your environment to use the conda command, and also auto-activate the base environment whenever you log in or start a job. The base environment is the one associated with the anaconda module you had loaded when you ran <code>conda init bash</code>, and can interfere with your desired environment. If running the above command and logging out and back in doesn't fix your issue, you can try to comment out these lines in your <code>.bashrc</code>.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#do-you-install-consistentcompatible-versions-of-packages","title":"Do you install consistent/compatible versions of packages?","text":"<p>Some versions of packages conflict with versions of other packages. Make sure you have consistent and compatible versions of your packages.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#are-you-running-the-expected-version-of-python","title":"Are you running the expected version of Python?","text":"<p>Put <code>which python</code> in your submission script right before you call your python script. This will print out the full path to the python executable that is being used when you run the command <code>python</code> and can help verify whether you are using the expected conda environment or anaconda module. If the Python it is using is located in the expected environment, then that usually means the environment is loaded properly.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#is-python-searching-in-the-right-place-for-your-packages","title":"Is Python searching in the right place for your packages?","text":"<p>Put the lines</p> <pre><code>import sys\nsys.path\n</code></pre> <p>at the top of your Python script. This prints out the list of directories that Python looks in when it searches for packages.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#you-tried-everything-here-and-youre-still-having-problems-with-python-packages-or-conda-environments","title":"You tried everything here and you're still having problems with Python packages or conda environments?","text":"<p>If you've tried/confirmed all of the above items and still have problems with your Python packages or conda environments, please let us know what you've tried, and also copy, paste and send any errors that you see to supercloud@mit.edu.</p>","tags":["Python","Troubleshooting"]},{"location":"python-troubleshooting-checklist/#references","title":"References","text":"<p>Here is a list of webpages that were mentioned here, plus some others that might be helpful:</p> <ul> <li>Modules</li> <li>Installing Python Packages</li> <li>Installing Your Package in Your Own Conda Environment</li> <li>Not Seeing Your Python Output?</li> </ul>","tags":["Python","Troubleshooting"]},{"location":"requesting-account/","title":"Requesting an Account","text":"<p>The MIT SuperCloud is intended to support research and collaboration between MIT Lincoln Laboratory and students, faculty and researchers at MIT and other academic institutions.\u00a0It is our practice to allow access from within the United States.</p>"},{"location":"requesting-account/#account-request-process","title":"Account Request Process","text":"<p>The account request, approval, and creation process is:</p> <ol> <li>Complete Cybersecurity Training: With the increased requirements for keeping systems, data, and information safe, all new users are required to successfully complete cybersecurity training before applying for a SuperCloud account. For MIT students and staff we recommend the MIT SANS IT Security Awareness I &amp; IT Security Awareness II available through the Learning Center in Atlas. The free and openly available Cybersecurity Awareness Challenge also meets this requirement. We are aware that some of the topics in the Cybersecurity Awareness Challenge are not relevant to everyone, and we suggest using the \u201cHelp Me\u201d button on each page for topics that may be unfamiliar. Alternative cybersecurity trainings offered through your organization may also be submitted for approval.</li> <li>Request: There are two steps to the request:<ol> <li>Fill out all fields of our Account Request     Form. In this     form we ask if you are using non-public data, see why     below. If you     are not part of an MGHPCC institution, list your MIT or Lincoln     Laboratory collaborator. Do not submit this form without     answering these question, it will cause significant delay in the     process. If you have any questions about the form, ask us by     sending email to supercloud@mit.edu.</li> <li>Ask your faculty advisor or PI to send us a short confirmation     email for your account verifying that you will be using your     SuperCloud account for your work. This email should be sent to     supercloud@mit.edu.\u00a0We     will not email your advisor for you. We will not proceed with     the account creation process until we receive an email from your     advisor/PI.</li> </ol> </li> <li>Faculty Advisor/PI Confirmation: Once we receive an email from your     faculty advisor/PI we can continue the next step. This confirmation     must come from a faculty member or PI on the project that you are     using SuperCloud for.</li> <li>Approval: This usually happens behind the scenes. You may receive an     email with additional questions before you are approved. While you     are waiting you can start learning to use your account by working     through the Practical     HPC course.</li> <li>Creation: When your account is created, you will receive an email     with your username and further instructions to set up your account.     When your account is first created you will have a small startup     allocation. Once you complete steps 5 and 6 you can request your     account be updated to the standard allocation.</li> <li>Set up your account:     Create an ssh key and     add it to     your account, then     make sure you can log in through ssh. The Practical     HPC\u00a0course also     has a section with videos that walks you through this process.</li> <li>Learn to use your account: Work through the Practical     HPC course. This     course:<ol> <li>Includes an Introduction to HPC, canonical HPC Workflows, and     the SuperCloud system.</li> <li>Walks you through setting up your account, installing software,     running your first test job, submitting your first batch job.</li> <li>Describes how to scale up efficiently and measure your     performance.</li> </ol> </li> </ol> <p>The account creation process is manual and can take approximately\u00a0two weeks. You can make this process smoother by making sure you have fully filled out your request form before submitting it and making sure your advisor has sent us an email confirmation. While you are waiting for your account you can get a head start learning how to use the account by reviewing the Practical HPC\u00a0course.</p>"},{"location":"requesting-account/#why-do-we-ask-if-you-are-using-data-that-is-not-publicly-available","title":"Why do we ask if you are using data that is not publicly available?","text":"<p>Not all data is appropriate for SuperCloud. If your data is not publicly available, we ask for any agreements or requirements you have for your data to make sure SuperCloud is the right place to be putting the data. Please be as detailed as you can. To get a general idea of the sorts of data that may or may not be appropriate for SuperCloud, take a look at MIT IS&amp;T's guidance for storing data in Dropbox, OneDrive, and Google Drive here.</p>"},{"location":"requesting-account/#generating-ssh-keys","title":"Generating ssh Keys","text":"<p>If you have any issues or questions regarding the generation of ssh keys, please contact the team at supercloud@mit.edu.\u00a0To access the system you will need ssh keys. For additional security you can create a passphrase when you generate your key, which you must enter every time you log in. Since you set this yourself on your own computer, we cannot help you reset it if you forget it. If you can't remember your passphrase you'll have to generate a new key and re-add it using the Web Portal.</p> <p>If you cannot generate ssh keys on your system, let us know and we can help you.</p> <p>If you have no existing ssh keys, from the command line in a terminal window, follow the steps below. On Mac and Linux, open your standard terminal window. On Windows 10 and higher, you can use the Windows command prompt. If the command prompt does not recognize the ssh-keygen command, you can install OpenSSH by following the instructions on this page. If your Windows operating system is older than Windows 10, see the note below.</p> <p>If you already have ssh keys then you can use those. You will need your public key, <code>id_rsa.pub.</code></p> <p><code>[user1234@yourMachine]$ ssh-keygen -t rsa</code></p> <p>You will see the following:</p> <p><code>Generating public/private rsa key pair.</code></p> <p>When answering the 3 prompts (first 3 lines) hit return to create passwordless keys and save them in the default location. Alternatively, for extra security you can create a passphrase for your key that you'll have to enter every time you log in. To do this, instead of pressing \"enter\" or \"return\", enter the passphrase you've chosen when prompted.</p> <pre><code>Enter file in which to save the key (/home/user1234/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/user1234/.ssh/id_rsa.\nYour public key has been saved in /home/user1234/.ssh/id_rsa.pub.\nThe key fingerprint is:\n88:90:6a:dc:f1:bd:ed:fb:b1:aa:46:14:34:5e:b9:70 user1234@yourMachine\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|\u00a0 \u00a0 \u00a0 .o ..\u00a0 \u00a0 \u00a0 |\n| \u00a0 .\u00a0 .ooE \u00a0 \u00a0 \u00a0 |\n|\u00a0 o. \u00a0 .+ .\u00a0 \u00a0 \u00a0 |\n|....o..o . \u00a0 \u00a0 \u00a0 |\n|.o ...o.S\u00a0 \u00a0 \u00a0 \u00a0 |\n|.\u00a0 \u00a0 \u00a0 .o\u00a0 \u00a0 \u00a0 \u00a0 |\n|\u00a0 \u00a0 \u00a0 .. . . \u00a0 \u00a0 |\n| \u00a0 \u00a0 \u00a0 .. \u00a0 o\u00a0 \u00a0 |\n|\u00a0 \u00a0 \u00a0 ...++o \u00a0 \u00a0 |\n+\u2014\u2014\u2014\u2014\u2014------------+`\n</code></pre> <p>To view your public ssh key, go to your .ssh directory.</p> <p><code>[user1234@yourMachine]$ cd .ssh</code></p> <p>In <code>~/.ssh</code> you would see two files <code>id_rsa</code> and <code>id_rsa.pub</code>. The <code>id_rsa.pub</code> file contains your public key.</p> <p><code>[user1234@yourMachine]$ ls   id_rsa\u00a0 id_rsa.pub</code></p> <p>This is the <code>id_rsa.pub</code> file content after generating a public SSH key\u00a0that we would require. To view it, type\u00a0<code>cat id_rsa.pub</code>\u00a0at the command line.</p> <pre><code>[user1234@yourMachine]$ cat id_rsa.pub\nssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA1NAD8v4nFzQ6G7KIEzkDLOnlH7t/4zmw0vVXlJjjFW4kLBgLJa0tkk61jHCxO2CurDr4zdEs2NeHG9agZJgMKMJZdIVaxtPcEBVVaNutvn/ZDRe3VsrRjToKEoR0xlAUdoef++AwiwI6K6vBOGIq6whLIlY5L9tZJfaLF3xMwmQRRhf4C+al/yZ5hX7BfGba2fqZmugTPpeSbLnFMVPKK/wy6XZasBSAKgLBA141EMXIKuGrpXpxLMECPBN5GDd/xmjmD0pC2o2z5OdfdYJj/FRWL2sC8hWTZSPa4p/n7Qc9ErFW5wM7FkynwguN4t/A+QOCa+p8C/nrOcTQKugrtw user1234@yourMachine\n</code></pre> <p>Copy the entire output, including the ssh-rsa at the beginning.</p>"},{"location":"requesting-account/#adding-your-ssh-keys-to-your-account","title":"Adding your SSH Keys to your Account","text":"<p>Once you have created your ssh keys and copied your public key, you can add your key to your account using the Web Portal:</p> <ol> <li>Go to https://txe1-portal.mit.edu.</li> <li>Log in. If you are an MIT affiliate or an affiliate at another     university or institution you can log in with your MIT or     institutional credentials. Click on MIT Touchstone/InCommon.<ol> <li>Select your institution (note these are spelled out, MIT is     listed as Massachusetts Institute of Technology, for example).</li> <li>Click the checkbox next to \"Remember my Choice\" and click the     \"Select\" button.</li> <li>Log in with your institutional credentials.</li> </ol> </li> <li>Click on the \"sshkeys\" link.</li> <li>Paste your public ssh key in the box at the bottom of the page,     click \"Update Keys\".</li> <li>Verify you can log in by running <code>ssh USERNAME@txe1-login.mit.edu</code>     in the terminal where you created your ssh keys, where <code>USERNAME</code> is     the username we sent you in your new account email.</li> </ol>"},{"location":"requesting-account/#pre-windows-10","title":"Pre-Windows 10","text":"<p>NOTE:\u00a0For other Windows users there are a number of ssh clients you can use. Some ssh clients like Moba Xterm and Cygwin give you a Linux-like environment, and so once you start the program (which should look include a command line window), you can follow the instructions for creating an ssh key in above once you install the client.</p> <p>To install Moba Xterm, follow the instructions here through the section \"Create Local Shell\". Anytime you are instructed to open a terminal window, you can follow the instructions to create a local shell. Once you have installed, follow the instructions above for creating an ssh key.</p> <p>Instructions for installing PuTTY are\u00a0here.\u00a0(Please note, the link will open in a new window.) Once PuTTY is installed please follow the instructions at this\u00a0link\u00a0to manually generate your ssh-keys, only follow the instructions in the \"Generating an SSH Key\" section.</p>"},{"location":"requesting-account/#current-approver-list","title":"Current Approver List","text":"<ul> <li>Boston University: Wayne Gilmore</li> <li>Harvard: Scott Yockel</li> <li>MIT: Jeremy Kepner, Vijay Gadepally, Chris Hill, Lauren Milechin</li> <li>Lincoln Laboratory: Jeremy Kepner, Vijay Gadepally, Albert Reuther</li> <li>Northeastern: David Kaeli</li> <li>UMass Amherst: John Griffin</li> <li>UMass Dartmouth:\u00a0Geoffrey Cowles</li> <li>UMass Lowell: Anne Maglia</li> <li>UMass Medical: Paul Langlois</li> <li>University of Rhode Island:\u00a0Gaurav Khanna</li> </ul>"},{"location":"resource-limit-enforcement-and-increases/","title":"Resource Limits","text":"<p>To keep any one user\u00a0from monopolizing the system, we impose limits on the number of nodes, and therefore cores and GPUs that each user can use at one time. You can find the resource allocations for each node type on the Systems and Software page.</p> <p>When your account is first created you will have a small startup allocation. Upon completing and earning a certificate for the Practical HPC course (requires a grade of 70% or better on the graded assignments), you can update your resource allocation to the standard allocation via the User Profile page on the Web Portal. In order to update your allocation, you will need the certificate ID number located at the bottom of the page of your certificate of course completion. Copy and paste the certificate ID number to the text box in the \u201cUser Resource Limits\u201d section. Complete the update by clicking \u201cSubmit\u201d. A successful update will display the message \u201cCertificate verified. Please allow 5 to 10 minutes for your SLURM limits to revert to the standard default limits.\u201d Please wait 5-10 minutes before refreshing the page. If you run into trouble with this process send an email to supercloud@mit.edu. Resource allocations are listed on the User Profile page and on the Systems and Software page.</p>","tags":["Getting Started"]},{"location":"resource-limit-enforcement-and-increases/#requesting-an-increase","title":"Requesting an Increase","text":"<p>Limiting each user to a fixed amount of resources prevents any one user from monopolizing the entire system and allows other users to run jobs. However, most users can get significant speedup by using less than their allocation.</p> <p>When using more CPUs we encourage you to benchmark your applications to determine that more CPUs results in an overall increase in performance. For example, you might time the rate at which your application makes progress for different numbers of CPUs: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512. This performance data can help you choose the optimal number of CPUs for your applications.</p> <p>If you are facing a deadline and need additional processing power you may request more processors by sending email to supercloud@mit.edu.</p> <p>Please include:</p> <ul> <li>Which nodes you are requesting</li> <li>How many additional node you need</li> <li>The length of time for which you need them</li> <li>Why you are asking for more resources</li> <li>How you are launching your jobs</li> <li>Any other supporting information showing your workload will scale well to the requested allocation</li> </ul> <p>Before requesting, check that resources are available with <code>LLfree</code>, and review the list of Best Practices to verify that you are following best practices. Best practices become more important as you scale up. If you are asking for GPU nodes, please first read through and try our tips on optimizing your GPU jobs.</p> <p>The SuperCloud Team will evaluate the current usage to determine how best to accommodate all user requests.</p>","tags":["Getting Started"]},{"location":"resource-limit-enforcement-and-increases/#memory-allocation","title":"Memory allocation","text":"<p>Please see the Submitting Jobs page for instructions on how to determine your job's memory requirements, and how to request additional resources if your job requires a lot of memory.</p>","tags":["Getting Started"]},{"location":"running-matlab/","title":"Running Matlab","text":"<p>Matlab is available on SuperCloud. You do not need to load a module to run the <code>matlab</code> command.</p>","tags":["Matlab"]},{"location":"running-matlab/#supported-matlab-versions","title":"Supported MATLAB\u00ae versions","text":"<p>To see the versions that are currently supported you can type \"matlab\" at the command prompt and then press the \"tab\" button 2 times. This will initiate tab complete, showing you the different Matlab commands available. Run <code>ls -l /usr/local/bin/matlab</code> to see the default.</p> <p>You can also use this command to see all versions of Matlab:</p> <pre><code>ls -l /usr/local/bin/matlab*\n</code></pre> <p>The output will look something like this:</p> <pre><code>lrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab -&gt; /state/partition1/llgrid/pkg/matlabr2020a/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab-lite -&gt; /state/partition1/llgrid/pkg/matlabr2020a-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2019b -&gt; /state/partition1/llgrid/pkg/matlabr2019b/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2019b-lite -&gt; /state/partition1/llgrid/pkg/matlabr2019b-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2020a -&gt; /state/partition1/llgrid/pkg/matlabr2020a/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2020a-lite -&gt; /state/partition1/llgrid/pkg/matlabr2020a-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2020b -&gt; /state/partition1/llgrid/pkg/matlabr2020b/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2020b-lite -&gt; /state/partition1/llgrid/pkg/matlabr2020b-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2021a -&gt; /state/partition1/llgrid/pkg/matlabr2021a/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2021a-lite -&gt; /state/partition1/llgrid/pkg/matlabr2021a-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2021b -&gt; /state/partition1/llgrid/pkg/matlabr2021b/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2021b-lite -&gt; /state/partition1/llgrid/pkg/matlabr2021b-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2022a -&gt; /state/partition1/llgrid/pkg/matlabr2022a/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2022a-lite -&gt; /state/partition1/llgrid/pkg/matlabr2022a-lite/bin/matlab*\nlrwxrwxrwx 1 root root 52 Jun \u00a05 00:40 /usr/local/bin/matlab2022b -&gt; /state/partition1/llgrid/pkg/matlabr2022b/bin/matlab*\nlrwxrwxrwx 1 root root 57 Jun \u00a05 00:40 /usr/local/bin/matlab2022b-lite -&gt; /state/partition1/llgrid/pkg/matlabr2022b-lite/bin/matlab*\n</code></pre> <p>The MATLAB\u00ae executable name is to the left of the <code>-&gt;</code> in the output. The associated MATLAB\u00ae version appears to the right of the <code>-&gt;</code>.</p>","tags":["Matlab"]},{"location":"running-matlab/#running-matlab-interactively-from-an-ssh-session","title":"Running MATLAB\u00ae interactively from an ssh session","text":"<p>Please do not run your code on the login nodes. You can find instructions on how to submit your job to run on the compute nodes on the below. If you would like to run MATLAB\u00ae interactively on a SuperCloud system, you should request an interactive session on a compute node by using the <code>LLsub</code> command. If you want to specify the node that you want to use, include the <code>-p &lt;partition&gt;</code> option with the command.</p> <pre><code>LLsub -i\n</code></pre> <p>Once you are on the compute node, this is how you launch the default version of MATLAB\u00ae:</p> <pre><code>matlab -nodisplay -singleCompThread\n</code></pre> <p>To launch a version other than the default version:</p> <pre><code>matlab&lt;#&gt; -nodisplay -singleCompThread\n</code></pre> <p>where <code>matlab&lt;#&gt;</code> is one of the MATLAB\u00ae executable names. For example, <code>matlab2022a</code> refers to MATLAB\u00ae 2022a.</p>","tags":["Matlab"]},{"location":"running-matlab/#running-matlab-in-a-batch-job","title":"Running MATLAB in a Batch Job","text":"<p>To run a Matlab script as a batch job use a job submission script like this:</p> matlab_job.sh<pre><code>#!/bin/bash\n\nmatlab -nodisplay -r \"myscript; exit\"\n</code></pre> <p>Where <code>myscript.m</code> is the name of your Matlab script. Note that in the<code>matlab</code> command in <code>matlab_job.sh</code> we leave off the <code>.m</code> file extension. You can then submit this job to the scheduler using either the <code>LLsub</code> or <code>sbatch</code> command.</p> <p>It is likely you will need more resources than a single core. See the Submitting Jobs page for more information on how to submit batch jobs and request resources.</p>","tags":["Matlab"]},{"location":"running-matlab/#matlab-lite","title":"MATLAB\u00ae Lite","text":"<p>MATLAB\u00ae Lite is MATLAB\u00ae without any toolboxes installed. This will help make the MATLAB\u00ae launch faster. A light version is available for each version of MATLAB\u00ae supported by the SuperCloud.</p>","tags":["Matlab"]},{"location":"running-matlab/#installed-matlab-toolboxes","title":"Installed MATLAB\u00ae Toolboxes","text":"<p>To see a list of the installed MATLAB\u00ae toolboxes, launch MATLAB\u00ae and use the <code>ver</code> command at the MATLAB\u00ae command prompt:</p> <pre><code>&gt;&gt; ver\n</code></pre> <p>The output will look something like this:</p> <p><pre><code>-----------------------------------------------------------------------------------------------------\nMATLAB Version: 9.3.0.713579 (R2017b)\nMATLAB License Number: 241630\nOperating System: Linux 4.4.109_grsec_llgrid_10ms #1 SMP Thu Jan 25 13:26:10 EST 2018 x86_64\nJava Version: Java 1.8.0_121-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n-----------------------------------------------------------------------------------------------------\nMATLAB \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 9.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 9.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nAerospace Blockset \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 3.20 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nAerospace Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 2.20 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nAntenna Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nAudio System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 1.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nAutomated Driving System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 1.1 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nBioinformatics Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 4.9 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nCommunications System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 6.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nComputer Vision System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 8.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nControl System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 10.3 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nCurve Fitting Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.5.6 \u00a0 \u00a0 \u00a0 (R2017b)\nDSP System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 9.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nDatabase Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 8.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nDatafeed Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 5.6 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nEconometrics Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 4.1 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nEmbedded Coder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 6.13 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nFinancial Instruments Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 2.6 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nFinancial Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 5.10 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nFixed-Point Designer \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 6.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nFuzzy Logic Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 2.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nGPU Coder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nGlobal Optimization Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.4.3 \u00a0 \u00a0 \u00a0 (R2017b)\nHDL Coder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.11 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nHDL Verifier \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 5.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nImage Acquisition Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 5.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nImage Processing Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 10.1 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nInstrument Control Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 3.12 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nLTE HDL Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nLTE System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 2.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nMATLAB Coder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 3.4 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nMATLAB Compiler \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 6.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nMATLAB Compiler SDK \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 6.4 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nMATLAB Distributed Computing Server \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 6.11 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nMATLAB Report Generator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 5.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nMapping Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 4.5.1 \u00a0 \u00a0 \u00a0 (R2017b)\nModel Predictive Control Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 6.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nNeural Network Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 11.0 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nOptimization Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 8.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nParallel Computing Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 6.11 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nPartial Differential Equation Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 2.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nPhased Array System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nPolyspace Bug Finder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 2.4 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nPolyspace Code Prover \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 9.8 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nPowertrain Blockset \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.2 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nRF Blockset \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 6.1 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nRF Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 3.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nRisk Management Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.2 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nRobotics System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nRobust Control Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 6.4 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSignal Processing Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 7.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimBiology \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 5.7 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimEvents \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 5.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimscape \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 4.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimscape Driveline \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 2.13 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nSimscape Electronics \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 2.12 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nSimscape Fluids \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 2.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimscape Multibody \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 5.1 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimscape Power Systems \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 6.8 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink 3D Animation \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 7.8 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Check \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 4.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Code Inspector \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.1 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Coder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 8.13 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nSimulink Control Design \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 5.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Coverage \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 4.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Design Optimization \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 3.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Report Generator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 5.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Requirements \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSimulink Test \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 2.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nStateflow \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 9.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nStatistics and Machine Learning Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 11.2 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\nSymbolic Math Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 8.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nSystem Identification Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 9.7 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nText Analytics Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 1.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nTracking and Sensor Fusion Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 1.0 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nTrading Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 3.3 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nVision HDL Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Version 1.5 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nWLAN System Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 1.4 \u00a0 \u00a0 \u00a0 \u00a0 (R2017b)\nWavelet Toolbox \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Version 4.19 \u00a0 \u00a0 \u00a0 \u00a0(R2017b)\n&gt;&gt;\n</code></pre> </p>","tags":["Matlab"]},{"location":"screen/","title":"Screen","text":"<p>If you ssh to one of the SuperCloud login nodes and need to run a script or command(s) that you expect to take a while to complete, you should use the Linux <code>screen</code> command which will keep processes running even if you lose your connection. If your ssh connection drops for some reason, <code>screen</code> allows you to reattach to a previous session so you can finish your task.</p> <p>If you will be\u00a0doing a lot of work\u00a0in an interactive session on a compute node, you should also\u00a0use the\u00a0<code>screen</code> command so that your interactive session isn't killed if you lose your connection.</p>","tags":["Tips and Best Practices","How To"]},{"location":"screen/#remember-your-login-node","title":"Remember your login node","text":"<p>It is important to make a note of which login node you are logged into when you issue the <code>screen</code> command. If you lose your connection, you will have to log back into the same login node in order to find and restore your screen session.</p> <p>To see which login node you are currently logged into, from your ssh terminal, run:</p> <pre><code>$ echo $HOSTNAME\nlogin-4\n</code></pre> <p>If you lose your ssh connection and need to restore your screen session, you'll have to log back in to the same login node you were on when you created your screen session:</p> <p><code>$ ssh USERNAME@txe1-login4.mit.edu</code></p>","tags":["Tips and Best Practices","How To"]},{"location":"screen/#using-screen","title":"Using screen","text":"<p>Here are the very basics that you need to know in order to use the <code>screen</code> command.\u00a0 You can find a more detailed introduction to\u00a0<code>screen</code> here\u00a0(ignore the sections on installing it; it is already installed on SuperCloud).</p> <p>Create a screen session</p> <p>With <code>screen</code> you can create one or more sessions in your current SSH terminal. To start it, just run:</p> <p><code>$ screen</code></p> <p>This creates a screen session in your current ssh terminal.</p> <p>Here are the most important <code>screen</code> commands that you need to control <code>screen</code>. These commands begin with <code>CTRL-a</code> to distinguish them from normal shell commands.</p> <p><code>Ctrl-a c</code>\u00a0 \u00a0 Create\u00a0a new screen session so that you can use more than one screen session at once. <code>Ctrl-a n</code>\u00a0 \u00a0 Switch\u00a0to the next screen session (if you use more than one). <code>Ctrl-a p</code>\u00a0 \u00a0 Switch\u00a0to the previous screen session (if you use more than one). <code>Ctrl-a d</code>\u00a0 \u00a0 Detach\u00a0a screen session (without killing the processes in it - they continue).</p> <p>To close a screen session where all tasks are finished, type:</p> <p><code>exit</code></p> <p>Get a list of your screen sessions</p> <p>Back in a regular ssh terminal on the login node where you created your screen session, you can get a list of your current screen sessions by typing:</p> <pre><code>$ screen -ls\nThere is a screen on:\n\u00a0 \u00a0 \u00a0 \u00a0 76645.pts-46.login-4 (Detached)\n1 Socket in /run/screen/S-AN23082.\n</code></pre> <p>Reconnect to a screen session</p> <p>To reconnect to the session, make sure you are logged in to the proper login node, then run:</p> <p><code>$ screen -r 76645</code></p>","tags":["Tips and Best Practices","How To"]},{"location":"segmentation-faults/","title":"Segmentation Faults","text":"","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"segmentation-faults/#what-is-a-segmentation-fault","title":"What is a segmentation fault?","text":"<p>A segmentation fault (also known as a segfault) is caused when code attempts to access memory that it doesn't have permission to access. Every program is given a piece of memory (RAM) to work with\u00a0and it is only allowed to access memory in that chunk.</p> <p>If your process crashed and you\u00a0see an error reporting that a segmentation fault has occurred, the problem is most likely in your code or a 3rd party code that you are using. One exception is when the segfault occurs because your program ran out of stack space. See the section Running out of stack space below on how to correct this problem.</p>","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"segmentation-faults/#common-segfault-scenarios","title":"Common segfault scenarios","text":"<p>In practice, segfaults are often due to trying to read or write a non-existent array element, not properly defining a pointer before using it, or accidentally using a variable's value as an address. Below, you will find several common scenarios that result in segfaults.</p>","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"segmentation-faults/#attempting-to-read-or-write-past-the-end-of-an-array","title":"Attempting to read or write past the end of an array","text":"<p>An array is a contiguous region of memory, where each successive element is located at the next address in memory. Most arrays don't have a sense of how large they are, or what the last element is, making it easy to blow past the end of the array without knowing it. If you read or write past the end of the array, you may wind up going into memory that is uninitialized or belongs to something else.</p>","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"segmentation-faults/#attempting-to-operate-on-a-memory-location-in-a-way-that-is-not-allowed","title":"Attempting to operate on a memory location in a way that is not allowed","text":"<p>An example of this would be attempting to write to a read-only location.</p>","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"segmentation-faults/#accessing-a-null-or-uninitialized-pointer","title":"Accessing a NULL or uninitialized pointer","text":"<p>If you have a pointer that is NULL (ptr=0) or that is completely uninitialized (it isn't set to anything at all yet), attempting to access memory or modify memory using that pointer will have undefined behavior.</p>","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"segmentation-faults/#running-out-of-stack-space","title":"Running out of stack space","text":"<p>Segfaults can also occur when your program runs out of stack space. This may not be a bug in your program, but may be due to your shell setting the stack size limit to a value that is too small. The usual remedy is to increase the stack size and re-run your program. For example, to set the stack size to unlimited, add this line to your job submission script:</p> <p><code>ulimit -s unlimited</code></p>","tags":["Tips and Best Practices","Troubleshooting"]},{"location":"shared-groups/","title":"Shared Groups","text":""},{"location":"shared-groups/#overview","title":"Overview","text":"<p>This page summarizes Shared Groups, including how to request to join or create a group, some best practices for working with shared groups, and how to inspect and change file permissions and group ownership.</p> <p>SuperCloud users are welcome to either join an existing group and receive the benefit of access to a shared file directory, or propose the creation of a group if there is some common interest amongst certain SuperCloud account holders, perhaps they are members of the same lab.</p> <p>You can look at the current list of groups by listing the groups directory:</p> <p><code>studentx@login-3:~$ ls /home/gridsan/groups/</code></p> <p>You can see what groups you are currently in by running the \"groups\" command:</p> <p><code>studentx@login-3:~$ groups</code></p>"},{"location":"shared-groups/#joining-or-creating-a-group","title":"Joining or Creating a Group","text":"<p>If you would like to join a group, send an e-mail to supercloud@mit.edu with that request and CC the group owner for approval. The group owner must give approval before we can add you to the group. If you are not sure who the group owner/approver is, you can send in your request and we will reach out to the approver.</p> <p>If you would like to create a group, please email supercloud@mit.edu with the following info.</p> <ul> <li>What should the group be called?</li> <li>Who should the group owner/approver be? We will ask this person for     approval if anyone asks to be added.</li> <li>Who should be in the group, listing SuperCloud usernames is most helpful to us,     but not required. Otherwise, provide full names and email addresses.</li> <li>Whether you plan to store any non-public data in the group. If so,     please list any requirements, restrictions, or agreements associated     with the data. The more information you give us, the better.</li> </ul>"},{"location":"shared-groups/#using-shared-groups-effectively","title":"Using Shared Groups Effectively","text":"<p>Once you have been added to a group you will be able to access that group's shared directory. All group directories are located in the <code>/home/gridsan/groups</code> directory on the filesystem. Since this is part of the central filesystem along with your home directory, all nodes in SuperCloud can access the group directories. We will also add a symlink in your home directory to your group shared directory, this symlink will have the suffix \"_shared\" to indicate it is linking to a group directory. If you are sharing code with other members of your team that includes paths to a shared group, it is good practice to use a path that does not include your home directory, otherwise your team members will get a permission denied error when they try to run your code. Instead, it is best to use the absolute path through <code>/home/gridsan/groups</code>.</p> <p>All of our Best Practices for using the Filesystem apply to the group directories. Additionally, NEVER use a GUI to drag and drop files into a group directory. Doing so can alter the permissions in the group directory, preventing others in your group from accessing the files you've moved into the shared group directory. When using <code>rsync</code> to transfer files into a group directory, be sure to use the <code>-g</code> flag, which will also help keep the group ownership set properly.</p>"},{"location":"shared-groups/#linux-file-permissions","title":"Linux File Permissions","text":"<p>Sometimes, despite your best efforts, the permissions on a group can be altered such that you or others in your group can't interact with a file the way they need to. If that happens, you can always contact us at supercloud@mit.edu and we can fix it. However, you may find it more convenient to fix it on your own. Here is a brief introduction to Linux File Permissions to help you learn what is going on and how to fix it.</p>"},{"location":"shared-groups/#inspecting-file-permissions","title":"Inspecting File Permissions","text":"<p>If you do a long form listing of the files in a directory using <code>ls -l</code>:</p> <pre><code>drwxrwx---\u00a0 \u00a0 2 studentz studentz\u00a0 \u00a0 \u00a04096 Jun 15 14:51\u00a0 mydirectory\nlrwxrwxrwx\u00a0 \u00a0 1 root\u00a0 \u00a0 \u00a0root\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 26 Jun 15 17:24\u00a0 files_shared -&gt; ../groups/fileshare\n-rw-------\u00a0 \u00a0 1 studenty studenty\u00a0 \u00a0 4096 Jun 30 09:02\u00a0 logfile1\n-rw-rw---\u00a0 \u00a0 \u00a01 studentx Alpha\u00a0 \u00a0 \u00a0 \u00a04096 Jun 30 09:02\u00a0 logfile2\n</code></pre> <p>You will see the file permissions of your various directories, symlinks, and files in the leftmost columns. The first column indicates whether the file is a directory (d), symlink (l), or a regular file. Columns 2 through 10 can be viewed as triplets that define access permissions for the file or folder. To explicitly define permissions you will need to reference the Permission Group and Permission Types:</p> <ul> <li>The Permission Groups are:\u00a0 u -- Owner\u00a0 \u00a0g -- Group\u00a0 o -- Others\u00a0\u00a0</li> <li>The Permission Types are:\u00a0 r -- Read\u00a0 w -- Write\u00a0 \u00a0x -- Execute</li> </ul> <p>The first of these triplets represent the Owner's permissions, the second the Group's, and the third Others'. An r,w, or x represent the ability to perform that action, and a \"-\" means that action is not permitted. For a file like <code>logfile1</code> above you can see that it is owned by user <code>studenty</code> (from group <code>studenty</code>) and only the owner has read and write permissions. The file named <code>logfile2</code> currently has the permissions set to <code>-rw-rw----</code>, which means that the owner and group have read and write permission. SuperCloud does not allow you to add read, write, or execute permissions for others, or all users. One important thing to note: in order to go into a directory you must have execute permissions on that directory. So if you get a \"Permission denied\" error when trying to enter or look at the files in a directory, check whether the directory has read and execute permissions.</p>"},{"location":"shared-groups/#changing-file-permissions","title":"Changing File Permissions","text":"<p>Now say we want to change permissions for a file. One of the easiest ways is to use the Assignment Operators, + (plus) and -- (minus). These are used to tell the system whether to add or remove the specific permissions.</p> <p>For example, to add group read and write permission for <code>logfile1</code>, you would invoke the command:</p> <p><code>chmod g+rw logfile1</code></p> <p>Now say you want your group to be able to read <code>logfile2</code>, but don't want anyone to accidentally modify it. To remove group write permissions you would invoke the command:</p> <p><code>chmod g-w logfile2</code></p> <p>It's very important to know that if you want to apply these changes recursively that you use the <code>-R</code> (with a capital R) flag. Using a lowercase <code>-r</code> flag like you do for other Linux commands like <code>cp</code> will remove write permissions for everyone, including yourself. If you make this mistake, it is not the end of the world, but you will need to send us an email and have us fix it.</p> <p>Alternately you can define the full permissions options with binary references like <code>chmod 750 logfile1</code> which would grant full privileges (7) to the owner, and rw privileges (5) to the group and nothing (0) to others in a single command. You can learn more options and about chmod either from an online tutorial or from your local man pages (<code>man chmod</code>, typing <code>q</code>\u00a0will exit) or with the quick cheat sheet you can display with <code>chmod --help</code>. The following is also pretty good tutorial, but be aware it talks about permissions in general, and not everything will be relevant to shared groups or SuperCloud: How to use the chmod Command on Linux.</p>"},{"location":"shared-groups/#linux-file-ownership","title":"Linux File Ownership","text":"<p>If we take another look at the example directory above:</p> <pre><code>drwxrwx---\u00a0 \u00a0 2 studentz studentz\u00a0 \u00a0 \u00a04096 Jun 15 14:51\u00a0 mydirectory\nlrwxrwxrwx\u00a0 \u00a0 1 root\u00a0 \u00a0 \u00a0root\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 26 Jun 15 17:24\u00a0 files_shared -&gt; ../groups/fileshare\n-rw-rw----\u00a0 \u00a0 1 studenty studenty\u00a0 \u00a0 4096 Jun 30 09:02\u00a0 logfile1\n-rw-r----\u00a0 \u00a0 \u00a01 studentx Alpha\u00a0 \u00a0 \u00a0 \u00a04096 Jun 30 09:02\u00a0 logfile2\n-rw-r-x---\u00a0 \u00a0 1 studenty studenty\u00a0 \u00a0 4096 Jun 30 09:02\u00a0 myscript.py\n</code></pre> <p>the 12th and 13th column of the <code>ls -l</code> output is the owner of the file, listed first, and the group for the file, listed second. For example, <code>logfile2</code> is owned by <code>studentx</code> and its group is <code>Alpha</code>. Based on the permissions above, <code>studentx</code> can read and write to the file, and anyone in the <code>Alpha</code> group can read the file, but cannot write to it.</p> <p>In a group directory the group owner for a file should usually be the group associated with that directory. Sometimes it unintentionally gets set to the username of the person who created or put the file there. This can easily be remedied by using the <code>chgrp</code> command. For example, let's say we'd like everyone in the <code>Alpha</code> group to be able to read and run (execute) the file <code>myscript.py</code>, but not have write permissions. The group permissions are set properly, but the group is set to <code>studenty</code>\u00a0instead of <code>Alpha</code>. To fix this, we can run:</p> <p><code>chgrp Alpha myscript.py</code></p> <p>Again, if you would like to apply this change recursively, the flag is <code>-R</code> (with a capital R).</p>"},{"location":"software-packages/","title":"Software and Package Management","text":"<p>The standard environment on the MIT SuperCloud System is sufficient for most. If it is not, first check to see if the tool you need is included in a module. Modules contain environment variables that set you up to use other software, packages, or compilers not included in the standard stack. If there is no module with what you need, you can often install your package or software in your home directory. Below we have instructions on how to install Julia, Python, and R packages. If you have explored both these options or are having trouble, send us an email at supercloud@mit.edu and let us know what software you are trying to install, what steps you took to try to install it, and what errors you got during the install.</p>"},{"location":"software-packages/#modules","title":"Modules","text":"<p>Modules are a handy way to set up environment variables for particular work, especially in a shared environment. They provide an easy way to load a particular version of a language or compiler.</p> <p>To see what modules are available, type the command:</p> <p><code>module avail</code></p> <p>To load a module, use the command:</p> <p><code>module load moduleName</code></p> <p>Where <code>moduleName</code> can be any of the modules listed by the module avail command.</p> <p>If you want to list the modules you currently have loaded, you can use the module list command:</p> <p><code>module list</code></p> <p>If you want to change to a different version of the module you have loaded, you can switch the module you have loaded. This is important to do when loading a different version of a module you already have loaded, as environment variables from one version could interfere with those of another. To switch modules:</p> <p><code>module switch oldModuleName newModuleName</code></p> <p>Where <code>oldModuleName</code> is the name of the module you currently have loaded, and <code>newModuleName</code> is the new module that you would like to load.</p> <p>If you would like to unload the module, or remove the changes the module has made to your environment, use the following command:</p> <p><code>module unload moduleName</code></p> <p>Finally, in order to use the module command inside a script, you will need to initialize it first.</p> <p>The following shows a bash shell script example:</p> <pre><code>#!/bin/bash\n\n# Initialize the module command first\nsource /etc/profile\n\n# Then use the module command to load the module needed for your work\nmodule load anaconda/2023a\n</code></pre>"},{"location":"software-packages/#installing-software-or-packages-in-your-home-directory","title":"Installing Software or Packages in your Home Directory","text":"<p>Many packages and software can be installed in user space, meaning they are installed just for the user installing the package or software. The way to do this for Julia, Python, and R packages is described below. For other software, look at their installation documentation and see if they have instructions on how to install in your home directory. Sometimes this is described changing the installation location. Often you will have to download the source and build the software in your home directory to do this. Any dependencies can usually be installed in a similar way. If you run into trouble installing software you can reach out to us for help. Let us know what you have tried so far and we can often point you in the right direction.</p>"},{"location":"software-packages/#julia-packages","title":"Julia Packages","text":"<p>Adding new packages in Julia doesn't require doing anything special. On the login node, load a julia module and start Julia. You can enter package mode by pressing the <code>]</code> key and entering <code>add packagename</code>, where <code>packagename</code> is the name of your package. Or you can load <code>Pkg</code> and run <code>Pkg.add(\"packagename\")</code>.</p> <p>The easiest way to check if a package already exists is to try to load it by running <code>using packagename</code>. The <code>Pkg.status()</code> command will only show packages that you have added to your home environment. If you would like a list of the packages we have installed, the following lines should do the trick (where v1.# is your version number, for example v1.3):</p> <pre><code>using Pkg\nPkg.activate(DEPOT_PATH[2]*\"/environments/v1.3\")\ninstalled_pkgs = Pkg.installed()\nPkg.activate(DEPOT_PATH[1]*\"/environments/v1.3\")\ninstalled_pkgs\n</code></pre> <p>If you get an error trying to install a Julia package, first check to make sure you are on the login node, as the compute nodes don't have internet access. If you are already on the login node, it is possible that the installation is filling up the <code>/tmp</code> directory. The errors for this can be vague and differ between the different Julia versions. You can try changing the temporary directory that Julia uses to download its packages for installation by setting the <code>$TMPDIR</code> environment variable. You can create the new temporary directory and set the environment variable like this:</p> <pre><code>mkdir /state/partition1/user/$USER\nexport TMPDIR=/state/partition1/user/$USER\n</code></pre> <p>Once you have done this you can start up Julia and install packages as you normally would. Once you are done it is good practice to delete these temporary files.</p> <p>Note</p> <p>If you are using Jupyter there is an additional step you can optionally do so that Jupyter can find both our installed packages and your own. You can also run this if you are missing a Julia Kernel. First load a Julia module. Then, in a Julia shell, run:</p> <pre><code>using IJulia\ninstallkernel(\"Julia MyKernel\", env=Dict(\"JULIA_LOAD_PATH\"=&gt;ENV[\"JULIA_LOAD_PATH\"]))\n</code></pre> <p>The first part <code>Julia MyKernel</code> is what you want to call your kernel, so feel free to change this. The second part makes sure both our packages and any you've installed in your home directory show up on the load path when you use a Jupyter Notebook with this kernel.</p>"},{"location":"software-packages/#python-packages","title":"Python Packages","text":"<p>Many python packages are included in the Anaconda distribution. The quickest way to check if the package you want is in our module is to load the anaconda module, start python, and try to import the package you are interested in. Importing the packages in our Anaconda modules will also be much faster than importing packages that are installed in your home directory. This is because the packages in our Anaconda modules are installed on the local disk of every node, which is faster to access than packages installed in your home directory.</p> <p>If the package you are looking to install is not included in Anaconda, then you can install it in user space in your home directory- this allows you to install the package for you to use without affecting other users. We recommend that you use pip to do this, as pip allows you to only install the additional packages that you need in your home directory. Conda environments will result in installing all packages in your home directory, which can slow down the import process quite a bit.</p>"},{"location":"software-packages/#installing-packages-in-your-home-directory-with-pip","title":"Installing Packages in your Home Directory with pip","text":"<p>First, load the Anaconda module that you want to use if you haven't already:</p> <p><code>module load anaconda/2023a</code></p> <p>Here we are loading the 2021a module, the newer modules will have newer packages. Then, install the package with pip as you normally would, but with the <code>--user</code> flag:</p> <p><code>pip install --user packageName</code></p> <p>Where <code>packageName</code> is the name of the package that you are installing.</p> <p>If you get an error trying to install a package with pip, first check to make sure you are on the login node, as the compute nodes don't have internet access. If you are already on the login node, it is possible that the installation is filling up the <code>/tmp</code> directory, you may get a \"Disk quota exceeded\" error. You can change the temporary directory that pip uses to download its packages for installation by setting the <code>$TMPDIR</code> environment variable. You can create the new temporary directory, set the environment variable, and install your package like this:</p> <pre><code>mkdir /state/partition1/user/$USER\nexport TMPDIR=/state/partition1/user/$USER\n</code></pre> <p>Once you are done it is good practice to delete these temporary files.</p>"},{"location":"software-packages/#installing-packages-with-conda","title":"Installing Packages with Conda","text":"<p>As mentioned above, if at all possible we recommend you install packages in your home directory with pip rather than create a conda environment, as it'll be much faster. However, if you need to use a conda environment (usually this is because a package isn't available through pip or to help manage complex dependencies), you can do so by loading our anaconda module (this will give you access to the \"conda\" command) and then creating an environment the same way you would anywhere else. For example:</p> <pre><code>module load anaconda/2023a\nconda create -n my_env python=3.8 pkg1 pkg2 pkg3\n</code></pre> <p>In this example I am loading the <code>anaconda/2023a</code> module, then creating a conda environment named <code>my_env</code> with Python 3.8 and installing packages pkg1, pkg2, pkg3. We have found that conda creates more robust environments when you include all the packages you need when you create the environment. Then, whenever you want to activate the environment, first load the anaconda module, then activate with <code>source activate my_env</code>. Using <code>source activate</code>\u00a0instead of <code>conda activate</code> allows you to use your conda environment at the command line and in submission scripts without additional steps.</p> <p>If you would like to use your conda environment in Jupyter, simply install the \"jupyter\" package into your environment. Once you have done that, you should see your conda environment listed in the available kernels.</p> <p>Note:\u00a0If you are using a conda environment and would like to install the package with pip in that environment rather than in the standard home directory location, you should not include the <code>--user</code> flag. Further, if you are using a conda environment and want Python to use packages in your environment first, you can run the following two command:</p> <p><code>export PYTHONNOUSERSITE=True</code></p> <p>This will make sure your conda environment packages will be chosen before those that may be installed in your home directory. If you are using Jupyter, you will need to add this line to the\u00a0<code>.jupyter/llsc_notebook_bashrc</code>\u00a0file.\u00a0See the section on the bottom of the Jupyter\u00a0page for more information.</p>"},{"location":"software-packages/#r-libraries","title":"R Libraries","text":"<p>There are two different ways we recommend that you use R. First, is using a preset R environment that comes with the anaconda module, second would be to create your own R conda environment. This first way works best if you don't need to install any additional packages than what we already have.</p> <p>To use our R conda environment, log in and load an anaconda module. Then you can activate the R environment with <code>source activate</code>. You can see what packages are installed with the <code>conda list</code> command. Any packages that start with <code>r-</code> are R libraries.</p> <pre><code>module load anaconda/2023a\nsource activate R\nconda list\n</code></pre> <p>Then you can use R as you did before.</p> <p>If you need to install additional packages, it's best to do it in a new conda environment.</p> <p>First thing to know is that many R packages are available through conda, and some are not. What you want to do is include as many R libraries that you'll need as you can when you create your conda environment- this helps avoid version conflicts. Conda r libraries are all prefixed with <code>r-</code>, so for example if you need rjava, you'd search for <code>r-rjava</code>. You can check if conda has a library with the command: <code>conda\u00a0search\u00a0r-LIBNAME</code>, where <code>LIBNAME</code> is the name of the library you're looking for. You'll see a lot of versions, but as long as you see something you should be good to add it.</p> <p>Once you have a list of all the libraries available through conda, create your conda environment (I'm calling the environment myR, feel free to change that):</p> <p><code>conda create -n myR -c conda-forge r-essentials r-LIB1 r-LIB2\u2026</code></p> <p>Where LIB1, LIB2, etc are the additional R libraries you'd like to include. Sometimes this step takes a while. It'll tell you which new packages are going to be installed, and then you can confirm by typing <code>y</code>.</p> <p>If you have any other libraries that weren't available through conda, install them now. First activate your new environment and then start R:</p> <pre><code>source activate myR\nR\n</code></pre> <p>Then you can install your remaining libraries. You can do some test loads here as well, to make sure the libraries installed properly.</p> <pre><code>install.packages(\u201cPKGNAME\u201d)\nlibrary(PKGNAME)\n</code></pre> <p>In Jupyter, you should see your environment show up as a kernel. For a batch job, you'll have to activate the environment either in your submission script or before you submit the job.</p>"},{"location":"ssh-troubleshooting-checklist/","title":"ssh Troubleshooting Checklist","text":"<p>On this page, we'll address issues you might run into with logging into the SuperCloud systems. \u00a0</p>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#if-you-cant-log-into-supercloud-or-you-cant-login-without-entering-your-username-andor-password","title":"If you can't log into SuperCloud or you can't login without entering your username and/or password","text":"<p>In this section you'll find a series of questions and answers that will help you resolve some of the more common passwordless ssh login problems.</p> <p>You should also take a look at the LLx Online Course \"Practical HPC\". This course includes videos that demonstrate how to create an ssh key, how to create an ssh config file, and troubleshooting ssh problems in the \"Getting Started with SuperCloud\" module, \"Accessing SuperCloud\" section.</p>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#did-you-create-ssh-keys-for-your-desktop-system","title":"Did you create ssh keys for your desktop system?","text":"<p>No:</p> <ul> <li>See Generating SSH     Keys     for instructions on creating your ssh keys.     \u00a0</li> </ul> <p>Yes</p> <ul> <li> <p>Open the public ssh key file (the file that has a .pub extension) in     an editor     \u00a0</p> </li> <li> <p>The public key begins with \"ssh-rsa\". It will look similar to this     (there will be real data for all of the Xs):     \u00a0</p> <p>ssh-rsa AAxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxx</p> <p>This is the key to add to the SSH Key Portal.</p> <p>Do not add a key that begins with \"-----BEGIN OPENSSH PRIVATE KEY-----\"</p> <p>After adding the key to the key portal, the entry will look like this (you may have more than 1 entry in the table):</p> <p></p> </li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#if-your-account-is-not-a-new-supercloud-account-did-you-use-the-ssh-key-portal-to-add-your-new-key","title":"If your account is not a new SuperCloud account: Did you use the ssh key portal to add your new key?","text":"<p>No</p> <ul> <li>See the Adding your SSH Keys to your Account     section on the Requesting an Account     page for instructions on how to add your key using the SSH Key     Portal.</li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#confirm-that-you-need-a-config-file-sshconfig-on-your-desktop-system","title":"Confirm that you need a config file (.ssh/config) on your desktop system","text":"<p>You might not need an ssh config file on your desktop system. Answer the next 4 questions to determine whether you'll need one.</p>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#1-is-your-local-username-on-your-desktop-system-the-same-as-your-supercloud-username","title":"1) Is your local username on your desktop system the same as your SuperCloud username?","text":"<p>See if your local username on your desktop system is the same as your SuperCloud username. </p> <p>To find your local username on your desktop system:</p> <ul> <li>Windows: open Settings &gt; Accounts &gt; Your Info</li> <li>MacOS and Linux: open a terminal window and type: <code>whoami</code> </li> </ul> <p>No</p> <ul> <li>Make sure you include your SuperCloud username when you ssh to the SuperCloud system:</li> </ul> <p><code>ssh USERNAME@txe1-login.mit.edu</code> </p>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#2-did-you-use-wslubuntu-shell-on-windows-to-generate-your-ssh-keys","title":"2) Did you use WSL/Ubuntu shell on Windows to generate your ssh keys?","text":"<p>Yes</p> <ul> <li> <p>Your WSL/Ubuntu username may be different than your SuperCloud username.     To find your WSL/Ubuntu username open a     WSL/Ubuntu terminal window and type: <code>whoami</code></p> <p>If your WSL/Ubuntu username is different than your SuperCloud username, you will need a config file and you must include the User directive  in the config file and specify your SuperCloud username. See the  Configuring different SSH keys per host page for instructions on creating  the ssh config file on your local desktop system.     \u00a0</p> </li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#3-did-you-select-a-different-non-default-path-andor-name-for-your-ssh-keys","title":"3) Did you select a different (non-default) path and/or name for your ssh keys?","text":"<p>A default path and name for your ssh keys was offered to you when you ran ssh-keygen. The default name is id_rsa. The default path varies, depending on which OS is running on your desktop computer:</p> <ul> <li>if you have a Linux desktop, the default location     is:\u00a0\u00a0/home/your-local-username/.ssh</li> <li>if you have a MacOS desktop, the default location     is:\u00a0\u00a0/Users/your-local-username/.ssh</li> <li>if you have a Windows desktop, the default location     is:\u00a0\u00a0C:\\Users\\your-local-username.ssh\u00a0 However, if you are using WSL, then     the default location is:\u00a0\u00a0/home/your-local-username/.ssh</li> </ul> <p>Yes</p> <ul> <li>You will need a config file. You must include the IdentityFile     directive in the config file and provide the path to and the name of     your ssh key file.     \u00a0</li> <li>See the Configuring different SSH keys per host     page for instructions on creating the ssh config file on your local     desktop system.</li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#4-do-you-have-ssh-keys-for-multiple-systems-eg-github","title":"4) Do you have ssh keys for multiple systems (e.g. GitHub)?","text":"<p>Yes</p> <ul> <li>You will need a config file. You must include the IdentityFile     directive in the config file and provide the path to and the name of     your ssh key file.     \u00a0</li> <li>See the Configuring different SSH keys per host     page for instructions on creating the ssh config file on your local     desktop system.</li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#so-you-need-an-ssh-config-file-do-you-have-the-file-sshconfig-on-your-desktop-system","title":"So you need an ssh config file. Do you have the file .ssh/config on your desktop system?","text":"<p>Based on your desktop operating system, look in the associated .ssh directory to see if you have a file named config:</p> <ul> <li>Windows:\u00a0\u00a0C:\\Users\\your-local-username.ssh\u00a0 However, if you are using WSL,     then the default location is:\u00a0\u00a0/home/your-local-username/.ssh</li> <li>MacOS:\u00a0\u00a0/Users/your-local-username/.ssh</li> <li>Linux:\u00a0\u00a0/home/your-local-username /.ssh     \u00a0</li> </ul> <p>No</p> <ul> <li>Make sure the file name is config and not config.txt.     \u00a0</li> <li>See the Configuring different SSH keys per host     page for instructions on creating the ssh config file on your local     desktop system.</li> </ul> <p>Yes</p> <ul> <li>Make sure the config file resides in the same directory as your ssh     keys.     \u00a0</li> <li> <p>Make sure the file name is config and not config.txt.</p> </li> <li> <p>Make sure the config file is owned by you, not the administrator. On     Windows systems, use this command to view the file owner: dir /Q</p> </li> <li> <p>Make sure you have the statements you need in your ssh config file:</p> <ul> <li>You must include the Host directive (line 1 in the example config file below) followed by     txe1-login.mit.edu     \u00a0</li> <li>If your user name on your desktop system is different from your     SuperCloud username, you will need to add the     User directive (see line 2 below), replacing your-SuperCloud-username     with your SuperCloud username</li> <li> <p>If you have multiple ssh keys for different systems, or you did not use the default path and/or name (id_rsa)     for your ssh keys, then you will need to add the     IdentityFile directive (line 3 below), replacing     path-and-name-of-private-key with the location and name of     your ssh private key file. E.g.     C:\\Users\\your-local-username\\.ssh\\id_rsa. Be sure to use the ssh key     file without the \".pub\" extension</p> <p>Example ssh config file</p> </li> </ul> <pre><code> \u00a0 Host txe1-login.mit.edu\n     \u00a0 \u00a0User your-SuperCloud-username\n     \u00a0 \u00a0IdentityFile path-and-name-of-private-key\n</code></pre> </li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#do-you-have-ssh-keys-for-multiple-systems","title":"Do you have ssh keys for multiple systems?","text":"<p>Yes</p> <ul> <li>Make sure the keys for the different systems have different     filenames     \u00a0</li> <li> <p>You'll need multiple entries in your ssh config file so that ssh     knows which set of keys goes with which system.</p> <p>You'll need one entry, similar to the one for accessing SuperCloud shown above, for each set of ssh keys. Each entry should include these directives:</p> <ul> <li> <p>Host - the system address</p> </li> <li> <p>IdentityFile - the full path and name of the private key file for the system specified in the Host directive</p> </li> <li> <p>User - the username that you use to log into the system, if it's different from your local username on your desktop system</p> </li> </ul> </li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#you-tried-everything-here-and-you-still-cant-connect","title":"You tried everything here and you still can't connect?","text":"<p>If you've tried/confirmed all of the above items and still can't connect, type the following command, then copy, paste and send the text output of the command to supercloud@mit.edu. Please do not send a screenshot - there will be a lot of output and it will be difficult for us to read.</p> <p><code>ssh -vvv USERNAME@txe1-login.mit.edu</code></p>","tags":["Troubleshooting","Getting Started"]},{"location":"ssh-troubleshooting-checklist/#references","title":"References","text":"<p>Here is a list of webpages that were mentioned here, plus some others that might be helpful:</p> <ul> <li>LLx Online Courses Practical     HPC course, \"Getting Started with SuperCloud\" module, \"Accessing SuperCloud\" section</li> <li>Generating SSH Keys</li> <li>Configuring different SSH keys per host <ul> <li>In the ssh config file, the Host should be txe1-login*.mit.edu. This will work for accessing all SuperCloud login nodes.</li> <li>The HostName tag isn't needed.</li> </ul> </li> <li>SSH Key Portal and     instructions for installing your ssh     key</li> </ul>","tags":["Troubleshooting","Getting Started"]},{"location":"submitting-jobs/","title":"Submitting Jobs","text":"<p>For most job types, there are two ways to start the job: using the commands provided by the scheduler, Slurm, or using wrapper command, LLsub, that we have provided. LLsub creates a scheduler command based on the arguments you feed it, and will output that command to show you what it is running. The scheduler commands may provide more flexibility, and the wrapper commands may be easier to use in some cases and are scheduler agnostic. We show some of the more commonly used options. More Slurm options can be seen on the Slurm documentation page, and more LLsub options can be seen by running <code>LLsub -h</code> at the command line.</p> <p>There are two main types of jobs that you can run: interactive and batch jobs. Interactive jobs allow you to run interactively on a compute node in a shell. Batch jobs, on the other hand, are for running a pre-written script or executable. Interactive jobs are mainly used for testing, debugging, and interactive data analysis. Batch jobs are the traditional jobs you see on an HPC system and should be used when you want to run a script that doesn't require that you interact with it.</p> <p>On this page we will go over:</p> <ul> <li>How to start an Interactive Job with LLsub</li> <li>How to submit a Basic Serial job with LLsub and sbatch</li> <li>How to request more resources with sbatch</li> <li>How to request more resources with LLsub</li> <li>How to submit an LLMapReduce Job</li> <li>How to submit a job with pMatlab, sbatch, or LaunchFunctionOnGrid</li> <li>How to get the most performance out of LLsub, LLMapReduce, and pMatlab using Triples Mode</li> </ul> <p>You can find examples of several job types in the Teaching Examples github repository. They are also in the <code>bwedx</code>shared group directory and anyone with a SuperCloud account can copy them to their home directory and use them as a starting point.</p>"},{"location":"submitting-jobs/#how-to-start-an-interactive-job-with-llsub","title":"How to start an Interactive Job with LLsub","text":"<p>Interactive jobs allow you to run interactively on a compute node in a shell. Interactive jobs are mainly used for testing, debugging, and interactive data analysis.</p> <p>Starting an interactive job with LLsub is very simple. To request a single core, run at the command line:</p> <p><code>LLsub -i</code></p> <p>As mentioned earlier on this page, when you run an LLsub command, you'll see the Slurm command that is being run in the background when you submit the job. Once your interactive job has started, you'll see the command line prompt has changed. It'll say something like:</p> <p><code>USERNAME@d-14-13-1:~$</code></p> <p>Where <code>USERNAME</code> is your username, and <code>d-14-13-1</code> is the hostname of the machine you are on. This is how you know you are now on a compute node in an interactive job.</p> <p>By default you will be allocated a single CPU core. We have a number of options that allow you to request additional resources. You can always view these options and more by running <code>LLsub -h</code>. We'll go over a few of those here. Note that these can (and often should) be combined.</p> <ul> <li>Full Exclusive Node: Add the word <code>full</code> to request an exclusive     node. No one else will be on the machine with you:</li> </ul> <p><code>LLsub -i full</code></p> <ul> <li>A number of cores: Use the <code>-s</code> option to request a certain     number of CPU cores, or slots. Here, for example, we are requesting     4 cores:</li> </ul> <p><code>LLsub -i -s 4</code></p> <ul> <li>GPUs: Use the <code>-g</code> option to request a GPU. You need to specify     the GPU type and the number of GPUs you want. You can request up to     the number of GPUs on a single node. Refer to the     Systems and Software page to see how     many GPUs are available per node. Remember you may want to also     allocate some number of CPUs in addition to your GPUs. To get 20     CPUs and 1 Volta GPU (half the resources on our Xeon-G6 nodes), you     would run:</li> </ul> <p><code>LLsub -i -s 20 -g volta:1</code></p>"},{"location":"submitting-jobs/#submitting-a-simple-serial-batch-job","title":"Submitting a Simple Serial Batch Job","text":"<p>Submitting a batch job to the scheduler is the same for most languages. This starts by writing a submission script. This script should be a bash script (it should start with <code>#!/bin/bash</code>) and contain the command(s) you need to run your code from the command line. It can also contain scheduler flags at the beginning of the script, or load modules or set environment variables you need to run your code.</p> <p>A job submission script for a simple, serial, batch job (for example, running a python script) looks like this:</p> <pre><code>#!/bin/bash\n\n# Loading the required module\nmodule load anaconda/2023a\n\n# Run the script\npython myScript.py\n</code></pre> <p>The first line is the <code>#!/bin/bash</code> mentioned earlier. It looks like a comment, but it isn't. This tells the machine how to interpret the script, that it is a bash script. Lines 3 and 4 demonstrate how to load a module in a submission script. The final line of the script runs your code. This should be the command you use to run your code from the command line, including any input arguments. This example is running a python script, therefore we have <code>python myScript.py</code>.</p>"},{"location":"submitting-jobs/#submitting-with-llsub","title":"Submitting with LLsub","text":"<p>To submit a simple batch job, you can use the LLsub command:</p> <p><code>LLsub myScript.sh</code></p> <p>Here <code>myScript.sh</code> can be a job submission script, or could be replaced by a compiled executable. The <code>LLsub</code> command, with no arguments, creates a scheduler command with some default options. If your submission script is <code>myScript.sh</code>, your output file will be <code>myScript.sh.log-%j</code>, where <code>%j</code> is a unique numeric identifier, the JobID for your job. The output file is where all the output for your job gets written. Anything that normally is written to the screen when you run your code, including any errors or print statements, will be printed to this file.</p> <p>When you run this command, the scheduler will find available resources to launch your job to. Then <code>myScript.sh</code> will run to completion, and the job will finish when the script is complete.</p>"},{"location":"submitting-jobs/#submitting-with-slurm-scheduler-commands","title":"Submitting with Slurm Scheduler Commands","text":"<p>To submit a simple batch job with the same default behavior as LLsub above, you would run:</p> <p><code>sbatch -o myScript.sh.log-%j myScript.sh</code></p> <p>Here <code>myScript.sh</code> can be a job submission script, or could be replaced by a compiled executable. The <code>-o</code> flag states the name of the file where any output will be written, the <code>%j</code> portion indicates job ID. If you do not include this flag, any output will be written to <code>slurm-JOBID.out</code>, which may make it difficult differentiate between job outputs.</p> <p>You can also incorporate this flag into your job submission script by adding lines starting with <code>#SBATCH</code> followed by the flag right after the first <code>#!/bin/bash</code> line:</p> <pre><code>#!/bin/bash\n\n# Slurm sbatch options\n#SBATCH -o myScript.sh.log-%j\n\n# Loading the required module(s)\nmodule load anaconda/2023a\n\n# Run the script\npython myScript.py\n</code></pre> <p>Like <code>#!/bin/bash</code>, these lines starting with <code>#SBATCH</code> look like comments, but they are not. As you add more flags to specify what resources your job needs, it becomes easier to specify them in your submission script, rather than having to type them out at the command line. If you incorporate Slurm flags in your script like this, you can submit it by running:</p> <p><code>sbatch myScript.sh</code></p> <p>When you run these commands, the scheduler will find available resources to launch your job to. Then <code>myScript.sh</code> will run to completion, and the job will finish when the script is complete.</p> <p>Note that when you start adding additional resources you need to make a choice between using <code>LLsub</code> and <code>sbatch</code>. If you have <code>sbatch</code> options in your submission script and submit it with <code>LLsub</code>, <code>LLsub</code> will ignore any additional command line arguments you give it and use those described in the script.</p>"},{"location":"submitting-jobs/#requesting-additional-resources-with-sbatch","title":"Requesting Additional Resources with sbatch","text":"<p>By default you will be allocated a single core for your job. This is fine for testing, but usually you'll want more than that. For example you may want:</p> <ul> <li>Additional cores on multiple nodes (distributed)</li> <li>Additional cores on the same node (shared memory or threading)</li> <li>Multiple independent tasks (job array/throughput)</li> <li>Exclusive node(s)</li> <li>More memory or cores per process/task/worker</li> <li>GPUs</li> </ul> <p>Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want.</p> <p>How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python's multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU.</p>"},{"location":"submitting-jobs/#additional-cores-on-multiple-nodes","title":"Additional Cores on Multiple Nodes","text":"<p>The flag to request a certain number of cores that can be on more than one node is <code>--ntasks</code>, or <code>-n</code> for short. A task is Slurm's terminology for an individual process or worker. For example, to request 4 tasks you can add the following to your submission script:</p> <p><code>#SBATCH -n 4</code></p> <p>You can control how many nodes these tasks are split onto using the <code>--nodes</code>, or <code>-N</code>. Your tasks will be split evenly across the nodes you request. For example, if I were to have the following in my script:</p> <p><code>#SBATCH -n 4 #SBATCH -N 2</code></p> <p>I would have four tasks on two nodes, two tasks on each node. Specify the number of nodes like this does not ensure that you have exclusive access to those nodes. It will by default allocate one core for each task, so in this case you'd get a total of four cores, two on each node. If you need more than one core for each task, take a look at the cpus-per-task option, and if you need exclusive access to those nodes see the exclusive option.</p>"},{"location":"submitting-jobs/#additional-cores-on-the-same-node","title":"Additional Cores on the Same Node","text":"<p>There are technically two ways to do this. You can use the same options as requesting tasks on multiple nodes and setting the number of Nodes to 1, say we want four cores:</p> <p><code>#SBATCH -n 4 #SBATCH -N 1</code></p> <p>Or you can use <code>-c</code>, or the <code>--cpus-per-task</code> option by itself:</p> <p><code>#SBATCH -c 4</code></p> <p>As far as the number of cores you get, the result will be the same. You'll get the four cores on a single node. There is a bit of a nuance on how Slurm sees it. The first allocates four tasks all on one node. The second allocates a single task with four CPUs or cores. You don't need to worry too much about this, choose whichever makes the most sense to you.</p>"},{"location":"submitting-jobs/#job-arrays","title":"Job Arrays","text":"<p>NOTE: We encourage everyone who runs a job array to use LLsub with Triples mode. See the page LLsub Job Array Triples to see how to set this up.</p> <p>A simple way to run the same script or command with different parameters or on different files in parallel is by using a Job Array. With a Job Array, the parallelism happens at the Scheduler level and is completely language agnostic. The best way to use a Job Array is to batch up your parameters so you have a finite number of tasks each running a set of parameters, rather than one task for each parameter. In your submission script you specify numeric indices, corresponding to the number of tasks that you want running at once. Those indices, or Task IDs are captured in environment variables, along with the total number of tasks, and passed into your script. Your script then has the information it needs to split up the work among tasks. This process is described in the Teaching Examples github repository, with examples in Julia and Python.</p> <p>First you want to take a look at your code. Code that can be submitted as a Job Array usually has one big for loop. If you are iterating over multiple parameters or files, and have nested for loops, you'll first want to enumerate all the combinations of what you are iterating over so you have one big loop. Then you want to add a few lines to your code to take in two arguments, the Task ID and the number of tasks, use those numbers to split up the thing you are iterating over. For example, I might have a list of filenames, <code>fnames</code>. In python I would add:</p> <pre><code># Grab the arguments that are passed in\nmy_task_id = int(sys.argv[1])\nnum_tasks = int(sys.argv[2])`\n\n# Assign indices to this process/task\nmy_fnames = fnames[my_task_id-1:len(fnames):num_tasks]\n\nfor f in my_fnames: ...\n</code></pre> <p>Notice that I am iterating over <code>my_fnames</code>, which is a subset of the full list of filenames determined by the task ID and number of tasks. This subset will be different for each task in the array. Note that the third line of code will be different for languages with arrays that start at index 1 (see the Julia Job Array code for an example of this).</p> <p>The submission script will look like this:</p> <pre><code>#!/bin/bash\n\n#SBATCH -o myScript.sh.log-%j-%a\n#SBATCH -a 1-4\n\n# Loading the required module(s)\nmodule load anaconda/2023a\n\npython top5each.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT\n</code></pre> <p>The <code>-a</code> (or <code>--array</code>) option is where you specify your array indices, or task IDs. Here I am creating an array with four tasks by specifying 1 \"through\" 4. When the scheduler starts your job, it will start up four independent tasks, each will run this script, and each will have <code>#SLURM_ARRAY_TASK_ID</code> set to its task ID. Similarly, <code>$SLURM_ARRAY_TASK_COUNT</code> will be set to the total number of tasks, in this case 4.</p> <p>You may have noticed that there is an additional <code>%a</code> in the output file name. There will be one output file for each task in the array, and the <code>%a</code> puts the task ID on at the end of the filename, so you know which file goes with which task.</p> <p>By default you will get one core for each task in the array. If you need more than one core for each task, take a look at the cpus-per-task option, and if you need to add a GPU to each task, check out the the GPUs section.</p>"},{"location":"submitting-jobs/#exclusive-nodes","title":"Exclusive Nodes","text":"<p>Requesting an exclusive node ensures that there will be no other users on the node with you. You might want to do this when you know you need to make use of the full node, when you are running performance tests, or when you think your program might affect other users. There is some software that have not been designed for a shared HPC environment, and so use all the cores on the node, whether you have allocated them or not. You can look through their documentation to see if there is a way to limit the number of cores it uses, or you can request an exclusive node. Another situation where you might affect other users is when you don't yet know what resources your code requires. For these first few runs it makes sense to request an exclusive node, and then look at the resources that your job used, and request those resources in the future.</p> <p>To request an exclusive node or nodes, you can add the following option:</p> <p><code>#SBATCH --exclusive</code></p> <p>This will ensure that wherever the tasks in your job land, those nodes will be exclusive. If you have four tasks, for example, specified with either <code>-n</code> (<code>--ntasks</code>) or in a job array, and those four tasks fall on the same node, you will get that one node exclusively. It will not force each task onto its own exclusive node without adding other options.</p>"},{"location":"submitting-jobs/#adding-more-memory-or-cores-per-task","title":"Adding More Memory or Cores per Task","text":"<p>You can ensure that each task has more than one core or the default amount of memory the same way. By default, each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the Systems and Software page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, with the Xeon-P8 nodes, they have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do this is using the <code>--cpus-per-task</code>, or <code>-c</code> option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I'd want to request five cores for each task:</p> <p><code>#SBATCH -c 5</code></p> <p>This works nicely with both the <code>-n</code> (<code>--ntasks</code>) and <code>-a</code> (<code>--array</code>) options. As the flag name implies, you will get 5 cpu cores for every task in your job. If you are already using the <code>-c</code> option for a shared memory or threaded job, you can either use the <code>-n</code> and <code>-N 1</code> alternative and save <code>-c</code> for adding additional memory, or you can increase what you put for <code>-c</code>. For example, if I know I'm going to use 4 cores in my code, but each will need 20 GB of RAM, I can request a total of 4*5 = 20 cores.</p> <p>How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first in exclusive  mode so your job can have access to the maximum amount of memory. Then you can use the <code>sacct</code> slurm command to get the memory used:</p> <p><code>sacct -j JOBID -o JobID,JobName,State,AllocCPUS,MaxRSS --units=G</code></p> <p>where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job.</p> <p>If the MaxRSS value is larger than the per-slot/core memory limit for the compute node (again, check the Systems and Software page to get this for the resource type you are requesting), you will have to request additional memory for your job.</p> <p>This formatting for the accounting data prints out a number of memory datapoints for the job. They are all described in the sacct man page.</p>"},{"location":"submitting-jobs/#requesting-gpus","title":"Requesting GPUs","text":"<p>Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA.</p> <p>Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, add the following line to your submission script:</p> <p><code>#SBATCH --gres=gpu:volta:1</code></p> <p>This flag will give you a single GPU. For multi-node jobs, it'll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task.</p> <p>Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our Systems and Software page.</p>"},{"location":"submitting-jobs/#requesting-additional-resources-with-llsub","title":"Requesting Additional Resources with LLsub","text":"<p>By default you will be allocated a single core for your job. This is fine for testing, but usually you'll want more than that. For example you may want:</p> <ul> <li>Additional cores on the same node (shared memory or threading)</li> <li>Multiple independent tasks (job array/throughput)</li> <li>More memory or cores per process/task/worker</li> <li>GPUs</li> </ul> <p>Here we have listed and will go over some of the more common resource requests. Most of these you can combine to get what you want. We will show the lines that you would add to your submission script, but note that you can also include these options at the command line if you want.</p> <p>How do you know what you should request? An in-depth discussion on this is outside the scope of this documentation, but we can provide some basic guidance. Generally, parallel programs are either implemented to be distributed or not. Distributed programs can communicate across different nodes, and so can scale beyond a single node. Programs written with MPI, for example, would be distributed. Non-Distributed programs you may see referred to as shared memory or multithreaded. Python's multiprocessing package is a good example of a shared memory library. Whether your program is Distributed or Shared Memory dictates how you request additional cores: do they need to be all on the same node, or can they be on different nodes? You also want to think about what you are running: if you are running a series of identical independent tasks, say you are running the same code over a number of files or parameters, this is referred to as Throughput and can be run in parallel using a Job Array. (If you are iterating over files like this, and have some reduction step at the end, take a look at LLMapReduce). Finally, you may want to think about whether your job could use more than the default amount of memory, or RAM, and whether it can make use of a GPU.</p> <p>If you are submitting your job with LLsub, you should be aware of its behavior. If you have any Slurm options in your submission script (any lines starting with <code>#SBATCH</code>) LLsub will ignore any command line arguments you give it and only use those you specify in your script. You can still submit this script with LLsub, but it won't add any extra command line arguments you pass it.</p>"},{"location":"submitting-jobs/#additional-cores-on-the-same-node-with-llsub","title":"Additional Cores on the Same Node with LLsub","text":"<p>Libraries that use shared memory or threading to handle parallelism require that all cores be on the same node. In this case you are constrained to the number of cores on a single machine. Check the Systems and Software page to see the number of cores available on the current hardware.</p> <p>To request multiple cores on the same node for your job you can use the <code>-s</code> option in <code>LLsub</code>. This stands for \"slots\". For example, if I am running a job and I'd like to allocate 4 cores to it, I would run:</p> <p><code>LLsub myScript.sh -s 4</code></p>"},{"location":"submitting-jobs/#job-array","title":"Job Array","text":"<p>See LLsub Job Array Triples.</p>"},{"location":"submitting-jobs/#adding-more-memory-or-cores","title":"Adding More Memory or Cores","text":"<p>If you anticipate that your job will use more than ~4 GB of RAM, you may need to allocate more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. Each core gets its fair share of the RAM on the node, calculated by the total amount of memory on the node divided by the number of cores. See the Systems and Software  page for a list of the amount of RAM, number of cores, and RAM per core for each resource type. For example, the Xeon-P8 nodes have 192 GB of RAM and 48 cores, so each core gets 4 GB of RAM. Therefore, the way to request more memory is to request more cores. Even if you are not using the additional core(s), you are using their memory. The way to do with LLsub is the <code>-s</code> (for slots) option. Say I know each task in my job will use about 20 GB of memory, with the Xeon-P8 nodes above, I'd want to request five cores for each task:</p> <p><code>LLsub myScript.sh -s 5</code></p> <p>If you are already using the <code>-s</code> option for a shared memory or threaded job, you should increase what you put for <code>-s</code>. For example, if I know I'm going to use 4 cores in my code, but each will need 20 GB of RAM, I can request a total of 4*5 = 20 cores:</p> <p><code>LLsub myScript.sh -s 20</code></p> <p>How do you know how much memory your job needs? You can find out how much memory a job used after the job is completed. You can run your job long enough to get an idea of the memory requirement first (you can request the maximum number of cores per node for this step). Then you can use the <code>sacct</code> slurm command to get the memory used:</p> <p><code>sacct -j JOBID -oJobID,JobName,State,AllocCPUS,MaxRSS --units=G</code></p> <p>where JOBID is your job ID. State shows the job status, keep in mind that the memory numbers are only accurate for jobs that are no longer running, and AllocCPUS is the number of CPU cores that were allocated to the job. MaxRSS is the maximum resident memory (maximum memory footprint) used by each job.</p> <p>If the MaxRSS value is larger than the per-slot/core memory limit for the compute node (again, check the Systems and Software page to get this for the resource type you are requesting), you will have to request additional memory for your job.</p> <p>This formatting for the accounting data prints out a number of memory data points for the job. They are all described in the sacct man page.</p>"},{"location":"submitting-jobs/#requesting-gpus-with-llsub","title":"Requesting GPUs with LLsub","text":"<p>Some code can be accelerated by adding a GPU, or Graphical Processing Unit. GPUs are specialized hardware originally developed for rendering the graphics you see on your computer screen, but have been found to be very fast at doing certain operations and have therefore been adopted as an accelerator. They are frequently used in Machine Learning libraries, but are increasingly used in other software. You can also write your own GPU code using CUDA.</p> <p>Before requesting a GPU, you should verify that the software, libraries, or code that you are using can make use of a GPU, or multiple GPUs. The Machine Learning packages available in our anaconda modules should all be able to take advantage of GPUs. To request a single GPU, use the following command:</p> <p><code>LLsub myScript.sh -g volta:1</code></p> <p>This flag will give you a single GPU. For multi-node jobs, it'll give you a single GPU for every node you end up on, and will give you a single GPU for every task in a Job Array. If your code can make use of multiple GPUs, you can set this to 2 instead of 1, and that will give you 2 GPUs for each node or Job Array task.</p> <p>Note that only certain operations are being done on the GPU, your job will still most likely run best given a number of CPU cores as well. If you are not sure how many to request, if you request 1 GPU, ask for 20 CPUs (half of the CPUs), if you request 2 GPUs, you can ask for all of the CPUs. You can check the current CPU and GPU counts for each node on our Systems and Software page. To request 20 cores and 1 GPU, run:</p> <p><code>LLsub myScript.sh -s 20 -g volta:1</code></p>"},{"location":"submitting-jobs/#llmapreduce","title":"LLMapReduce","text":"<p>The LLMapReduce command scans the user-specified input directory and translates each individual file as a computing task for the user-specified application. Then, the computing tasks will be submitted to scheduler for processing. If needed, the results can be post-processed by setting up a user-specified reduce task, which is dependent on the mapping task results. The reduce task will wait until all the results become available.</p> <p>You can view the most up-to-date options for the LLMapReduce command by running the command LLMapReduce -h. You can see examples of how to use LLMapReduce jobs in /usr/local/examples directory on the SuperCloud system nodes. Some of these may be in the <code>examples</code> directory in your home directory. You can copy any that are missing from <code>/usr/local/examples</code> to your home directory. We also have an example in the Teaching Examples github repository, with examples in Julia and Python. These examples are also available in the bwedx shared group directory and can be copied to your home directory from there.</p> <p>LLMapReduce can work with any programs and we have a couple of examples for Java, Matlab, Julia, and Python. By default, it cleans up the temporary directory, <code>MAPRED.PID</code>. However, there is an option to keep (<code>--keep=true</code>) the temporary directory if you want it for debugging. The current version also supports a nested LLMapReduce call.</p>"},{"location":"submitting-jobs/#matlaboctave-tools","title":"Matlab/Octave Tools","text":""},{"location":"submitting-jobs/#pmatlab","title":"pMatlab","text":"<p>pMatlab was created at MIT Lincoln Laboratory to provide easy access to parallel computing for engineers and scientists using the MATLAB(R) language. pMatlab provides the interfaces to the communication libraries necessary for distributed computation. In addition to MATLAB(R), pMatlab works seamlessly with Octave, and open source Matlab toolkit.</p> <p>MATLAB(R) is the primary development language used by Laboratory staff, and thus the place to start when developing an infrastructure aimed at removing the traditional hurdles associated with parallel computing. In an effort to develop a tool that will enable the researcher to seamlessly move from desktop (serial) to parallel computing, pMatlab has adopted the use of Global Array Semantics. Global Array Semantics is a parallel programming model in which the programmer views an array as a single global array rather than multiple subarrays located on different processors. The ability to access and manipulate related data distributed across processors as a single array more closely matches the serial programming model than the traditional parallel approach, which requires keeping track of which data resides on any given individual processor.</p> <p>Along with global array semantics, pMatlab uses the message-passing capabilities of MatlabMPI to provide a global array interface to MATLAB(R) programmers. The ultimate goal of pMatlab is to move beyond basic messaging (and its inherent programming complexity) towards higher level parallel data structures and functions, allowing MATLAB(R) users to parallelize their existing programs by simply changing and adding a few lines.</p> <p>Any pMatlab code can be run on the MIT SuperCloud using standard pMatlab submission commands. The Practical High Performance Computing course on our online course platform provides a very good introduction for how to use pMatlab. There is also an examples directory in your home directory that provides several examples. The Param_Sweep example is a good place to start. There is an in-depth explanation of this example in the Teaching Examples github repository.</p> <p>If you anticipate that your job will use more than 4 GB of RAM, you may need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our xeon-p8 nodes have 48 cores and 192 GB of RAM, therefore each core represents about 4 GB. So if your job needs ~8 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with someone else's job.</p> <p>To do this with pMatlab, you can add the following line to your run script, before you the <code>eval(pRUN(...))</code> command:</p> <p><code>setenv('GRIDMATLAB_MT_SLOTS','2')</code></p>"},{"location":"submitting-jobs/#submitting-with-llsub-or-sbatch","title":"Submitting with LLsub or Sbatch","text":"<p>You can always submit a Matlab(R) script with a submission script through sbatch or LLsub. The basic submission script looks like this:</p> <pre><code>#!/bin/bash\n\n# Run the script\nmatlab -nodisplay -r \"myScript; exit\"\n</code></pre> <p>Where <code>myScript</code> is the name of the Matlab script that you want to run. When running a Matlab script through a submission script, you do need to specify that Matlab should exit after it runs your code. Otherwise it will continue to run, waiting for you to give it the next command.</p>"},{"location":"submitting-jobs/#launchfunctionongrid-and-launchparforongrid","title":"LaunchFunctionOnGrid and LaunchParforOnGrid","text":"<p>If you want to launch your serial MATLAB scripts or functions on LLSC systems, you can use the <code>LaunchFunctionOnGrid()</code> function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. Its usage, in Matlab, is as follows:</p> <p><code>launch_status = LaunchFunctionOnGrid(m_file) launch_status = LaunchFunctionOnGrid(m_file,variables)</code></p> <p>Where <code>m_file</code> is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants.</p> <p>If you want to launch your MATLAB scripts or functions that call the <code>parfor()</code> function on LLSC systems, you can use the <code>LaunchParforOnGrid()</code> function. You can execute your code without any modification (if it is written for a Linux environment) as a batch job. While <code>LaunchParforOnGrid()</code> will work functionally, it has significant limitations in performance, both at the node level and the cluster level; it might be better to use pMatlab instead. To use the <code>LaunchParforOnGrid()</code> function in MATLAB:</p> <p><code>launch_status = LaunchParforOnGrid(m_file) launch_status = LaunchParforOnGrid(m_file,variables)</code></p> <p>Where <code>m_file</code> is a string that specifies the script or function to be run, and variables is the list of variables that are being passed in. Note that variables must be variables, not constants.</p> <p>If you anticipate that your job will use more than 4 GB of RAM, you may need to allocated more resources for your job. You can be sure your job has enough memory to run by allocating more slots, or cores, to each task or process in your job. For example, our xeon-p8 nodes have 48 cores and 192 GB of RAM, therefore each core represents about 4 GB. So if your job needs ~8 GB, allocate two cores or slots per process. Doing so will ensure your job will not fail due running out of memory, and not interfere with someone else's job.</p> <p>To do this with LaunchFunctionOnGrid or LaunchParforOnGrid, you can add the following line to your run script, before you use the <code>LaunchFunctionOnGrid()</code> or <code>LaunchParforOnGrid()</code> command:</p> <p><code>setenv('GRIDMATLAB_MT_SLOTS','2')</code></p>"},{"location":"submitting-jobs/#triples-mode","title":"Triples Mode","text":"<p>Triples mode is a way to launch pMatlab, LLsub Job Array, and LLMapReduce jobs that gives you better performance and more flexibility to manage memory and threads. Unless you are requesting a small number of cores for your job, we highly encourage you to migrate to this model.</p> <p>With triples mode, you specify the resources for your job by providing 3 parameters:</p> <p><code>[Nodes NPPN NTPP]</code></p> <p>where</p> <ul> <li><code>Nodes</code>is number of compute nodes</li> <li><code>NPPN</code>is number of processes per node</li> <li><code>NTPP</code>is number of threads per process (default is 1)</li> </ul> <p>With triples mode your job will have exclusive use of each of the nodes that you request.</p>"},{"location":"submitting-jobs/#llsub","title":"LLsub","text":"<p>A brief introduction to LLsub is provided above. To use triples mode to launch LLsub job on SuperCloud, run as follows:</p> <p><code>LLsub ./submit.sh [Nodes,NPPN,NTPP]</code></p> <p>A more in-depth guide on how to convert an existing Job Array to an LLsub Triples submission is provided on the page LLsub Job Array.</p>"},{"location":"submitting-jobs/#llmapreduce-with-triples","title":"LLMapReduce with Triples","text":"<p>A brief introduction to LLMapReduce is provided above. To use triples mode to launch your LLMapReduce job on SuperCloud, use the <code>--np</code> option with the triple as its parameter, as follows:</p> <p><code>--np=[Nodes,NPPN,NTPP]</code></p>"},{"location":"submitting-jobs/#pmatlab-with-triples","title":"pMatlab with Triples","text":"<p>A brief introduction to pMatlab is provided above. To use triples mode to launch your pMatlab job on SuperCloud, you use the pRUN() function. Its usage, in Matlab, is as follows:</p> <p><code>eval(pRUN('mfile', [Nodes NPPN NTPP], 'grid'))</code></p>"},{"location":"submitting-jobs/#triples-mode-tuning","title":"Triples Mode Tuning","text":"<p>Triples mode tuning provides greater efficiency by allowing you to better tune your resource requests to your application. This one-time tuning process typically takes ~1 hour:</p> <ol> <li>Instrument your code to print a rate (work/time) giving a sense of     the speed from a ~1 minute run.</li> <li>Determine best number of threads (<code>NTPPBest</code>) by examining rate     from runs with varying numbers of threads:     <code>[1,1,1], [1,1,2], [1,1,4]</code>, ... \u00a0</li> <li>Determine best number of processes per node (<code>NPPNbest</code>) by     examining rate from runs with varying numbers of processes:     <code>[1,1,NTPPBest], [1,2,NTPPBest], [1,4,NTPPBest]</code>, ... \u00a0</li> <li>Determine best number of nodes (<code>NodesBest</code>) by examining rate from     runs of with varying numbers of nodes:     <code>[1,NPPNbest,NTPPBest], [2,NPPNbest,NTPPBest], [4,NPPNbest,NTPPBest]</code>,     ... \u00a0</li> <li>Run your production jobs using <code>[NodesBest,NPPNbest,NTPPBest]</code></li> </ol> <p>You could tune <code>NPPN</code> first, then <code>NTPP</code>. This would be a better approach if you are memory bound. You can find the max <code>NPPN</code> that will fit, then keep increasing <code>NTPP</code> until you stop getting more performance.</p> <p>\"Good\" <code>NPPN</code> values for Xeon-P8: 1, 2, 4, 8, 16, 24, 32, 48</p> <p>\"Good\" <code>NPPN</code> values for Xeon-G6: 1, 2, 4, 8, 16, 20, 32, 40</p> <p>Triples mode tuning results in a ~2x increase efficiency for many users.</p> <p>Once the best settings have been found, they can be reused as long as the code remains roughly similar. Recording the rates from the above process can often result in a publishable IEEE HPEC paper. We are happy to work with you to guide you through this tuning process.</p>"},{"location":"supercloud-software/","title":"Available Software","text":"<p>This page lists information about available software, including languages, compilers, modules, etc. This is only a partial list, so if there is anything you are interested in that isn't listed here, please contact us.</p>"},{"location":"supercloud-software/#available-languages","title":"Available Languages","text":"<ul> <li>Julia</li> <li>Python (Anaconda available through modules)</li> <li>Matlab(R)/Octave</li> <li>R</li> <li>C/C++</li> <li>Fortran</li> <li>Java</li> <li>Perl 5</li> <li>Ruby</li> </ul>"},{"location":"supercloud-software/#modules","title":"Modules","text":"<p>To see the most up-to-date list of currently available modules, run the command <code>module avail</code>. For more information about modules, see the module section on the Software and Package Management page.</p>"},{"location":"supercloud-software/#software","title":"Software","text":""},{"location":"supercloud-software/#machine-learning-tools","title":"Machine Learning Tools","text":"<ul> <li>Tensorflow</li> <li>Pytorch</li> </ul>"},{"location":"supercloud-software/#big-data-software-stack","title":"Big Data Software Stack","text":"<ul> <li>Hadoop</li> <li>Zookeeper</li> <li>Accumulo</li> </ul>"},{"location":"supercloud-software/#middleware-software-stack","title":"Middleware Software Stack","text":"<ul> <li>ARPACK</li> <li>ATLAS</li> <li>Boost</li> <li>BLAS</li> <li>FFTW</li> <li>LAPAC</li> <li>OpenMPI</li> <li>OpenBLAS</li> </ul>"},{"location":"supercloud-software/#lincoln-laboratory-developed-software","title":"Lincoln Laboratory Developed Software","text":"<ul> <li>pMatlab</li> <li>D4M</li> <li>Graphulo</li> <li>LLMapReduce</li> </ul>"},{"location":"supercloud-software/#compilers","title":"Compilers","text":"<ul> <li>gcc</li> <li>g++</li> <li>gfortran</li> <li>icc</li> </ul>"},{"location":"systems-and-software/","title":"SuperCloud System","text":""},{"location":"systems-and-software/#mghpcc-tx-e1-specifications","title":"MGHPCC TX-E1 Specifications","text":"Summary Number of Nodes 704 Total CPU Cores 32000 Total GPUs 448 Distributed Storage 873 TB"},{"location":"systems-and-software/#individual-nodes","title":"Individual Nodes","text":"Processor Nodes CPU Cores Node RAM RAM/core GPU Type GPU RAM GPUs/node Local Disk Intel Xeon   Platinum 8260 480 48 192 GB 4 GB NA NA NA 4.4 TB Intel Xeon  Gold 6248 224 40 384 GB 9 GB NVidiaVolta V100 32 GB 2 3.8 TB"},{"location":"systems-and-software/#resource-allocations","title":"Resource Allocations","text":"Processor Partition Starting Standard Intel Xeon  Platinum 8260 xeon-p8 2 Nodes(96 Cores) 8 Nodes(384 Cores) Intel Xeon  Gold 6248 xeon-g6-volta 1 Node(40 Cores 2 GPUs) 4 Nodes(160 cores, 8 GPUs)"},{"location":"tensorboard/","title":"Tensorboard","text":"<p>You can run Tensorboard as a job, this is the preferred method of   doing this. \u00a0 First start an interactive session with a reserved port:</p> <pre><code>[studentx@login-1 ~]$ LLsub -i --resv-ports 1     \u00a0     \nsalloc --immediate=60 -p normal --constraint=xeon-e5 --cpus-per-task=4 --qos=high\u00a0 srun --resv-ports=1 --pty bash -i     \nsalloc: Granted job allocation 355286     \nsalloc: Waiting for resource configuration     \nsalloc: Nodes node-052 are ready for job     \u00a0`\n</code></pre> <p>Then create your logging directory in TMPDIR:</p> <p><code>[studentx@node-052 ~]$ mkdir -p ${TMPDIR}/tensorboard</code></p> <p>Set up your forwarding name and file:</p> <pre><code>[studentx@node-052 ~]$ PORTAL_FWNAME=\"$(id -un tr '[A-Z]' '[a-z]')-tensorboard\"     \n[studentx@node-052 ~]$ PORTAL_FWFILE=\"/home/gridsan/portal-url-fw/${PORTAL_FWNAME}\"     \n[studentx@node-052 ~]$ echo $PORTAL_FWFILE     \n/home/gridsan/portal-url-fw/studentx-tensorboard    \n\n[studentx@node-052 ~]$ echo \"Portal URL is: https://${PORTAL_FWNAME}.fn.txe1-portal.mit.edu/\"     \nPortal URL is: https://studentx-tensorboard.fn.txe1-portal.mit.edu/`\n</code></pre> <p>Put the forward URL in the forwarding file (when you run \"cat   \\$PORTAL_FWFILE\" you should only see one line- if you see two or more,   delete all but the last line): \u00a0 <pre><code>[studentx@node-052 ~]$ echo \"http://$(hostname -s):${SLURM_STEP_RESV_PORTS}/\" &gt;&gt; $PORTAL_FWFILE     \n[studentx@node-052 ~]$ cat $PORTAL_FWFILE     \nhttp://node-052:12637/\n</code></pre></p> <p>Set the permissions on the forward file properly:</p> <p><code>[studentx@node-052 ~]$ chmod u+x ${PORTAL_FWFILE}</code></p> <p>Load an anaconda module and start tensorboard</p> <pre><code>[studentx@node-052 ~]$ module load anaconda/2023a\n[studentx@node-052 ~]$ tensorboard --logdir ${TMPDIR}/tensorboard --host \"$(hostname -s)\" --port ${SLURM_STEP_RESV_PORTS}     \u00a0\n</code></pre> <p>In the browser, go to the URL listed above (for example, mine is https://studentx-tensorboard.fn.txe1-portal.mit.edu/)</p>"},{"location":"tips-for-analyzing-large-datasets/","title":"Tips for Analyzing Large Datasets","text":"<p>If you are analyzing a dataset with a large number of files, avoid using <code>dir</code>, <code>ls</code>, and any other commands that scan the file system. Running thousands of file system scans can overload the file system. Here are some tips for working with large datasets.</p>","tags":["File System","Tips and Best Practices"]},{"location":"tips-for-analyzing-large-datasets/#construct-a-filename","title":"Construct a filename","text":"<p>Instead of getting a list of all the files, it is better to construct a filename and check to see if that filename exists before reading or creating the file.</p>","tags":["File System","Tips and Best Practices"]},{"location":"tips-for-analyzing-large-datasets/#create-another-file-that-lists-the-full-file-paths-of-each-file-in-the-dataset","title":"Create another file that lists the full file paths of each file in the dataset","text":"<p>Before launching your job, create a text file that lists the full paths of each file in the dataset. Each processor that is processing the data can read in this single file and won't need to execute <code>ls</code> or any other metadata intensive operations to get the filenames. This also makes it easy to divide up the files amongst processors in a nice \"mimo\" (multiple inputs, multiple outputs) manner that minimizes the number of starts and stops, compared to a \"siso\" (single input, single output) model.</p>","tags":["File System","Tips and Best Practices"]},{"location":"tips-for-analyzing-large-datasets/#siso-vs-mimo","title":"\"siso\" vs \"mimo\"","text":"<p>\"siso\" is an acronym for single input, single output. In a siso model, a single process calls your application multiple times to process all of the input files that are assigned to it. Each time your application is called, the software or program is loaded, it processes a single input file, then the software is unloaded.  unloaded.</p> <p>\"mimo\" is an acronym for multiple input, multiple output. The advantage of using a mimo model is that your application is called and loaded just once to process all of its assigned input files. When your application starts, it should read the text file containing the list of the data files and process the appropriate subset of files before being unloaded.</p> <p>See the LLMapReduce page, particularly the section discussion the <code>--apptype=APPLICATION-TYPE</code> option for a more detailed explanation of the mimo model and the changes to your application that are required in order to operate in mimo mode.</p>","tags":["File System","Tips and Best Practices"]},{"location":"tips-for-analyzing-large-datasets/#generating-the-list-of-files","title":"Generating the list of files","text":"<p>To generate the text file which will contain a list of files to process, log into the login node via ssh or in a Jupyter Notebook Terminal window and issue the following commands at the Linux Command line:</p> <ul> <li><code>$ cd datasetPath</code></li> <li><code>$ lfs find \"$(pwd -P)\" | grep \"fileSearchPattern\" | sort &gt; ./inputFilename</code></li> </ul> <p>where:</p> <ul> <li><code>datasetPath</code> is the path to your dataset</li> <li><code>fileSearchPattern</code> is a Linux regular expression that the\u00a0<code>grep</code> command uses to find filenames that you want to include,</li> <li><code>inputFilename</code> is the name of the text file that will contain a list of all your data filenames</li> </ul> <p>Note that the <code>lfs</code> command in the example above can only be used if your files reside on the Lustre filesystem (somewhere within /home/gridsan). If your data files are located on a node's local disk (in <code>$TMPDIR</code> or <code>/state/partition1</code>), drop <code>lfs</code> from the beginning of the command above.</p> <p>Unfortunately, you can't use regular wildcards (for example: *.txt) to identify the files that you want to match because the\u00a0<code>grep</code> command expects a regular expression.\u00a0\u00a0You can find information on how to use regular expressions in Linux from these external websites (link will open in a new tab):</p> <ul> <li>Using Grep &amp; Regular Expressions to Search for Text Patterns in Linux</li> <li>Regex tutorial - A quick cheatsheet by examples</li> <li>Linux Tutorial - Cheat Sheet</li> </ul>","tags":["File System","Tips and Best Practices"]},{"location":"tips-for-analyzing-large-datasets/#example-1","title":"Example 1","text":"<p>Generate a file containing a list of files within  <code>~/examples/LLGrid_MapReduce/MultiLevelData/data</code> that end in <code>.txt</code> and save the list to a file called txt-files</p> <p>From the Linux command line, execute the following commands - Command 1:      - <code>$ cd ~/examples/LLGrid_MapReduce/MultiLevelData/data</code> - Command 2:      - <code>$ lfs find \"$(pwd -P)\" | grep \"\\.txt$\" | sort &gt; ./txt-files</code></p> <p>The contents of the file txt-files is below</p> <p>$ cat ./txt-files</p> <p>/home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a1/a11.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a1/a12.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a1/a13.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a2/a21.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a2/a22.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a2/a23.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a3/a31.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a3/a32.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a3/a33.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b1/b11.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b1/b12.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b1/b13.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b2/b21.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b2/b22.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b2/b23.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b3/b31.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b3/b32.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b3/b33.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c1/c11.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c1/c12.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c1/c13.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c2/c21.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c2/c22.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c2/c23.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c3/c31.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c3/c32.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c3/c33.txt</p>","tags":["File System","Tips and Best Practices"]},{"location":"tips-for-analyzing-large-datasets/#example-2","title":"Example 2","text":"<p>Generate a file containing a list of files within <code>~/examples/LLGrid_MapReduce/MultiLevelData/data</code> that begin\u00a0with a lowercase letter, followed by the number 2, and end in <code>.txt</code> and save the list to a file called az-2-txt-files</p> <p>From the Linux command line, execute the following commands</p> <ul> <li>Command 1:<ul> <li><code>$ cd ~/examples/LLGrid_MapReduce/MultiLevelData/data</code></li> </ul> </li> <li>Command 2:<ul> <li><code>$ lfs find \"$(pwd -P)\" | grep \"[a-z]2.*\\.txt$\" | sort &gt; ./az-2-txt-files</code></li> </ul> </li> </ul> <p>The contents of the file az-2-txt-files is below</p> <p><code>$ cat ./az-2-txt-files</code></p> <p>/home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a2/a21.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a2/a22.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/a/a2/a23.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b2/b21.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b2/b22.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/b/b2/b23.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c2/c21.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c2/c22.txt /home/gridsan/AN23082/examples/LLGrid_MapReduce/MultiLevelData/data/c/c2/c23.txt</p>","tags":["File System","Tips and Best Practices"]},{"location":"transferring-files/","title":"Transferring Files","text":"<p>There are number of ways to access and transfer data and files between your computer and the MIT SuperCloud System depending on your OS (Mac/Linux or Windows) and your comfort with Linux/Unix commands. Regardless of OS, files can be downloaded, but not uploaded, through the web portal and both uploaded and downloaded through the Jupyter web interface. For Mac/Linux or Windows with Cygwin, you have the additional options of using <code>scp</code> and <code>rsync</code> from a terminal window. If you are using PuTTY on Windows, there is a way to transfer through that program as well. All of these are described below.</p>"},{"location":"transferring-files/#maclinux-and-most-windows","title":"Mac/Linux and Most Windows","text":""},{"location":"transferring-files/#via-scp","title":"Via scp","text":"<p>You can use <code>scp</code> (secure copy) to copy files to the MIT SuperCloud. The <code>scp</code> command expects a path to the file or directory name that you would like to transfer followed by a space and then the destination. Note, it is best to copy to or from your local machine. In addition, the location on the system is the login node followed by a <code>:</code> (colon) followed by the path to the directory where you want to put the file. If you do not specify the directory location <code>scp</code> will place it in your top level directory, which on the SuperCloud system is your home directory. An example of copying a directory to TX-E1 is shown below. Note that the <code>scp</code> command is followed by <code>-r</code>, to recursively copy all the files in the directory, and the directory name is followed by a <code>/</code> so that <code>scp</code> copies all of the files in the directory.</p> <p><code>myLocalMachine UserName &gt; scp -r testCodes/ &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/snippets</code></p> <p>If you want to copy data from TX-E1 to your computer, you just swap the \"from\" and \"to\":</p> <p><code>myLocalMachine UserName &gt; scp -r &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/results/</code></p>"},{"location":"transferring-files/#via-rsync","title":"Via rsync","text":"<p>The <code>rsync</code> (remote synchronization) command is similar to the <code>scp</code> command. However, after the first time that you copy files, executing the <code>rsync</code> command will copy only the files that have changed since the last rsync. This can often save time. Note that if you have the slash at the end, it means \"all contents in the directory\". If not, it means the directory itself and its contents. As shown in case A below, the directory, <code>myDir</code>, and its all contents will be copied over to the snippets directory on the remote host. However, in case B, only the contents in the <code>myDir</code> directory will be copied to the snippets directory on the remote host.</p> <p>CaseA: <code>rsync -rlug myDir &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/snippets</code> CaseB: <code>rsync -rlug myDir/ &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/snippets</code></p> <p>There are a number of options that can be used with rsync. Among the most commonly used are:</p> <ul> <li><code>r</code> to recursively copy files in all sub-directories</li> <li><code>l</code> to copy and retain symbolic links.</li> <li><code>u</code> is needed if you have modified files on the destination and you     don't want the old file to overwrite over the newer version on the     destination.</li> <li><code>g</code> is used to preserve group attributes associated with files in a     shared group.</li> <li><code>h</code> human readable</li> <li><code>v</code> verbose so that you get any error or warning information</li> </ul> <p>Again, if you want to copy data from TX-E1 to your computer, you just swap the \"from\" and \"to\":</p> <p>CaseA: <code>rsync -rlug &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/results myDir</code> CaseB: <code>rsync -rlug &lt;username&gt;@txe1-login.mit.edu:/home/gridsan/&lt;username&gt;/results/ myDir</code></p>"},{"location":"transferring-files/#windows","title":"Windows","text":"<p>Depending on which ssh client you use you will transfer data differently. If you use Bash on Windows 10, Mobaxterm, Cygwin, or other Linux-like command line environment, you can use scp or rsync directly and should follow the instructions above for Mac/Linux. If you use PuTTY, you can follow the directions below to use pscp. You run these commands from your own computer, not from the MIT SuperCloud login node (if you have already logged in with ssh, you've gone too far).</p> <p>IMPORTANT: MIT SuperCloud runs a Linux operating system. Two big differences between Windows and Linux are the direction that the folder separators face (Windows uses \"\\\" and Linux uses \"/\") and case sensitivity (Linux file and directory names are case sensitive, while Windows file and folder names are not). Whether you are using scp, rsync, or pscp you must use the Linux slashes \"/\" in your path and the correct case in your filenames (Documents =/= documents). It will also be much easier to transfer data if you avoid using spaces in your file and directory/folder names.</p> <p>HINT: Windows paths sometimes specify different \"drives\" (such as the C: drive, where most local documents are kept). Different ssh clients treat these drives differently. Using the example of a folder in the C drive (say C:myFolder), the translations for each are:</p> <ul> <li>Bash for Windows 10: <code>/mnt/c/myFolder</code></li> <li>Mobaxterm: <code>/drives/c/myFolder</code></li> <li>Cygwin: <code>/cygdrive/C/myFolder</code></li> </ul> <p>When in doubt, you can navigate to the directory that you want to transfer and issue the <code>pwd</code> command, which should give you the full path to your current location.</p>"},{"location":"transferring-files/#putty","title":"PuTTY","text":"<p>When transferring files from a Windows machine to the SuperCloud system, which are linux machines, you need to use a version of <code>scp</code>, or secure copy. The software package, PuTTY, which you installed to provide <code>ssh</code>, includes <code>scp</code>. To transfer a file between your Windows desktop and the SuperCloud linux system:</p> <ul> <li>Open a command window by typing <code>run</code> in the search box. The <code>scp</code>     command must be run from a command window.</li> <li>Set the <code>PATH</code> variable so that your system sees the putty command:</li> <li>To do it for this session only, type         <code>set PATH=C:\\Program Files\\PuTTY</code> at the command prompt</li> <li>To make this change more permanent, click on the start button,         in the search box type Environment Variables:<ul> <li>Select Add/Modify Environment Variables for user</li> <li>Click on Path and then click to edit Path.</li> <li>Add <code>C:\\Program File\\PuTTY</code> (This assumes that you installed         PuTTY in the default folder.)</li> <li>Confirm that PuTTY is in your path by typing: <code>%ECHO PATH%</code>         at the command line.</li> </ul> </li> <li>Navigate to the directory (folder) that holds the file you want to     transfer</li> <li>The <code>pscp</code> command has the format     <code>pscp &lt;Path_to_FileToBeTransfered&gt; &lt;Path_To_NewLocation&gt;</code></li> <li>For example to use <code>pscp</code> to copy the file <code>myMatlabScript.m</code> from     my current directory (folder) to my home directory on the SuperCloud     system I would type</li> </ul> <p>If I wanted to copy <code>myResults.dat</code> from my home directory on the SuperCloud system to a folder called <code>goodData</code> on my local machine I would first change directories so that I was in goodDate, using: <code>cd goodData</code>. Then, secure copy the file from SuperCloud to my local system using:</p> <p><code>pscp &lt;myUserID&gt;@txe1-login.mit.edu:/home/gridsan/&lt;myUserID&gt;/myResults.dat .</code></p> <p>Note the <code>.</code> at the end of the line - that is equivalent to \"place it here\". Also note that in Windows you can use <code>\\</code> or <code>/</code> but Linux only understands <code>/</code>.</p>"},{"location":"transferring-files/#cygwin","title":"Cygwin","text":"<p>Cygwin also supports the secure copy protocol. This works similarly to scp in a Linux/Mac. When using Cygwin, you will be able to run <code>scp</code>, however it can be tricky to determine to the path to your files. This is of the form <code>/cygdrive/C/&lt;your_File_Path&gt;</code>.</p> <p>Say you are using scp to copy the file myScript.sh from your current directory (folder) to your home directory on the SuperCloud system. Type:</p> <p><code>scp /cygdrive/C/&lt;path_to_Folder&gt;/mScript.sh &lt;myUserID&gt;@txe1-login.mit.edu:/home/gridsan/&lt;myUserID&gt;</code></p> <p>If you wanted to copy <code>myResults.txt</code> from your home directory on the SuperCloud system to a folder called <code>goodData</code> on your local machine, first change directories so that you are in <code>goodData</code>, using: <code>cd goodData.</code></p> <p>Secure copy the file from the SuperCloud system to the local system using:</p> <p><code>scp &lt;myUserID&gt;@txe1-login.mit.edu:/home/gridsan/&lt;myUserID&gt;/myResults.txt  /cyqdrive/c/&lt;path_to_goodData&gt;/</code></p> <p>Note that in Windows you can use <code>\\</code> or <code>/</code> but Linux only understands <code>/</code>.</p>"},{"location":"transferring-files/#downloading-files-through-the-web-portal","title":"Downloading Files through the Web Portal","text":"<p>The web portal can be accessed at:</p> <p>https://txe1-portal.mit.edu/gridsan/USERNAME/</p> <p>Where USERNAME is your username. When you navigate to the portal page, you will be prompted for your username and password. Through the portal, you can navigate through your directories and download files. You cannot upload files this way.</p>"},{"location":"transferring-files/#uploading-and-downloading-files-using-jupyter","title":"Uploading and Downloading Files Using Jupyter","text":"<p>You can both upload and download files using the Jupyter interface through your browser. If you do not already have a Jupyter instance running, start one up. The Jupyter portal is located at:</p> <p>https://txe1-portal.mit.edu/jupyter/jupyter_notebook.php</p> <p>Once you have an instance running, follow the \"Open Notebook\" link to open Jupyter. You will see the files and directories in your home directory. Here you can right click on a file and select \"Save Link As\" to download a file. The \"Upload\" button is located in the top right of the page, right next to the \"New\" button. You can select multiple files to upload at a time.</p>"},{"location":"troubleshoot-jupyter-notebooks/","title":"Troubleshooting Jupyter Notebooks","text":"","tags":["Jupyter","Troubleshooting"]},{"location":"troubleshoot-jupyter-notebooks/#the-jupyter-notebook-doesnt-launch","title":"The Jupyter Notebook doesn't launch","text":"<p>While the scheduler is attempting to launch your Jupyter Notebook job, you may see it count through a number of retries before you see the \"Open Notebook\" link for accessing your notebook.</p> <p>If you do not see the \"Open Notebook\" link after 10 retries, you might see one of the following errors instead:</p> <p></p> <p></p> <p>In these situations, try the following steps and if you still can't launch a notebook, send email to supercloud@mit.edu for assistance.</p> <ol> <li>Check to see if the job is running:     Enter the <code>LLstat</code> command in an ssh session to the SuperCloud and look     for a job with the name JupyterNotebook</li> <li>If the job is running (you will see \"R\" in the \"ST\" column),     refresh the browser page and you should see the \"Open Notebook\"     link.</li> <li>If the job is pending (you will see \"PD\" in the \"ST\" column),     there aren't resources to launch your job. This can occur if the     system is heavily loaded, or if you have exhausted your allocated     number of cores.</li> <li>If you have not exhausted your core allocation, check to see if     there are system resources available:     Enter the <code>LLfree</code> command in the ssh session</li> </ol> <p>Other things to try if you are unable to launch a Jupyter Notebook:</p> <ul> <li>Refresh\u00a0the browser page and relaunch\u00a0the notebook</li> <li>Close\u00a0your browser, open\u00a0a new browser, and relaunch\u00a0the notebook</li> <li>Delete\u00a0the file\u00a0<code>$HOME/.jupyter/llsc_notebook_bashrc</code>\u00a0and     relaunch\u00a0the notebook</li> </ul>","tags":["Jupyter","Troubleshooting"]},{"location":"troubleshoot-jupyter-notebooks/#other-jupyter-notebook-problems","title":"Other Jupyter Notebook problems","text":"<p>If you run into problems after your notebook has launched, you should check the log file for error messages. You can find the log file in your SuperCloud home directory:</p> <p><code>$HOME/slurm-xxxxx.out</code></p> <p>where <code>xxxxx</code> is the job number of your Jupyter Notebook.</p> <p>Send email to supercloud@mit.edu with the job number and any error messages that appear in the log file.</p>","tags":["Jupyter","Troubleshooting"]},{"location":"using-environment-vars/","title":"Using Environment Variables","text":"<p>Environment variables allow you to customize the environment in which programs run. For example, you can define an environment variable to set the path to data files, or enable or disable features.</p> <p>SuperCloud has created some modulefiles that will set environment variables that allow you to select a particular version of executables or libraries, specify the location of datasets, or enable/disable functionality. In order use these modulefiles, you'll have to load them from the Linux command prompt. </p> <p>On this page, we will go over how to use environment variables in:</p> <ul> <li>Linux shell script</li> <li>Slurm\u00a0environment</li> <li>Python</li> <li>MATLAB\u00ae</li> </ul> <p>In the instructions below, <code>VARNAME</code> is the name of the environment variable, and <code>value</code> is the value assigned to the environment variable. By convention, environment variable names are all UPPERCASE.</p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#using-environment-variables-in-a-linux-shell-script","title":"Using environment variables in a Linux shell script","text":"","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#bash-shell-variable-naming","title":"bash shell variable naming","text":"<p>The Linux bash shell is pretty particular with its variable names. A name is a word consisting only of alphanumeric characters and underscores, and beginning with an alphabetic character or an underscore. In addition, there are a few naming conventions:</p> <ul> <li>Variable Names: Lower-case, with underscores to separate words.     Example: <code>my_variable_name</code></li> <li>Constants and Environment Variable Names: All caps, separated with     underscores, declared at the top of the file. Example: <code>MY_CONSTANT</code></li> </ul>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-get-the-value-of-an-environment-variable","title":"To get the value of an environment variable","text":"<p>In a Linux shell script, you get the value of an environment variable by adding the dollar sign <code>$</code> in front of the environment variable name.</p> <p><code>$VARNAME</code></p> <p>Example of saving the value of an environment variable to a variable local to your script:</p> <p><code>path_to_data=$IMAGENET_PATH</code></p> <p>Notice that there are no spaces on either side of the equals sign. In Linux, there must not be any spaces around the equals sign when assigning the value of an environment variable to another variable.</p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-set-the-value-of-an-environment-variable","title":"To set the value of an environment variable","text":"<p>You can set environment variables from a Linux shell script that remain in effect while your script and any child processes that it creates are\u00a0running. Environment variables are local to the process in which they were set. When a child process is created, it inherits all the environment variables and their values from the parent process.</p> <p><code>VARNAME=value</code></p> <p>Example of setting the value of an environment variable:</p> <p><code>IMAGENET_PATH=/home/gridsan/groups/datasets/ImageNet</code></p> <p>Notice that there are no spaces on either side of the equals sign. In Linux, there must not be any spaces around the equals sign when assigning a value to an environment variable.</p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#using-slurm-environment-variables","title":"Using Slurm environment variables","text":"<p>Once a job is submitted to Slurm, Slurm provides information about its execution environment through environment variables. For example: the working directory, the job id, total number of tasks in a job array, job array id number. These Slurm environment variables may be useful in your code and for monitoring your jobs.</p> <p>You can find a full list of Slurm's environment variables on these pages:</p> <ul> <li>Slurm input environment     variables</li> <li>Slurm output environment     variables</li> </ul>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-get-the-value-of-an-environment-variable_1","title":"To get the value of an environment variable","text":"<p>To get the value of a Slurm environment variable, add a dollar sign <code>$</code> in front of the environment variable name.</p> <p><code>$VARNAME</code></p> <p>Example of passing the array task id and count as parameters to your python code:</p> <p><code>python test.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT</code></p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#setting-the-value-of-an-environment-variable","title":"Setting the value of an environment variable","text":"<p>The Slurm input environment variables are assigned values based on the flags that are passed to Slurm when the job is submitted.</p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#using-environment-variables-in-python","title":"Using environment variables in Python","text":"","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-get-the-value-of-an-environment-variable_2","title":"To get the value of an environment variable","text":"<p><code>import os</code> <code>os.environ.get('VARNAME')</code> <code>os.getenv('VARNAME')</code></p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-set-the-value-of-an-environment-variable_1","title":"To set the value of an environment variable","text":"<p>In Python, when you set an environment variable, it is set for the Python session.</p> <p><code>import os</code> <code>os.environ['VARNAME'] = 'value'</code></p> <p>It's important to remember that the settings you apply in a Python script don't work outside that specific process; os.environ doesn't overwrite the environment variables system-wide.</p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#using-environment-variables-in-matlab","title":"Using environment variables in MATLAB\u00ae","text":"","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-get-the-value-of-an-environment-variable_3","title":"To get the value of an environment variable","text":"<p><code>getenv('VARNAME')</code></p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-environment-vars/#to-set-the-value-of-an-environment-variable_2","title":"To set the value of an environment variable","text":"<p><code>setenv('VARNAME','value')</code></p>","tags":["Getting Started","How To","Tips and Best Practices","Linux"]},{"location":"using-the-download-partition/","title":"Using the Download Partition","text":"<p>Compute nodes on SuperCloud do not have internet access.</p> <p>This means that if you need to check out git repositories, download packages or download small data sets, you will need to do this on the login node before you start or launch your job. The login nodes have internet access for pip installs, git repo checkouts, etc.</p> <p>To accommodate large data downloads, we have added a separate partition named \"download\" with data transfer nodes. This node is schedulable through this new download partition.</p> <p>You can request use of this partition by requesting it with your job submission. If you are using sbatch or sbatch flags in your job script you can use the <code>-p</code> or <code>--partition</code> flag to specify the download partition:</p> <ul> <li><code>--partition=download</code></li> <li><code>-p download</code></li> </ul> <p>If you are using <code>LLsub</code> the flag to select a partition is <code>-q</code>:</p> <ul> <li><code>-q download</code></li> </ul> <p>This enables you to launch a data download job before your job starts or after your job completes in order to sync whatever data you need.</p>","tags":["Running Jobs"]},{"location":"using-the-download-partition/#examples","title":"Examples","text":"","tags":["Running Jobs"]},{"location":"using-the-download-partition/#interactive-job-with-llsub","title":"Interactive Job with LLsub","text":"<p><code>LLsub -i -q download</code> </p>","tags":["Running Jobs"]},{"location":"using-the-download-partition/#job-submission-script","title":"Job Submission Script","text":"my_download_job.sh<pre><code>#!/bin/bash\n\n#SBATCH -p download\n#SBATCH -o my_download.out-%j\n\nwget URL\n</code></pre> <p>where URL is the URL to the file you are trying to download. You would launch this job with either:</p> <pre><code>sbatch my_download_job.sh\n</code></pre> <p>or</p> <pre><code>LLsub my_downlod_job.sh\n</code></pre>","tags":["Running Jobs"]},{"location":"using-the-download-partition/#monitoring-your-download-job","title":"Monitoring your download job","text":"<p>You can execute the <code>LLstat</code> command at the Linux command line to see if your download job is running. We recommend checking the log file to check for errors and to track the progress of your download.</p>","tags":["Running Jobs"]},{"location":"using-top-and-htop/","title":"Troubleshooting Your Job - using top and htop","text":"<p>If your job is terminating prematurely or taking longer than expected to complete, there are a couple of Linux commands that you can use to help diagnose the problem. These suggestions will require that some portion of your job is currently running, You'll have to log onto one of the compute nodes where your job is running and execute a Linux command to obtain information about your process(es) running on that node.</p>","tags":["Troubleshooting","Tips and Best Practices","How To"]},{"location":"using-top-and-htop/#how-to-log-onto-the-compute-node","title":"How to log onto the compute node","text":"<p>To find out which compute node(s) your job is running on, use the <code>LLstat</code> command on one of the login nodes. You can find the names of the compute nodes in the NODELIST(REASON) column.</p> <p>Then log onto one of the compute nodes by issuing the <code>ssh</code> command:</p> <p><code>$ ssh &lt;compute-node-name&gt;.mit.edu</code></p> <p>Now you'll be logged onto a compute node where one or more of your processes is running. The following sections will help you gather more information about your running job.</p>","tags":["Troubleshooting","Tips and Best Practices","How To"]},{"location":"using-top-and-htop/#if-your-job-is-terminating-prematurely","title":"If your job is terminating prematurely","text":"<p>One common reason jobs terminate prematurely is because the node runs out of memory. We have instructions for checking the memory usage of a completed job on the page Finding the Memory Requirements of My Job. While your job is currently running, you can use the Linux <code>top</code> command to monitor the memory that your process(es) is using on a particular node.</p> <p>Once you have logged onto the compute node, use the Linux <code>top</code> command to see information about your process(es) running on that node. Although there may be other processes belonging to other users also running on the compute node, you'll see only your processes.</p> <p>Below is a snippet of sample output from the <code>top</code> command. The output is automatically refreshed every 3 seconds. You can click on the screenshot to view a larger version, then use the browser's back arrow to return to this page.</p> <p>In the tasks section of the output (below the white line), you will see information about your individual processes. The RES column shows the physical memory used by each process, and the %MEM column shows the percentage of the total system RAM being used by each process. This will help you determine whether your process is using too much memory.</p> <p>Enter 'q' (for quit) to return to the Linux shell prompt.</p> <p> </p>","tags":["Troubleshooting","Tips and Best Practices","How To"]},{"location":"using-top-and-htop/#if-your-job-is-running-slowly","title":"If your job is running slowly","text":"<p>If your job is taking much longer to run than you expect, one possible explanation is that your processes may not be properly distributed across the processors on the node. You can log onto the compute node and use the Linux <code>htop</code> command to see how the load is distributed across the processors on the node.</p> <p>Once you have logged onto the compute node, use the Linux <code>htop</code> command to see information about load distribution.</p> <p>Below are some sample output screens from the <code>htop</code> command. The output is automatically refreshed every 3 seconds. You can click on the screenshot to view a larger version, then use the browser's back arrow to return to this page.</p> <p>The <code>htop</code> output displays instantaneous cpu usage (specified as a percentage) for each process. In the sample screenshots below, the compute node that we're looking at has 256 processors, which are numbered 1 - 256. The progress bar next to each process describes its usage.</p> <p>Beneath the cpu usage section, you'll see \"Load average\", a set of 3 values that represent the average system load for the last 1, 5 and 15 minute periods. Since these represent averages, load gives you a better idea of the overall work that is being done on the node.</p> <p>In the screenshots below, we submitted a pMatlab job using Triples Mode to specify our resource request, which allows us to specify how we want the MATLAB\u00ae processes distributed.  In this example, we specified [1 4 2], which means:</p> <ul> <li>we want to use 1 compute node</li> <li>we want to launch 4 MATLAB\u00ae processes per node</li> <li>we want to limit the number of OpenMP threads to 2</li> </ul> <p>Enter 'q' (for quit) to return to the Linux shell prompt.</p>","tags":["Troubleshooting","Tips and Best Practices","How To"]},{"location":"using-top-and-htop/#well-distributed-processes","title":"Well distributed processes","text":"<p>In this first <code>htop</code> screenshot, we see that our 4 MATLAB\u00ae processes are running on different processors. </p> <p></p>","tags":["Troubleshooting","Tips and Best Practices","How To"]},{"location":"using-top-and-htop/#improperly-distributed-processes","title":"Improperly distributed processes","text":"<p>In this second <code>htop</code> screenshot, it looks like all of the work is being performed by a single processor. If you monitor <code>htop</code> for a while and see that only a single processor seems to be performing the work most of the time, contact supercloud@mit.edu; we can provide some recommendations on how to get the processes distributed properly. </p> <p></p>","tags":["Troubleshooting","Tips and Best Practices","How To"]},{"location":"using-trusted-x-forwarding/","title":"X Forwarding","text":"<p>If you need to use a tool that has a GUI (Graphical User Interface), you can use X forwarding.\u00a0X forwarding is a mechanism that allows you to start up remote applications but forward the application display to your local desktop machine. When using trusted X forwarding, the remote machine is treated as a trusted client.\u00a0A program with access to the display is trusted with access to the entire display.</p> <p>Note, using X11 forwarding over long-distance networks can be slow. This is because the computer rendering the X11 windows is sending rendering commands to your local desktop which then renders them. More details are available in this informative posting: https://superuser.com/questions/1217280/why-is-x11-forwarding-so-inefficient.</p> <p>For certain applications, Jupyter Notebooks may be a good alternative.</p>","tags":["How To","Using the System"]},{"location":"using-trusted-x-forwarding/#required-software","title":"Required software","text":"<p>In order to use X forwarding, your desktop system needs to run an X server. If your desktop system is a Linux system, you're all set.\u00a0 However, if you have a Mac or a Windows desktop, you'll need to install some software.</p> <p>For Mac, install XQuartz.\u00a0You can find XQuartz at https://www.xquartz.org/.</p> <p>For Windows, we recommend using MobaXterm. You can find MobaXterm here: https://mobaxterm.mobatek.net/.</p>","tags":["How To","Using the System"]},{"location":"using-trusted-x-forwarding/#running-your-application","title":"Running your application","text":"<p>If you have a GUI based application that you want to run on the SuperCloud system, you can start the application on a compute node and display the GUI back to your desktop. In order to do this, you must use the <code>-Y</code> option (this provides trusted X11 forwarding) of the <code>ssh</code> command when accessing the SuperCloud login node from your desktop:</p> <p><code>ssh -Y USERNAME@txe1-login.mit.edu</code></p> <p>Note: For users with a Windows 10 desktop, instead of using the Windows Command Prompt to ssh to the login node, we recommend connecting using MobaXterm.</p> <p>Once you're logged on to the login node, execute the following command to obtain a compute node through Slurm allocation and\u00a0create the\u00a0ssh tunnel between the login node and the compute nodes.</p> <p><code>LLsub -i --x11</code></p> <p>Note: <code>--x11</code> uses a lowercase x.</p> <p>From this terminal, you can launch your GUI application and it will be displayed on your local desktop.\u00a0 For a quick test of X11 forwarding, try running <code>xeyes</code>, which puts a couple of eyes on your screen that follow your mouse.</p>","tags":["How To","Using the System"]},{"location":"verify-pmatlab-setup/","title":"Verifying pMatlab Setup","text":"<p>If you're planning to launch pMatlab jobs on SuperCloud (from a login node or an interactive session on a compute node), then you should run the <code>Param_Sweep</code> example to verify your SuperCloud account configuration.</p> <p>An easy way to check on potential SuperCloud account pMatlab configuration issues is to run the <code>Param_Sweep</code> example in the <code>examples</code> directory in your SuperCloud home directory.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"verify-pmatlab-setup/#verifying-your-supercloud-pmatlab-configuration","title":"Verifying your SuperCloud pMatlab Configuration","text":"<p>If you have altered any of the <code>.m</code> files in the <code>Param_Sweep</code> directory, send an email to supercloud@mit.edu to get a new copy and then follow the directions below for running the application.</p> <p>If you have not altered the <code>Param_Sweep</code> files in your example directory,</p> <ul> <li>In the terminal that is connected to a login node or compute node,     go to the <code>Param_Sweep</code> directory: <code>cd ~/examples/Param_Sweep</code>.</li> <li>Start MATLAB\u00ae from the command line:     <code>matlab -nodisplay -singleCompThread</code></li> <li>At the MATLAB\u00ae prompt type: <code>RUNv2</code>.</li> <li>You can check the status of your job at the MATLAB\u00ae prompt by     typing: <code>LLstat</code>.</li> <li>Take note of the name of the compute node that your job is running     on by looking at the output of <code>LLstat</code>, in the     NODELIST(REASON) column. An example compute node name is\u00a0<code>a-21-35</code>.</li> <li>When LLstat returns 0, your job is done.</li> <li>Check the output log files to verify the job completed successfully.<ul> <li>Exit MATLAB\u00ae or use another ssh session's terminal window.</li> <li>Go to the log directory <code>cd ~/examples/Param_Sweep/MatMPI</code></li> <li>The output log files will be in a directory called     <code>p0-p3_&lt;name-of-compute-node&gt;</code> where <code>&lt;name-of-compute-node&gt;</code> is     what you saw\u00a0in the output of <code>LLstat</code>, in the     NODELIST(REASON) column. For example, <code>a-21-35</code>.</li> <li>Go into the log file directory and look at the     file\u00a0<code>param_sweep_parallel_v2.0.out</code>. It should look similar to     this:</li> </ul> </li> </ul> <pre><code>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 &lt; M A T L A B (R) &gt;\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Copyright 1984-2020 The MathWorks, Inc.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 R2020a (9.8.0.1323502) 64-bit (glnxa64)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 February 25, 2020\n\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\nLLSC: GRIDMATLAB_MANYCORE is defined. gridMatlab launch is optimized for\nmany core nodes\n&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Time for the host-to-rank map with TMPDIR (sec):\n1.4764\n\nMANYCORE JOB BEGIN: on a-21-35\n\nmy_rank =\n\u00a0 \u00a0 \u00a00\nSUCCESS\n\u00a0 \u00a0 1.0000\u00a0 \u00a0 \u00a0 \u00a0\u00a0 0\u00a0 \u00a0 2.5000\n\u00a0 \u00a0 2.0000\u00a0 \u00a0 \u00a0 \u00a0\u00a0 0\u00a0 \u00a0 5.0000\n\u00a0 \u00a0 3.0000\u00a0 \u00a0 \u00a0 \u00a0\u00a0 0\u00a0 \u00a0 7.5000\n\u00a0 \u00a0 4.0000\u00a0 \u00a0 \u00a0 \u00a0\u00a0 0\u00a0\u00a0 10.0000\n\u00a0 \u00a0 5.0000\u00a0 \u00a0 1.0000\u00a0\u00a0 12.5000\n\u00a0 \u00a0 6.0000\u00a0 \u00a0 1.0000\u00a0\u00a0 15.0000\n\u00a0 \u00a0 7.0000\u00a0 \u00a0 1.0000\u00a0\u00a0 17.5000\n\u00a0 \u00a0 8.0000\u00a0 \u00a0 1.0000\u00a0\u00a0 20.0000\n\u00a0 \u00a0 9.0000\u00a0 \u00a0 2.0000\u00a0\u00a0 22.5000\n\u00a0 \u00a010.0000\u00a0 \u00a0 2.0000\u00a0\u00a0 25.0000\n\u00a0 \u00a011.0000\u00a0 \u00a0 2.0000\u00a0\u00a0 27.5000\n\u00a0 \u00a012.0000\u00a0 \u00a0 2.0000\u00a0\u00a0 30.0000\n\u00a0 \u00a013.0000\u00a0 \u00a0 3.0000\u00a0\u00a0 32.5000\n\u00a0 \u00a014.0000\u00a0 \u00a0 3.0000\u00a0\u00a0 35.0000\n\u00a0 \u00a015.0000\u00a0 \u00a0 3.0000\u00a0\u00a0 37.5000\n\u00a0 \u00a016.0000\u00a0 \u00a0 3.0000\u00a0\u00a0 40.0000\n\nMANYCORE JOB END: on a-21-35\n</code></pre> <p>If you are able to run <code>Param_Sweep</code> and your output looks similar to the above, your SuperCloud pMatlab configuration is correct.</p> <p>If you are unable to run <code>Param_Sweep</code> or you do not see the similar results, send email to supercloud@mit.edu and include the information that you cannot run the application - this indicates to us that something has happened with your pMatlab configuration.</p>","tags":["pMatlab","Troubleshooting","Tips and Best Practices"]},{"location":"web-portal/","title":"Web Portal","text":"<p>You can get to the SuperCloud Web Portal at this URL:</p> <p>https://txe1-portal.mit.edu</p> <p>On this page you will find links to a number of useful tools. These include</p> <ul> <li>Page to add or remove ssh keys</li> <li>Home directory file browser</li> <li>Dynamic database launching system</li> <li>Jupyter Notebook portal</li> </ul>"},{"location":"web-portal/#portal-authentication","title":"Portal Authentication","text":"<p>There are three ways you can authenticate into the Web Portal: PKI Certificate/Smart Card, MIT Touchstone/InCommon Federation, Username and Password.</p>"},{"location":"web-portal/#mit-touchstoneincommon-federation","title":"MIT Touchstone/InCommon Federation","text":"<p>Most SuperCloud users can log in via the MIT Touchstone/InCommon Federation link. These include users with an active MIT Kerberos account or account at most other educational institutions. When you click the \"MIT Touchstone/InCommon Federation\" link for the first time you will be taken to a page where you can select your institution. Note most options are spelled out, for example \"MIT\" is listed as \"Massachusetts Institute of Technology\". UMass Lowell users should select \"University of Massachusetts System\". When you have selected your institution, you should check the box that says \"Remember my Choice\" and click select. You'll be taken to your institution's login page where you can log in. Then you will be taken to the Portal main page.</p> <p>If you get a message saying \"The MIT Touchstone / InCommon Federation principal you presented is not associated with an account on this system.\u00a0Please sign up at: https://supercloud.mit.edu/requesting-account.\" and you already have an account contact us and let us know.</p>"},{"location":"web-portal/#username-and-password","title":"Username and Password","text":"<p>If you do not have an active login for an educational institution you may need to log into the portal using your username and password. Let us know and we will let you know how to do this.</p>"},{"location":"web-portal/#pki-certificatesmart-card","title":"PKI Certificate/Smart Card","text":"<p>Those who have a Smart Card or PKI Certificate can log in using the first link on the page. If you would like to log in using your PKI Certificate or Smart Card, please let us know.</p>"},{"location":"web-portal/#addingremoving-ssh-keys","title":"Adding/Removing SSH Keys","text":"<p>The first link on the Portal Home page is \"/sshkeys/\". Clicking on this link takes you to a page where you can add new keys or remove old ones. To add a new key, paste the key in the box at the bottom of the page and click \"Update Keys\". You should see your new key populated in the table and display similarly to the other keys listed. To remove an old key, click the check box next to the key you would like to remove and click \"Update Keys\" at the bottom. Instructions for generating a new key are here.</p>"},{"location":"web-portal/#browsing-your-home-directory","title":"Browsing your Home Directory","text":"<p>The second link on the Portal Home page is \"/gridsan/\". Click this link and then select your username to see the files in your home directory. You can click on a file to download it or click on a directory to navigate to that directory.</p>"},{"location":"where-to-find-examples/","title":"Where to Find Examples","text":"<p>There are several places where you can find examples that demonstrate how to submit jobs, use Jupyter Notebooks, and use other tools and features that we provide):</p> <ul> <li>on Github:<ul> <li>https://github.com/llsc-supercloud/teaching-examples </li> </ul> </li> <li> <p>on SuperCloud (PLEASE NOTE: when using the examples on SuperCloud you     will need to copy them to your home directory):</p> <ul> <li>/home/gridsan/&lt;your-user-name&gt;/examples\u00a0 (these are already in     your home directory, and there is no need to copy them     elsewhere)</li> <li>/home/gridsan/groups/bwedx/teaching-examples</li> <li>/home/gridsan/groups/bwedx/SummerWorkshop2019,      includes examples used for workshops.</li> <li>Practical HPC Series has a few different examples each year:<ul> <li>/home/gridsan/groups/bwedx/Practical_HPC_2020</li> <li>/home/gridsan/groups/bwedx/Practical_HPC_2021</li> <li>/home/gridsan/groups/bwedx/Practical_HPC_2022</li> <li>/home/gridsan/groups/bwedx/Practical_HPC_2023</li> </ul> </li> <li>/usr/local/examples</li> </ul> </li> </ul> <p>In addition to these examples, we highly recommend taking the LLx online course Practical HPC. You can access the LLx Online Courses here: https://learn.llx.edly.io/.</p>","tags":["Getting Started","How To"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#databases","title":"Databases","text":"<ul> <li>Databases</li> </ul>"},{"location":"tags/#datasets","title":"Datasets","text":"<ul> <li>Datasets</li> </ul>"},{"location":"tags/#file-system","title":"File System","text":"<ul> <li>Tips for Analyzing Large Datasets</li> </ul>"},{"location":"tags/#filesystem","title":"Filesystem","text":"<ul> <li>Disk Space Limits</li> <li>File Locking</li> <li>Using Scratch Space with LLcopy2tmp</li> </ul>"},{"location":"tags/#getting-started","title":"Getting Started","text":"<ul> <li>Fixing Scripts Written in Windows</li> <li>Getting Started Tutorial</li> <li>Glossary of Unix Terms</li> <li>Getting Started with pMatlab</li> <li>SuperCloud Policies and Practices</li> <li>Preferred Shells for Windows</li> <li>Resource Limits</li> <li>ssh Troubleshooting Checklist</li> <li>Using Environment Variables</li> <li>Where to Find Examples</li> </ul>"},{"location":"tags/#how-to","title":"How To","text":"<ul> <li>Installing Windows Subsystem for Linux</li> <li>Converting Jupyter Notebooks to Scripts</li> <li>Screen</li> <li>Using Environment Variables</li> <li>Job Profiling with top and htop</li> <li>X Forwarding</li> <li>Where to Find Examples</li> </ul>"},{"location":"tags/#jupyter","title":"Jupyter","text":"<ul> <li>Converting Jupyter Notebooks to Scripts</li> <li>Best Practices for Jupyter Notebooks</li> <li>Troubleshooting Jupyter Notebooks</li> </ul>"},{"location":"tags/#llmapreduce","title":"LLMapReduce","text":"<ul> <li>LLMapReduce</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Fixing Scripts Written in Windows</li> <li>Getting Started Tutorial</li> <li>Glossary of Unix Terms</li> <li>Installing Windows Subsystem for Linux</li> <li>Using Environment Variables</li> </ul>"},{"location":"tags/#matlab","title":"Matlab","text":"<ul> <li>Running Matlab</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>Python Troubleshooting Checklist</li> </ul>"},{"location":"tags/#running-jobs","title":"Running Jobs","text":"<ul> <li>Using the Download Partition</li> </ul>"},{"location":"tags/#submitting-jobs","title":"Submitting Jobs","text":"<ul> <li>Launching pMatlab Jobs</li> <li>LLMapReduce</li> </ul>"},{"location":"tags/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Disk Space Limits</li> <li>File Locking</li> <li>Fixing Scripts Written in Windows</li> <li>Converting Jupyter Notebooks to Scripts</li> <li>Best Practices for Jupyter Notebooks</li> <li>Using Scratch Space with LLcopy2tmp</li> <li>pMatlab Job Errors</li> <li>pMatlab Job Problems</li> <li>pMatlab Troubleshooting Checklist</li> <li>Screen</li> <li>Segmentation Faults</li> <li>Tips for Analyzing Large Datasets</li> <li>Using Environment Variables</li> <li>Job Profiling with top and htop</li> <li>Verifying pMatlab Setup</li> </ul>"},{"location":"tags/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>File Locking</li> <li>Fixing Scripts Written in Windows</li> <li>GridMatlab Environment Variables</li> <li>pMatlab Job Errors</li> <li>pMatlab Job Problems</li> <li>pMatlab Troubleshooting Checklist</li> <li>Python Troubleshooting Checklist</li> <li>Segmentation Faults</li> <li>ssh Troubleshooting Checklist</li> <li>Troubleshooting Jupyter Notebooks</li> <li>Job Profiling with top and htop</li> <li>Verifying pMatlab Setup</li> </ul>"},{"location":"tags/#using-the-system","title":"Using the System","text":"<ul> <li>X Forwarding</li> </ul>"},{"location":"tags/#windows","title":"Windows","text":"<ul> <li>Fixing Scripts Written in Windows</li> <li>Installing Windows Subsystem for Linux</li> <li>Preferred Shells for Windows</li> </ul>"},{"location":"tags/#pmatlab","title":"pMatlab","text":"<ul> <li>Finding pMatlab Output</li> <li>GridMatlab Environment Variables</li> <li>Launching pMatlab Jobs</li> <li>Getting Started with pMatlab</li> <li>pMatlab Job Errors</li> <li>pMatlab Job Problems</li> <li>pMatlab Troubleshooting Checklist</li> <li>Verifying pMatlab Setup</li> </ul>"}]}